\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={}||exercise-2=={}||exercise-3=={}||exercise-4=={}||exercise-5=={}||exercise-6=={}||exercise-7=={}||exercise-8=={}||exercise-9=={}||exercise-10=={}||exercise-11=={}||exercise-12=={}||exercise-13=={}||exercise-14=={}||exercise-15=={}||exercise-16=={}||exercise-17=={}||exercise-18=={}||exercise-19=={}||exercise-20=={}||exercise-21=={}||exercise-22=={}||exercise-23=={}||exercise-24=={}||exercise-25=={}||exercise-26=={}||exercise-27=={}||exercise-28=={}||exercise-29=={}||exercise-30=={}||exercise-31=={}||exercise-32=={}||exercise-33=={}||exercise-34=={}||exercise-35=={}||exercise-36=={}||exercise-37=={}||exercise-38=={}||exercise-39=={}||exercise-40=={}||exercise-41=={}||exercise-42=={}||exercise-43=={}||exercise-44=={}||exercise-45=={}||exercise-46=={}||exercise-47=={}||exercise-48=={}||exercise-49=={}||exercise-50=={}||exercise-51=={}||exercise-52=={}||exercise-53=={}||exercise-54=={}||exercise-55=={}||exercise-56=={}||exercise-57=={}||exercise-58=={}||exercise-59=={}||exercise-60=={}||exercise-61=={}||exercise-62=={}||exercise-63=={}||exercise-64=={}||exercise-65=={}||exercise-66=={}||exercise-67=={}||exercise-68=={}||exercise-69=={}||exercise-70=={}||exercise-71=={}||exercise-72=={}||exercise-73=={}||exercise-74=={}||exercise-75=={}||exercise-76=={}||exercise-77=={}||exercise-78=={}||exercise-79=={}||exercise-80=={}||exercise-81=={}||exercise-82=={}||exercise-83=={}||exercise-84=={}||exercise-85=={}||exercise-86=={}||exercise-87=={}||exercise-88=={}||exercise-89=={}||exercise-90=={}||exercise-91=={}||exercise-92=={}||exercise-93=={}||exercise-94=={}||exercise-95=={}||exercise-96=={}||exercise-97=={}||exercise-98=={}||exercise-99=={}||exercise-100=={}||exercise-101=={}||exercise-102=={}||exercise-103=={}||exercise-104=={}||exercise-105=={}||exercise-106=={}||exercise-107=={}||exercise-108=={}||exercise-109=={}||exercise-110=={}||exercise-111=={}||exercise-112=={}||exercise-113=={}||exercise-114=={}||exercise-115=={}||exercise-116=={}||exercise-117=={}||exercise-118=={}||exercise-119=={}||exercise-120=={}||exercise-121=={}||exercise-122=={}||exercise-123=={}||exercise-124=={}||exercise-125=={}||exercise-126=={}||exercise-127=={}||exercise-128=={}||exercise-129=={}||exercise-130=={}||exercise-131=={}||exercise-132=={}||exercise-133=={}||exercise-134=={}||exercise-135=={}||exercise-136=={}||exercise-137=={}||exercise-138=={}||exercise-139=={}||exercise-140=={}||exercise-141=={}||exercise-142=={}||exercise-143=={}||exercise-144=={}||exercise-145=={}||exercise-146=={}||exercise-147=={}||exercise-148=={}||exercise-149=={}||exercise-150=={}||exercise-151=={}}
\XSIM{exercise-body}{exercise-1=={\dots the weights may be reduced to zero. \begin {choice}(4) * L1 and L2 * \correctchoice {L1} * L2 * None of the above \end {choice}}||exercise-2=={Bagging is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * \correctchoice {Trains multiple models on different subsets of the data} * Constructs an ensemble by iteratively updating weights * Uses a committee of experts to make predictions \end {choice}}||exercise-3=={Which of the following is/are Limitations of deep learning? \begin {choice} (2) * Data labeling * Obtain huge training datasets * \correctchoice {Both \circled {A} and \circled {B}} * None of the previous \end {choice}}||exercise-4=={Which neural network has only one hidden layer between the input and output? \begin {choice} (2) * \correctchoice {Shallow neural network} * Deep neural network * Feed-forward neural networks * Recurrent neural networks \end {choice}}||exercise-5=={CNN is mostly used when there is an? \begin {choice} (4) * structured data * \correctchoice {unstructured data} * both \circled {A} and \circled {B} * None of the previous \end {choice}}||exercise-6=={Which of the following is well suited for perceptual tasks? \begin {choice} (1) * feed-forward neural networks * recurrent neural networks * \correctchoice {convolutional neural networks} * Reinforcement learning \end {choice}}||exercise-7=={Which of the following is/are Common uses of RNNs? \begin {choice} * BusinessesHelp securities traders to generate analytic reports * Detect fraudulent credit-card transaction * Provide a caption for images * \correctchoice {All of the above} \end {choice}}||exercise-8=={Boosting is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * \correctchoice {Constructs an ensemble by iteratively updating weights} * Uses a committee of experts to make predictions \end {choice}}||exercise-9=={What steps can we take to prevent overfitting in a Neural Network? \begin {choice}(2) * Data Augmentation * Weight Sharing * Early Stopping * Dropout * \correctchoice {All of the previous} \end {choice}}||exercise-10=={Which of the following is an example of an ensemble learning algorithm? \begin {choice} (4) * Decision tree * SVM * \correctchoice {Random Forest} * KNN \end {choice}}||exercise-11=={AdaBoost is an example of: \begin {choice} (2) * Bagging algorithm * \correctchoice {Boosting algorithm} * Randomized algorithm * Reinforcement learning algorithm \end {choice}}||exercise-12=={Gradient Boosting is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * \correctchoice {Constructs an ensemble by iteratively updating weights} * Uses a committee of experts to make predictions \end {choice}}||exercise-13=={XGBoost is a popular implementation of: \begin {choice}(2) * Bagging algorithm * \correctchoice {Boosting algorithm} * Random Forest Algorithm * K-Means clustering algorithms \end {choice}}||exercise-14=={Stacking is an ensemble technique that: \begin {choice} (1) * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * Constructs an ensemble by iteratively updating weights * \correctchoice {Trains a meta-model to make predictions based on outputs of base models} \end {choice}}||exercise-15=={Which ensemble learning algorithm uses bootstrapping and feature sampling? \begin {choice} (4) * \correctchoice {Random Forest} * AdaBoost * Gradient Boosting * Stacking \end {choice}}||exercise-16=={The purpose of using ensemble learning is to: \begin {choice} * \correctchoice {Reduce overfitting and improve generalization} * Increase training time and complexity * Decrease the number of models required * Eliminate the need for labeled data \end {choice}}||exercise-17=={Bagging algorithms are effective in: \begin {choice}(2) * \correctchoice {Handling imbalanced datasets} * sequential data prediction * Clustering high-dimensional data * Text classification tasks \end {choice}}||exercise-18=={Which ensemble learning algorithm assigns weights to base models based on their performance? \begin {choice} (4) * \correctchoice {AdaBoost} * Random Forest * Gradient Boosting * Stacking \end {choice}}||exercise-19=={Which ensemble learning algorithm uses a committee of experts to make predictions? \begin {choice} (4) * Bagging * Boosting * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-20=={Which ensemble learning algorithm is prone to overfitting if the base models are too complex? \begin {choice} (4) * Bagging * \correctchoice {Boosting} * Random Forest * Stacking \end {choice}}||exercise-21=={Which ensemble learning algorithm can handle both regression and classification tasks? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}||exercise-22=={Ensemble learning algorithms are useful when: \begin {choice} * The dataset is small and low-dimensional * \correctchoice {The dataset is large and high-dimensional} * The dataset is perfectly balanced * The dataset contains categorical variables \end {choice}}||exercise-23=={Ensemble learning algorithms can improve model performance by: \begin {choice} (2) * Reducing bias * \correctchoice {Reducing variance} * Increasing interpretability * Increasing training time \end {choice}}||exercise-24=={Which ensemble learning algorithm can handle both numerical and categorical data without requiring one-hot encoding? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Graident Boosting} * Stacking \end {choice}}||exercise-25=={Which ensemble learning algorithm is less sensitive to outliers? \begin {choice} (4) * Bagging * Boosting * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-26=={The majority voting method in ensemble learning refers to: \begin {choice} * Combining predictions by averaging their probabilities * \correctchoice {Combining predictions by taking the mode of their classes} * Combining predictions by multiplying their probabilities * Combining predictions by taking the median of their values \end {choice}}||exercise-27=={Which ensemble learning algorithm can handle missing values in the dataset? \begin {choice} (4) * \correctchoice {Bagging} * AdaBoost * Gradient Boosting * Stacking \end {choice}}||exercise-28=={Ensemble learning algorithms are useful for: \begin {choice} (2) * \correctchoice {Improving model stability} * Increasing model complexity * Reducing feature importance * Eliminating the need for cross-validation \end {choice}}||exercise-29=={Which ensemble learning algorithm can handle non-linear relationships in the data? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Graident Boosting} * Stacking \end {choice}}||exercise-30=={Ensemble learning algorithms are effective in: \begin {choice} (2) * Reducing model interpretability * Increasing model training * \correctchoice {Handling unbalanced datasets} * Eliminating the need for hyperparameter tuning \end {choice}}||exercise-31=={Which ensemble learning algorithm can handle both numerical and categorical features effectively? \begin {choice} (4) * \correctchoice {Bagging} * AdaBoost * Gradient Boosting * Stacking \end {choice}}||exercise-32=={Which ensemble learning algorithm is less susceptible to overfitting compared to others? \begin {choice} (4) * Bagging * Boosting * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-33=={Which ensemble learning algorithm uses a weighted sum of predictions from base models? \begin {choice} (4) * Bagging * \correctchoice {AdaBoost} * Gradient boosting * Stacking \end {choice}}||exercise-34=={Which ensemble learning algorithm can be used to identify important features in a dataset? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}||exercise-35=={The ReLu activation has no effect on back-propagation and the vanishing gradient. \begin {choice} (2) * True * \correctchoice {False} * can be true and false * can't say \end {choice}}||exercise-36=={Why is the vanishing gradient a problem? \begin {choice} * Training is quick if the gradient is large and slow if it's small * with back propagation, the gradient becomes smaller as it works back through the net * The gradient is calculated multiplying two numbers between 0 and 1 * \correctchoice {All of the previous} \end {choice}}||exercise-37=={Which of the following functions can be used as an activation function in the output layer if we wish to predict the probabilities of \(n\) classes \((p_1,p_2,...,p_k)\) such that sum of \(p\) over all \(n\) equals to 1? \begin {choice} (4) * \correctchoice {Softmax} * ReLu * Sigmoid * \(\tanh \) \end {choice}}||exercise-38=={Which of the following would have a constant input in each epoch of training a Deep Learning model? \begin {choice} * \correctchoice {Weight between input and hidden layer} * Weight between hidden and output layer * Biases of all hidden layer neurons * Activation Function of output layer * none of the previous \end {choice}}||exercise-39=={Assume a simple MLP model with 3 neurons and inputs= 1,2,3. The weights to the input neurons are 4,5 and 6 respectively. Assume the activation function is a linear constant value of 3. What will be the output ? \begin {choice} (4) * 32 * 64 * \correctchoice {96} * 128 \end {choice}}||exercise-40=={The input image has been converted into a matrix of size 28 X 28 and a kernel/filter of size 7 X 7 with a stride of 1. What will be the size of the convoluted matrix? \begin {choice} (4) * \(20 \times 20\) * \(21 \times 21\) * \correctchoice {\(22 \times 22\)} * \(25 \times 25\) \end {choice}}||exercise-41=={The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer are \dots \begin {choice} (4) * \correctchoice {50} * less than 50 * more than 50 * it's an arbitrary value. \end {choice}}||exercise-42=={Which of the following statements is true when you use \(1\times 1\) convolutions in a CNN? \begin {choice} * It can help in dimensionality reduction * It can be used for feature pooling * It suffers less overfitting due to small kernel size * \correctchoice {all of the previous} \end {choice}}||exercise-43=={Deep learning algorithms are \dots more accurate than machine learning algorithm in image classification. \begin {choice} (4) * 33 \% * 37\% * 40\% * \correctchoice {41\%} \end {choice}}||exercise-44=={Which of the following are universal approximators? \begin {choice} (4) * Kernel SVM * Neural Networks * Boosted Decision trees * \correctchoice {All of the above} \end {choice}}||exercise-45=={In which of the following applications can we use deep learning to solve the problem? \begin {choice} (2) * Protein structure prediction * Prediction of chemical reactions * Detection of exotic particles * \correctchoice {all of the previous} \end {choice}}||exercise-46=={Which of following activation function can't be used at output layer to classify an image ? \begin {choice} (4) * Sigmoid * \(\tanh \) * \correctchoice {ReLU} * None of the previous \end {choice}}||exercise-47=={Dropout can be applied at visible layer of Neural Network model? \begin {choice} (2) * \correctchoice {True} * False \end {choice}}||exercise-48=={Which of the following neural network training challenge can be solved using batch normalization? \begin {choice} (2) * overfitting * Restrict activation to become too high or low * Training is too slow * Both \circled {B} and \circled {C} * \correctchoice {All of the previous} \end {choice}}||exercise-49=={Changing Sigmoid activation to ReLu will help to get over the vanishing gradient issue? \begin {choice} (2) * \correctchoice {True} * False \end {choice}}||exercise-50=={In CNN, having max pooling always decrease the parameters? \begin {choice} (4) * True * \correctchoice {False} * can be true and false * can't say \end {choice}}||exercise-51=={Bagging is more sensitive to noise. \begin {choice} (2) * True * \correctchoice {False} \end {choice}}||exercise-52=={What is \textbf {true} about the functions of a Multi Layer Perceptron? \begin {choice} (1) * The first neural nets that were born out of the need to address teh inaccuracy of an early classifier, the perceptron. * It predicts which group of given set of inputs falls into. * It generates a score that determines the confidence level of the prediction * \correctchoice {all of the previous} \end {choice}}||exercise-53=={Select reason(s) for using a Deep Neural Network. \begin {choice} * Some patterns are very complex and can't be deciphered precisely by alternate means * Deep nets are great at recognizing patterns and using them as building blocks in deciphering inputs * We finally have the technology, GPUs, to accelerate the training process by several folds of magnitude. * \correctchoice {All of the above} \end {choice}}||exercise-54=={Sentiment analysis using Deep Learning is a many-to one prediction task \begin {choice} (4) * \correctchoice {True} * False * Can be true and false * can't say \end {choice}}||exercise-55=={BackPropogation cannot be applied when using pooling layers \begin {choice}(2) * True * \correctchoice {False} \end {choice}}||exercise-56=={What is the primary purpose of regularization in deep learning? \begin {choice}(1) * to increase computational efficiency * to reduce the number of layers in a neural network * \correctchoice {to prevent overfitting} * to speed up the training process \end {choice}}||exercise-57=={Which of the following regularization techniques adds a penalty term based on the absolute values of the weights? \begin {choice} (4) * \correctchoice {L1 regularization} * L2 regularization * Dropout * Elastic Net \end {choice}}||exercise-58=={In neural networks, what does L2 regularization encourage? \begin {choice} (2) * Sparse weight matrices * large weight values * \correctchoice {small weight values} * No impact on weight values \end {choice}}||exercise-59=={How does dropout regularization work in a neural network? \begin {choice}(1) * It randomly drops input features during training * \correctchoice {It randomly drops entire layers during training} * It adds noise to the input data * It introduces a penalty term for large weights. \end {choice}}||exercise-60=={Which regularization technique combines both L1 and L2 penalties? \begin {choice} (2) * Dropout * Ride regression * \correctchoice {Elastic Net} * Batch Normalization \end {choice}}||exercise-61=={What is the purpose of early stopping as a form of regularization? \begin {choice} (1) * To stop the training process when the model is underfitting * \correctchoice {To prevent the model from memorizing the training data} * To speed up the convergence of the training process * To reduce the impact of outliers in the training data \end {choice}}||exercise-62=={Which of the following statements is true about the bias-variance tradeoff in the context of regularization? \begin {choice} (1) * Regularization always increases bias and decreases variance * Regularization always increases both bias and variance * \correctchoice {Regularization can help balance bias and variance} * Regularization has no impact on the bias-variance tradeoff \end {choice}}||exercise-63=={In the context of neural networks, what does weight decay refer to? \begin {choice} (1) * The gradual increase in weight values during training * \correctchoice {The gradual decrease in weight values during training} * The removal of unnecessary weights from the network * The introduction of noise to the weight values \end {choice}}||exercise-64=={Which of the following is a disadvantage of using a high regularization strength in a neural network? \begin {choice} (1) * Increased risk of overfitting * Faster convergence during training * Enhanced generalization to new data * \correctchoice {Reduced capacity to capture complex patterns} \end {choice}}||exercise-65=={What is weight decay? \begin {choice} * \correctchoice {A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.} * Gradual corruption of the weights in the neural network if it's training on noisy data. * The process of gradually decreasing the learning rate during training * A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights. \end {choice}}||exercise-66=={If you have 10,000,000 examples, how would you split the train/dev/test set? \begin {choice}(1) * \correctchoice {98\% train. 1\% dev. 1\% test} * 33\% train. 33\% dev. 33\% test * 60\% train. 20\% dev. 20\% test \end {choice}}||exercise-67=={The dev and test set should: \begin {choice}(1) * \correctchoice {Come from the same distribution} * Come from different distributions * Be identical to each other(same \((x,y)\) pairs) * Have the same number of examples \end {choice}}||exercise-68=={If your Neural Network model seems to have high variance, what of the following would be promising things to try? (choose all that apply) \begin {choice} (2) * Make the Neural network deeper * \correctchoice {Get more training data} * Get more test data * Increase the number of units in each hidden layer * \correctchoicetwo {Add regularization} \end {choice}}||exercise-69=={You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5\% and a dev set error of 7\%. Which of the following are promising things to try to improve your classifier? (Check all that apply) \begin {choice}(1) * \correctchoice {Increase the regularization parameter lambda} * decrease the regularization parameter lambda * \correctchoicetwo {get more training data} * use a bigger neural network \end {choice}}||exercise-70=={What happens when you increase the regularization hyperparameter lambda? \begin {choice} * \correctchoice {Weights are pushed twoard becoming smaller (closer to 0)} * weights are pushed toward becoming bigger (further from 0) * doubling lambda should roughly result in doubling the weights * Gradient descent taking bigger steps with each iteration (proportional to lambda) \end {choice}}||exercise-71=={With the inverted dropout, at test time: \begin {choice} * You don't apply dropout (do not randomly eliminate units), but keep \texttt {1/keep\_prob} factor in the calculations used in training * \correctchoice {You don't apply dropout (do not randomly eliminate units) and do not keep the \texttt {1/keep\_prob} factor in the calculations usd in the training} * You apply dropout (randomly eliminate units) but keep \texttt {1/keep\_prob} factor in the calculations used in training * You apply dropout (randomly eliminate units) and do not keep \texttt {1/keep\_prob} factor in the calculations used in training \end {choice}}||exercise-72=={Which of these techniques are useful for reducing variance (reduce overfitting)? (check all that apply) \begin {choice} (3) * \correctchoice {Dropout} * Gradient Checking * \correctchoicetwo {Data augmentation} * Vanishing gradient * Xavier initialization * \correctchoicethree {L2 regularization} * Exploding gradient \end {choice}}||exercise-73=={Why do we normalize the inputs \(x\)? \begin {choice} * Normalization is another word for regularization--it helps to reduce variance * \correctchoice {It makes the cost function faster to optimize} * It makes it easier to visualize the data. * It makes the parameter initialization faster. \end {choice}}||exercise-74=={What is the role of the temperature parameter in the context of knowledge distillation as a form of regularization? \begin {choice} (1) * Controls the learning rate * Adjusts the level of noise in the input data * \correctchoice {Regulates the softness of the target distribution} * Sets the threshold for dropout during training \end {choice}}||exercise-75=={In the context of neural networks, what does dropout rate refer to? \begin {choice}(1) * The percentage of training samples used during each iteration * The rate at which weight are decayed during training * \correctchoice {The probability of dropping out a unit in the hidden layers during training} * The learning rate for stochastic gradient descent. \end {choice}}||exercise-76=={Which of the following is a technique used for dynamic adjustment of the learning rate during training to improve convergence in deep learning? \begin {choice}(2) * Adversarial training * \correctchoice {Learning rate annealing} * Batch Normalization * Feature Scaling \end {choice}}||exercise-77=={What is the purpose of adding noise to the input data as a form of regularization? \begin {choice} (1) * To make the training process deterministic * To improve model interpretability * \correctchoice {To reduce the impact of outliers in the input data} * To prevent the model from memorizing the training data \end {choice}}||exercise-78=={In the context of regularization, what does the term "shrinkage" refer to? \begin {choice} (1) * Reducing the size of the input data * Reducing the number of hidden layers in the network * \correctchoice {Constraining the magnitude of the weights in the model} * Eliminating unnecessary features from the dataset \end {choice}}||exercise-79=={Which of the following statements is true about the dropout technique? \begin {choice} (1) * Dropout is more effective in shallow networks than deep networks * Dropout can be applied only to input layers * Dropout introduces random variations only during testing * \correctchoice {Dropout helps prevent co-adaptation of hidden units} \end {choice}}||exercise-80=={What is the primary goal of ensemble methods in machine learning? \begin {choice}(1) * To reduce the computational complexity of models * To increase the training time of individual models * \correctchoice {To improve the predictive performance of a model by combining multiple models} * To decrease the diversity among base models \end {choice}}||exercise-81=={Which of the following statements is true about bagging (Bootstrap Aggregating)? \begin {choice} (1) * It trains multiple models sequentially. * \correctchoice {It trains multiple models independently on different subsets of the training data.} * It combines models using a weighted average. * It is not suitable for high-variance models. \end {choice}}||exercise-82=={What is the purpose of random forests in ensemble learning? \begin {choice}(1) * To create a forest of decision trees with high correlation * To reduce the number of trees in the ensemble * \correctchoice {To introduce randomness by considering a random subset of features for each tree} * To eliminate the need for decision trees in the ensemble \end {choice}}||exercise-83=={In boosting, how are the weights assigned to misclassified instances during training? \begin {choice} (1) * Equally to all instances * Proportional to the difficulty of the instance * \correctchoice {Sequentially, with higher weights for misclassified instances} * Inversely proportional to the number of features \end {choice}}||exercise-84=={Which ensemble method combines the predictions of base models by taking a weighted average, where the weights are learned based on the performance of each model? \begin {choice}(4) * Bagging * \correctchoice {Stacking} * Boosting * Random Forest \end {choice}}||exercise-85=={What is the primary advantage of ensemble methods over individual base models? \begin {choice} * Ensemble methods are always faster than individual models. * Ensemble methods can handle only linear relationships. * \correctchoice {Ensemble methods often generalize better and have improved robustness.} * Ensemble methods are more prone to overfitting. \end {choice}}||exercise-86=={In the context of boosting, what does the term "weak learner" refer to? \begin {choice} * A model with high training accuracy * \correctchoice {A model that performs slightly better than random chance} * A model with a large number of parameters * A model that is highly overfit \end {choice}}||exercise-87=={Which ensemble method trains multiple models independently on different subsets of the training data? \begin {choice}(4) * Boosting * Stacking * \correctchoice {Bagging} * Random Forest \end {choice}}||exercise-88=={What is bagging short for in the context of ensemble methods? \par \begin {choice}(4) * \correctchoice {Bootstrap Aggregating} * Boosting Algorithm * Bagged Aggregation * Batch Aggregation \end {choice}}||exercise-89=={Which ensemble method is known for building a sequence of weak learners, each correcting the errors of its predecessor? \begin {choice} (4) * Bagging * \correctchoice {AdaBoost} * Random Forest * Gradient Boosting \end {choice}}||exercise-90=={What is the primary advantage of ensemble methods over individual base models? \begin {choice} * Faster training time * \correctchoice {Improved generalization and robustness} * Lower computational complexity * Higher sensitivity to outliers \end {choice}}||exercise-91=={Which ensemble method is based on constructing a forest of decision trees with high diversity? \begin {choice}(4) * Bagging * AdaBoost * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-92=={What does the acronym "LSTM" stand for in the context of deep learning? \begin {choice}(2) * \correctchoice {Long Short-Term Memory} * Linear Short-Term Memory * Limited Short-Term Memory * Lasting Short-Term Memory \end {choice}}||exercise-93=={In boosting, what is the purpose of the learning rate parameter? \begin {choice} * It controls the number of weak learners \correctchoice {It adjusts the amount by which weights are updated during each iteration} * It determines the depth of decision trees * It sets the threshold for feature selection \end {choice}}||exercise-94=={What distinguishes Random Forest from traditional bagging techniques? \begin {choice} * Random Forest uses a single decision tree * Random Forest trains models sequentially * \correctchoice {Random Forest introduces randomness by considering a random subset of features for each tree} * Random Forest assigns equal weights to all instances \end {choice}}||exercise-95=={How does stacking differ from bagging and boosting in ensemble methods? \begin {choice} * Stacking trains models independently on different subsets * Stacking combines predictions using a weighted average * Stacking builds a sequence of weak learners * \correctchoice {Stacking uses multiple base models to form a meta-model} \end {choice}}||exercise-96=={What role does the concept of "bias-variance tradeoff" play in ensemble methods? \begin {choice} (1) * Ensemble methods eliminate the bias-variance tradeoff * Ensemble methods intensify the bias-variance tradeoff * \correctchoice {Ensemble methods help balance bias and variance} * Ensemble methods have no impact on bias and variance \end {choice}}||exercise-97=={What is the primary limitation of using too many weak learners in boosting? \begin {choice} (2) * \correctchoice {Increased risk of overfitting} * Decreased computational complexity * Improved generalization * Faster training time \end {choice}}||exercise-98=={In bagging, how are the subsets of the training data created for each base model? \begin {choice} * \correctchoice {Randomly and with replacement} * Randomly and without replacement * Sequentially and with replacement * Sequentially and without replacement \end {choice}}||exercise-99=={What is the primary advantage of using gradient boosting over traditional AdaBoost? \begin {choice} (2) * Faster convergence * \correctchoice {Better handling of outliers} * Reduced risk of overfitting * Simplicity in implementation \end {choice}}||exercise-100=={Which ensemble method is prone to becoming computationally expensive as the number of models increases? \begin {choice} (4) * Bagging * Stacking * \correctchoice {Boosting} * Random Forest \end {choice}}||exercise-101=={What does the term "stacking" refer to in ensemble learning? \begin {choice} * Combining models using a weighted average * Training models independently on different subsets * Constructing a sequence of weak learners * \correctchoice {Using multiple base models to form a meta-model} \end {choice}}||exercise-102=={Which ensemble method is known for its ability to handle both linear and non-linear relationships in the data? \begin {choice}(4) * Bagging * Stacking * \correctchoice {Random Forest} * Gradient Boosting \end {choice}}||exercise-103=={Explain the concept of "out-of-bag" error in the context of bagging. \begin {choice} * It is the error rate calculated on the training set * It is the error rate on the validation set * \correctchoice {It is an estimate of the test error obtained from the unused samples during training} * It is a measure of the model's performance on out-of-distribution data \end {choice}}||exercise-104=={What is the role of the hyperparameter "max depth" in decision trees within a Random Forest? \begin {choice} * It controls the number of trees in the forest * \correctchoice {It limits the maximum depth of individual decision trees} * It sets the learning rate for boosting * It adjusts the weights assigned to misclassified instances \end {choice}}||exercise-105=={In the context of ensemble methods, what is "early stopping," and how does it contribute to regularization? \begin {choice} * Early stopping involves terminating the training process when the model is underfitting, contributing to model simplicity. * \correctchoice {Early stopping prevents overfitting by stopping the training process when the model starts to memorize the training data.} * Early stopping introduces noise to the input data during training, preventing overfitting. * Early stopping is not related to regularization in ensemble methods. \end {choice}}||exercise-106=={What is the impact of increasing the number of base models on the computational complexity of stacking? \begin {choice} * The computational complexity decreases linearly * \correctchoice {The computational complexity increases linearly} * The computational complexity remains constant * The computational complexity depends on the type of base models used \end {choice}}||exercise-107=={Explain the concept of "adversarial training" in the context of ensemble methods. \begin {choice} * \correctchoice {Adversarial training involves training models to be robust against adversarial attacks.} * Adversarial training focuses on maximizing the accuracy on the training set. * Adversarial training eliminates the need for ensemble methods. * Adversarial training refers to using adversarial examples as additional training data. \end {choice}}||exercise-108=={How does the concept of "stacking with cross-validation" address the risk of overfitting in stacking? \begin {choice} * It eliminates the need for cross-validation in stacking. * \correctchoice {It uses multiple cross-validated models, reducing overfitting.} * It increases the depth of individual base models. * It has no impact on the risk of overfitting. \end {choice}}||exercise-109=={What is the primary drawback of using a high learning rate in boosting algorithms? \begin {choice}(2) * Slower convergence * \correctchoice {Increased risk of overfitting} * Decreased model performance * Improved generalization \end {choice}}||exercise-110=={Explain the concept of "feature importance" in the context of Random Forest. \begin {choice} * Feature importance represents the number of times a feature is selected by a base model. * \correctchoice {Feature importance indicates the relevance of a feature in predicting the target variable.} * Feature importance is not applicable to ensemble methods. * Feature importance measures the computational cost of using a specific feature. \end {choice}}||exercise-111=={What is the role of the "n estimators" hyperparameter in ensemble methods such as Random Forest and Gradient Boosting? \begin {choice} * It controls the learning rate in boosting algorithms. * It sets the maximum depth of individual decision trees. * \correctchoice {It specifies the number of base models in the ensemble.} * It determines the subset of features considered for each base model. \end {choice}}||exercise-112=={Explain the concept of "stacking with meta-features" in the context of ensemble methods. \begin {choice} * \correctchoice {Stacking with meta-features involves using the output of base models as features for a meta-model.} * Stacking with meta-features eliminates the need for multiple base models. * Stacking with meta-features refers to combining models using a weighted average. * Stacking with meta-features involves using only one type of base model in the ensemble. \end {choice}}||exercise-113=={What is Dropout in the context of neural networks? \begin {choice} * Adding noise to input features * \correctchoice {Removing random neurons during training} * Reducing the learning rate * Increasing the number of hidden layers \end {choice}}||exercise-114=={What is the main purpose of Dropout in neural networks? \begin {choice} * To increase overfitting * To speed up the training process * \correctchoice {To prevent co-adaptation of neurons} * To eliminate the need for activation functions \end {choice}}||exercise-115=={Which of the following statements is true about the application of Dropout during training? \begin {choice} * Dropout is only applied to input layers * \correctchoice {Dropout is applied to all layers except the output layer} * Dropout is applied during both training and testing * Dropout is never applied to neural networks \end {choice}}||exercise-116=={How does Dropout contribute to regularization in neural networks? \begin {choice} * By increasing the number of parameters * By introducing noise to the input data * \correctchoice {By reducing the model's capacity} * By promoting co-adaptation of neurons \end {choice}}||exercise-117=={In terms of training, what does it mean if a neuron is "dropped out"? \begin {choice} * The neuron's weights are set to zero * \correctchoice {The neuron is removed from the network temporarily} * The neuron's activation function is bypassed * The neuron's output is squared \end {choice}}||exercise-118=={What challenge does Dropout aim to address in neural networks? \begin {choice} (4) * Underfitting * \correctchoice {Overfitting} * Vanishing gradients * Exploding gradients \end {choice}}||exercise-119=={How does Dropout affect the training time of a neural network? \begin {choice} * \correctchoice {Slows down the training process} * Speeds up the training process * No impact on training time * Depends on the type of activation function used \end {choice}}||exercise-120=={What is the recommended range for Dropout rates in neural networks? \begin {choice} (4) * 0.0 to 0.1 * \correctchoice {0.2 to 0.5} * 0.5 to 0.8 * 0.9 to 1.0 \end {choice}}||exercise-121=={How does Dropout contribute to model generalization? \begin {choice} * By memorizing the training data * By promoting co-adaptation of neurons * \correctchoice {By reducing the sensitivity of neurons to specific input features} * By increasing the number of hidden layers \end {choice}}||exercise-122=={When applying Dropout, which phase is used for adjusting the weights of the neural network? \begin {choice} * \correctchoice {Training phase} * Testing phase * Both training and testing phases * Neither training nor testing phases \end {choice}}||exercise-123=={Explain the term "co-adaptation of neurons" in the context of neural networks and how Dropout addresses it. \begin {choice} * \correctchoice {Co-adaptation refers to neurons relying too much on each other, and Dropout breaks these dependencies by randomly dropping neurons during training.} * Co-adaptation is a form of regularization, and Dropout exacerbates co-adaptation by introducing noise. * Co-adaptation occurs when neurons are independent, and Dropout enforces co-adaptation by removing dependencies. * Co-adaptation is unrelated to Dropout; Dropout only affects the learning rate. \end {choice}}||exercise-124=={How does the effectiveness of Dropout vary with the size and complexity of a neural network? \begin {choice} * Dropout is more effective in small and simple networks * \correctchoice {Dropout is more effective in large and complex networks} * Dropout is equally effective across all network sizes and complexities * Dropout is irrelevant to network size and complexity \end {choice}}||exercise-125=={What is the relationship between Dropout and the concept of ensemble learning? \begin {choice} * Dropout is a type of ensemble learning * Ensemble learning and Dropout are unrelated concepts * \correctchoice {Dropout and ensemble learning achieve the same result in terms of model diversity} * Dropout eliminates the need for ensemble learning \end {choice}}||exercise-126=={Explain the trade-off between using a high Dropout rate and a low Dropout rate in neural networks. \begin {choice} * \correctchoice {High Dropout rates lead to overfitting, while low Dropout rates may result in underfitting.} * High Dropout rates always improve model generalization, while low Dropout rates reduce model capacity. * There is no trade-off; the Dropout rate does not impact model performance. * The trade-off depends on the type of activation function used in the network. \end {choice}}||exercise-127=={How does Dropout contribute to mitigating the vanishing gradient problem in deep neural networks? \begin {choice} * a. By increasing the learning rate * By preventing co-adaptation of neurons * \correctchoice {By introducing noise to the input data} * By reducing the sensitivity of neurons to specific input features \end {choice}}||exercise-128=={What is the primary goal of data augmentation in machine learning? \begin {choice} * To decrease the size of the dataset * To increase the computational complexity * \correctchoice {To improve model performance by increasing the diversity of the training data} * To eliminate the need for validation data \end {choice}}||exercise-129=={Which of the following is a common technique used in data augmentation for image data? \begin {choice} (2) * Principal Component Analysis (PCA) * Feature scaling * \correctchoice {Image rotation} * Lasso regularization \end {choice}}||exercise-130=={How does data augmentation contribute to preventing overfitting in machine learning models? \begin {choice} * By reducing the size of the training dataset * By increasing the number of layers in the model * By introducing noise to the input data * \correctchoice {By providing a more diverse set of training examples} \end {choice}}||exercise-131=={In text data augmentation, what technique involves replacing words with their synonyms? \begin {choice}(4) * Tokenization * Embedding * \correctchoice {Word substitution} * Lemmatization \end {choice}}||exercise-132=={Which of the following is a disadvantage of data augmentation? \begin {choice} * Increased model generalization * \correctchoice {Potential introduction of unrealistic patterns} * Improved model robustness * Decreased computational efficiency \end {choice}}||exercise-133=={What is the purpose of random cropping in image data augmentation? \begin {choice} * To decrease the image resolution * To remove irrelevant features from the image * \correctchoice {To create variations in the spatial location of objects} * To increase the image contrast \end {choice}}||exercise-134=={Which type of data augmentation is commonly used for time series data? \begin {choice}(4) * Image rotation * \correctchoice {Time warping} * Word substitution * Feature scaling \end {choice}}||exercise-135=={Explain the concept of "jittering" in the context of data augmentation. \begin {choice} * \correctchoice {Jittering refers to the introduction of noise to input features} * Jittering involves the random selection of a subset of data points * Jittering is a synonym for image rotation * Jittering is irrelevant to data augmentation \end {choice}}||exercise-136=={In the context of image data augmentation, what is the purpose of horizontal flipping? \begin {choice} (2) * To rotate images clockwise * \correctchoice {To create mirror images} * To adjust the image brightness * To resize images \end {choice}}||exercise-137=={How does data augmentation differ from feature engineering? \begin {choice} * \correctchoice {Data augmentation focuses on creating new samples, while feature engineering manipulates existing features.} * Feature engineering is limited to image data, while data augmentation is applicable to all data types. * Data augmentation involves scaling features, while feature engineering involves randomization. * Feature engineering and data augmentation are synonymous terms. \end {choice}}||exercise-138=={What is the role of dropout in the context of data augmentation? \begin {choice} (1) * Dropout is not related to data augmentation * \correctchoice {Dropout enhances data augmentation by randomly removing features during training} * Dropout is a type of data augmentation technique * Dropout prevents data augmentation from introducing unrealistic patterns \end {choice}}||exercise-139=={Which data augmentation technique is commonly used for audio data to introduce variations in pitch? \begin {choice} (2) * Time warping * \correctchoice {Spectrogram augmentation} * Random cropping * Jittering \end {choice}}||exercise-140=={What is the purpose of elastic deformation in image data augmentation? \begin {choice} * To adjust the image contrast * \correctchoice {To introduce non-linear distortions to the image} * To resize the image * To rotate the image \end {choice}}||exercise-141=={In natural language processing, which technique involves randomly removing words from sentences during data augmentation? \begin {choice} (1) * Tokenization * Word substitution * Sentence splitting * \correctchoice {Sentence dropout} \end {choice}}||exercise-142=={Explain the concept of "adversarial training" in the context of data augmentation and how it addresses robustness. \begin {choice} (1) * \correctchoice {Adversarial training focuses on creating adversarial examples to test the model's robustness against unseen patterns introduced by data augmentation.} * Adversarial training is irrelevant to data augmentation. * Adversarial training involves increasing the size of the training set. * Adversarial training enhances data augmentation by introducing adversarial noise during the augmentation process. \end {choice}}||exercise-143=={How does data augmentation contribute to handling class imbalance in classification tasks? \begin {choice} (1) * Data augmentation exacerbates class imbalance * Data augmentation is not related to class imbalance * \correctchoice {Data augmentation generates additional samples for minority classes, addressing class imbalance} * Data augmentation reduces the need for addressing class imbalance \end {choice}}||exercise-144=={What challenges might arise when applying data augmentation to non-image data types, such as tabular data? \begin {choice}(1) * Difficulty in implementing data augmentation for non-image data * Limited applicability of data augmentation to non-image data * \correctchoice {The potential introduction of unrealistic patterns} * No challenges; data augmentation is equally effective for all data types \end {choice}}||exercise-145=={Explain the term "mixup" in the context of data augmentation and how it differs from traditional augmentation techniques. \begin {choice}(1) * \correctchoice {Mixup involves blending two or more samples, creating new synthetic samples with averaged labels.} * Mixup is a synonym for image rotation. * Mixup refers to the addition of random noise to input features. * Mixup is irrelevant to data augmentation. \end {choice}}||exercise-146=={How does data augmentation impact the interpretability of machine learning models? \begin {choice} (1) * Data augmentation improves model interpretability by providing more diverse training examples. * Data augmentation has no impact on model interpretability. * \correctchoice {Data augmentation reduces model interpretability due to the introduction of synthetic samples.} * Data augmentation improves model interpretability by eliminating the need for validation data. \end {choice}}||exercise-147=={What is the role of "cutout" in image data augmentation? \begin {choice} (1) * \correctchoice {To remove random portions from images} * To blur the edges of images * To rotate images * To resize images \end {choice}}||exercise-148=={In the context of data augmentation, explain how the technique of "shearing" is applied to image data. \begin {choice} (1) * Shearing involves adjusting the brightness of images. * Shearing is irrelevant to data augmentation. * \correctchoice {Shearing introduces non-linear distortions to the image by tilting it along one of its axes.} * Shearing is a synonym for image rotation. \end {choice}}||exercise-149=={Which ensemble learning algorithm can be applied to both regression and classification tasks? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-150=={Ensemble learning algorithms can be computationally expensive when: \begin {choice} (2) * The dataset is small * The base models are simple * The ensemble size is small * \correctchoice {The dataset is large} \end {choice}}||exercise-151=={Which ensemble learning algorithm can be used to identify important features in a dataset? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7||8||9||10||11||12||13||14||15||16||17||18||19||20||21||22||23||24||25||26||27||28||29||30||31||32||33||34||35||36||37||38||39||40||41||42||43||44||45||46||47||48||49||50||51||52||53||54||55||56||57||58||59||60||61||62||63||64||65||66||67||68||69||70||71||72||73||74||75||76||77||78||79||80||81||82||83||84||85||86||87||88||89||90||91||92||93||94||95||96||97||98||99||100||101||102||103||104||105||106||107||108||109||110||111||112||113||114||115||116||117||118||119||120||121||122||123||124||125||126||127||128||129||130||131||132||133||134||135||136||137||138||139||140||141||142||143||144||145||146||147||148||149||150||151}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}||exercise-79=={true}||exercise-80=={true}||exercise-81=={true}||exercise-82=={true}||exercise-83=={true}||exercise-84=={true}||exercise-85=={true}||exercise-86=={true}||exercise-87=={true}||exercise-88=={true}||exercise-89=={true}||exercise-90=={true}||exercise-91=={true}||exercise-92=={true}||exercise-93=={true}||exercise-94=={true}||exercise-95=={true}||exercise-96=={true}||exercise-97=={true}||exercise-98=={true}||exercise-99=={true}||exercise-100=={true}||exercise-101=={true}||exercise-102=={true}||exercise-103=={true}||exercise-104=={true}||exercise-105=={true}||exercise-106=={true}||exercise-107=={true}||exercise-108=={true}||exercise-109=={true}||exercise-110=={true}||exercise-111=={true}||exercise-112=={true}||exercise-113=={true}||exercise-114=={true}||exercise-115=={true}||exercise-116=={true}||exercise-117=={true}||exercise-118=={true}||exercise-119=={true}||exercise-120=={true}||exercise-121=={true}||exercise-122=={true}||exercise-123=={true}||exercise-124=={true}||exercise-125=={true}||exercise-126=={true}||exercise-127=={true}||exercise-128=={true}||exercise-129=={true}||exercise-130=={true}||exercise-131=={true}||exercise-132=={true}||exercise-133=={true}||exercise-134=={true}||exercise-135=={true}||exercise-136=={true}||exercise-137=={true}||exercise-138=={true}||exercise-139=={true}||exercise-140=={true}||exercise-141=={true}||exercise-142=={true}||exercise-143=={true}||exercise-144=={true}||exercise-145=={true}||exercise-146=={true}||exercise-147=={true}||exercise-148=={true}||exercise-149=={true}||exercise-150=={true}||exercise-151=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}||exercise-79=={true}||exercise-80=={true}||exercise-81=={true}||exercise-82=={true}||exercise-83=={true}||exercise-84=={true}||exercise-85=={true}||exercise-86=={true}||exercise-87=={true}||exercise-88=={true}||exercise-89=={true}||exercise-90=={true}||exercise-91=={true}||exercise-92=={true}||exercise-93=={true}||exercise-94=={true}||exercise-95=={true}||exercise-96=={true}||exercise-97=={true}||exercise-98=={true}||exercise-99=={true}||exercise-100=={true}||exercise-101=={true}||exercise-102=={true}||exercise-103=={true}||exercise-104=={true}||exercise-105=={true}||exercise-106=={true}||exercise-107=={true}||exercise-108=={true}||exercise-109=={true}||exercise-110=={true}||exercise-111=={true}||exercise-112=={true}||exercise-113=={true}||exercise-114=={true}||exercise-115=={true}||exercise-116=={true}||exercise-117=={true}||exercise-118=={true}||exercise-119=={true}||exercise-120=={true}||exercise-121=={true}||exercise-122=={true}||exercise-123=={true}||exercise-124=={true}||exercise-125=={true}||exercise-126=={true}||exercise-127=={true}||exercise-128=={true}||exercise-129=={true}||exercise-130=={true}||exercise-131=={true}||exercise-132=={true}||exercise-133=={true}||exercise-134=={true}||exercise-135=={true}||exercise-136=={true}||exercise-137=={true}||exercise-138=={true}||exercise-139=={true}||exercise-140=={true}||exercise-141=={true}||exercise-142=={true}||exercise-143=={true}||exercise-144=={true}||exercise-145=={true}||exercise-146=={true}||exercise-147=={true}||exercise-148=={true}||exercise-149=={true}||exercise-150=={true}||exercise-151=={true}}
\XSIM{total-number}{151}
\XSIM{exercise}{151}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}||8=={exercise}||9=={exercise}||10=={exercise}||11=={exercise}||12=={exercise}||13=={exercise}||14=={exercise}||15=={exercise}||16=={exercise}||17=={exercise}||18=={exercise}||19=={exercise}||20=={exercise}||21=={exercise}||22=={exercise}||23=={exercise}||24=={exercise}||25=={exercise}||26=={exercise}||27=={exercise}||28=={exercise}||29=={exercise}||30=={exercise}||31=={exercise}||32=={exercise}||33=={exercise}||34=={exercise}||35=={exercise}||36=={exercise}||37=={exercise}||38=={exercise}||39=={exercise}||40=={exercise}||41=={exercise}||42=={exercise}||43=={exercise}||44=={exercise}||45=={exercise}||46=={exercise}||47=={exercise}||48=={exercise}||49=={exercise}||50=={exercise}||51=={exercise}||52=={exercise}||53=={exercise}||54=={exercise}||55=={exercise}||56=={exercise}||57=={exercise}||58=={exercise}||59=={exercise}||60=={exercise}||61=={exercise}||62=={exercise}||63=={exercise}||64=={exercise}||65=={exercise}||66=={exercise}||67=={exercise}||68=={exercise}||69=={exercise}||70=={exercise}||71=={exercise}||72=={exercise}||73=={exercise}||74=={exercise}||75=={exercise}||76=={exercise}||77=={exercise}||78=={exercise}||79=={exercise}||80=={exercise}||81=={exercise}||82=={exercise}||83=={exercise}||84=={exercise}||85=={exercise}||86=={exercise}||87=={exercise}||88=={exercise}||89=={exercise}||90=={exercise}||91=={exercise}||92=={exercise}||93=={exercise}||94=={exercise}||95=={exercise}||96=={exercise}||97=={exercise}||98=={exercise}||99=={exercise}||100=={exercise}||101=={exercise}||102=={exercise}||103=={exercise}||104=={exercise}||105=={exercise}||106=={exercise}||107=={exercise}||108=={exercise}||109=={exercise}||110=={exercise}||111=={exercise}||112=={exercise}||113=={exercise}||114=={exercise}||115=={exercise}||116=={exercise}||117=={exercise}||118=={exercise}||119=={exercise}||120=={exercise}||121=={exercise}||122=={exercise}||123=={exercise}||124=={exercise}||125=={exercise}||126=={exercise}||127=={exercise}||128=={exercise}||129=={exercise}||130=={exercise}||131=={exercise}||132=={exercise}||133=={exercise}||134=={exercise}||135=={exercise}||136=={exercise}||137=={exercise}||138=={exercise}||139=={exercise}||140=={exercise}||141=={exercise}||142=={exercise}||143=={exercise}||144=={exercise}||145=={exercise}||146=={exercise}||147=={exercise}||148=={exercise}||149=={exercise}||150=={exercise}||151=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}||exercise-8=={all exercises}||exercise-9=={all exercises}||exercise-10=={all exercises}||exercise-11=={all exercises}||exercise-12=={all exercises}||exercise-13=={all exercises}||exercise-14=={all exercises}||exercise-15=={all exercises}||exercise-16=={all exercises}||exercise-17=={all exercises}||exercise-18=={all exercises}||exercise-19=={all exercises}||exercise-20=={all exercises}||exercise-21=={all exercises}||exercise-22=={all exercises}||exercise-23=={all exercises}||exercise-24=={all exercises}||exercise-25=={all exercises}||exercise-26=={all exercises}||exercise-27=={all exercises}||exercise-28=={all exercises}||exercise-29=={all exercises}||exercise-30=={all exercises}||exercise-31=={all exercises}||exercise-32=={all exercises}||exercise-33=={all exercises}||exercise-34=={all exercises}||exercise-35=={all exercises}||exercise-36=={all exercises}||exercise-37=={all exercises}||exercise-38=={all exercises}||exercise-39=={all exercises}||exercise-40=={all exercises}||exercise-41=={all exercises}||exercise-42=={all exercises}||exercise-43=={all exercises}||exercise-44=={all exercises}||exercise-45=={all exercises}||exercise-46=={all exercises}||exercise-47=={all exercises}||exercise-48=={all exercises}||exercise-49=={all exercises}||exercise-50=={all exercises}||exercise-51=={all exercises}||exercise-52=={all exercises}||exercise-53=={all exercises}||exercise-54=={all exercises}||exercise-55=={all exercises}||exercise-56=={all exercises}||exercise-57=={all exercises}||exercise-58=={all exercises}||exercise-59=={all exercises}||exercise-60=={all exercises}||exercise-61=={all exercises}||exercise-62=={all exercises}||exercise-63=={all exercises}||exercise-64=={all exercises}||exercise-65=={all exercises}||exercise-66=={all exercises}||exercise-67=={all exercises}||exercise-68=={all exercises}||exercise-69=={all exercises}||exercise-70=={all exercises}||exercise-71=={all exercises}||exercise-72=={all exercises}||exercise-73=={all exercises}||exercise-74=={all exercises}||exercise-75=={all exercises}||exercise-76=={all exercises}||exercise-77=={all exercises}||exercise-78=={all exercises}||exercise-79=={all exercises}||exercise-80=={all exercises}||exercise-81=={all exercises}||exercise-82=={all exercises}||exercise-83=={all exercises}||exercise-84=={all exercises}||exercise-85=={all exercises}||exercise-86=={all exercises}||exercise-87=={all exercises}||exercise-88=={all exercises}||exercise-89=={all exercises}||exercise-90=={all exercises}||exercise-91=={all exercises}||exercise-92=={all exercises}||exercise-93=={all exercises}||exercise-94=={all exercises}||exercise-95=={all exercises}||exercise-96=={all exercises}||exercise-97=={all exercises}||exercise-98=={all exercises}||exercise-99=={all exercises}||exercise-100=={all exercises}||exercise-101=={all exercises}||exercise-102=={all exercises}||exercise-103=={all exercises}||exercise-104=={all exercises}||exercise-105=={all exercises}||exercise-106=={all exercises}||exercise-107=={all exercises}||exercise-108=={all exercises}||exercise-109=={all exercises}||exercise-110=={all exercises}||exercise-111=={all exercises}||exercise-112=={all exercises}||exercise-113=={all exercises}||exercise-114=={all exercises}||exercise-115=={all exercises}||exercise-116=={all exercises}||exercise-117=={all exercises}||exercise-118=={all exercises}||exercise-119=={all exercises}||exercise-120=={all exercises}||exercise-121=={all exercises}||exercise-122=={all exercises}||exercise-123=={all exercises}||exercise-124=={all exercises}||exercise-125=={all exercises}||exercise-126=={all exercises}||exercise-127=={all exercises}||exercise-128=={all exercises}||exercise-129=={all exercises}||exercise-130=={all exercises}||exercise-131=={all exercises}||exercise-132=={all exercises}||exercise-133=={all exercises}||exercise-134=={all exercises}||exercise-135=={all exercises}||exercise-136=={all exercises}||exercise-137=={all exercises}||exercise-138=={all exercises}||exercise-139=={all exercises}||exercise-140=={all exercises}||exercise-141=={all exercises}||exercise-142=={all exercises}||exercise-143=={all exercises}||exercise-144=={all exercises}||exercise-145=={all exercises}||exercise-146=={all exercises}||exercise-147=={all exercises}||exercise-148=={all exercises}||exercise-149=={all exercises}||exercise-150=={all exercises}||exercise-151=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11||exercise-12||exercise-13||exercise-14||exercise-15||exercise-16||exercise-17||exercise-18||exercise-19||exercise-20||exercise-21||exercise-22||exercise-23||exercise-24||exercise-25||exercise-26||exercise-27||exercise-28||exercise-29||exercise-30||exercise-31||exercise-32||exercise-33||exercise-34||exercise-35||exercise-36||exercise-37||exercise-38||exercise-39||exercise-40||exercise-41||exercise-42||exercise-43||exercise-44||exercise-45||exercise-46||exercise-47||exercise-48||exercise-49||exercise-50||exercise-51||exercise-52||exercise-53||exercise-54||exercise-55||exercise-56||exercise-57||exercise-58||exercise-59||exercise-60||exercise-61||exercise-62||exercise-63||exercise-64||exercise-65||exercise-66||exercise-67||exercise-68||exercise-69||exercise-70||exercise-71||exercise-72||exercise-73||exercise-74||exercise-75||exercise-76||exercise-77||exercise-78||exercise-79||exercise-80||exercise-81||exercise-82||exercise-83||exercise-84||exercise-85||exercise-86||exercise-87||exercise-88||exercise-89||exercise-90||exercise-91||exercise-92||exercise-93||exercise-94||exercise-95||exercise-96||exercise-97||exercise-98||exercise-99||exercise-100||exercise-101||exercise-102||exercise-103||exercise-104||exercise-105||exercise-106||exercise-107||exercise-108||exercise-109||exercise-110||exercise-111||exercise-112||exercise-113||exercise-114||exercise-115||exercise-116||exercise-117||exercise-118||exercise-119||exercise-120||exercise-121||exercise-122||exercise-123||exercise-124||exercise-125||exercise-126||exercise-127||exercise-128||exercise-129||exercise-130||exercise-131||exercise-132||exercise-133||exercise-134||exercise-135||exercise-136||exercise-137||exercise-138||exercise-139||exercise-140||exercise-141||exercise-142||exercise-143||exercise-144||exercise-145||exercise-146||exercise-147||exercise-148||exercise-149||exercise-150||exercise-151}
\setcounter{totalexerciseinall exercises}{151}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}}
\XSIM{counter}{exercise-1=={0.1}||exercise-2=={0.2}||exercise-3=={0.3}||exercise-4=={0.4}||exercise-5=={0.5}||exercise-6=={0.6}||exercise-7=={0.7}||exercise-8=={0.8}||exercise-9=={0.9}||exercise-10=={0.10}||exercise-11=={0.11}||exercise-12=={0.12}||exercise-13=={0.13}||exercise-14=={0.14}||exercise-15=={0.15}||exercise-16=={0.16}||exercise-17=={0.17}||exercise-18=={0.18}||exercise-19=={0.19}||exercise-20=={0.20}||exercise-21=={0.21}||exercise-22=={0.22}||exercise-23=={0.23}||exercise-24=={0.24}||exercise-25=={0.25}||exercise-26=={0.26}||exercise-27=={0.27}||exercise-28=={0.28}||exercise-29=={0.29}||exercise-30=={0.30}||exercise-31=={0.31}||exercise-32=={0.32}||exercise-33=={0.33}||exercise-34=={0.34}||exercise-35=={0.35}||exercise-36=={0.36}||exercise-37=={0.37}||exercise-38=={0.38}||exercise-39=={0.39}||exercise-40=={0.40}||exercise-41=={0.41}||exercise-42=={0.42}||exercise-43=={0.43}||exercise-44=={0.44}||exercise-45=={0.45}||exercise-46=={0.46}||exercise-47=={0.47}||exercise-48=={0.48}||exercise-49=={0.49}||exercise-50=={0.50}||exercise-51=={0.51}||exercise-52=={0.52}||exercise-53=={0.53}||exercise-54=={0.54}||exercise-55=={0.55}||exercise-56=={0.56}||exercise-57=={0.57}||exercise-58=={0.58}||exercise-59=={0.59}||exercise-60=={0.60}||exercise-61=={0.61}||exercise-62=={0.62}||exercise-63=={0.63}||exercise-64=={0.64}||exercise-65=={0.65}||exercise-66=={0.66}||exercise-67=={0.67}||exercise-68=={0.68}||exercise-69=={0.69}||exercise-70=={0.70}||exercise-71=={0.71}||exercise-72=={0.72}||exercise-73=={0.73}||exercise-74=={0.74}||exercise-75=={0.75}||exercise-76=={0.76}||exercise-77=={0.77}||exercise-78=={0.78}||exercise-79=={0.79}||exercise-80=={0.80}||exercise-81=={0.81}||exercise-82=={0.82}||exercise-83=={0.83}||exercise-84=={0.84}||exercise-85=={0.85}||exercise-86=={0.86}||exercise-87=={0.87}||exercise-88=={0.88}||exercise-89=={0.89}||exercise-90=={0.90}||exercise-91=={0.91}||exercise-92=={0.92}||exercise-93=={0.93}||exercise-94=={0.94}||exercise-95=={0.95}||exercise-96=={0.96}||exercise-97=={0.97}||exercise-98=={0.98}||exercise-99=={0.99}||exercise-100=={0.100}||exercise-101=={0.101}||exercise-102=={0.102}||exercise-103=={0.103}||exercise-104=={0.104}||exercise-105=={0.105}||exercise-106=={0.106}||exercise-107=={0.107}||exercise-108=={0.108}||exercise-109=={0.109}||exercise-110=={0.110}||exercise-111=={0.111}||exercise-112=={0.112}||exercise-113=={0.113}||exercise-114=={0.114}||exercise-115=={0.115}||exercise-116=={0.116}||exercise-117=={0.117}||exercise-118=={0.118}||exercise-119=={0.119}||exercise-120=={0.120}||exercise-121=={0.121}||exercise-122=={0.122}||exercise-123=={0.123}||exercise-124=={0.124}||exercise-125=={0.125}||exercise-126=={0.126}||exercise-127=={0.127}||exercise-128=={0.128}||exercise-129=={0.129}||exercise-130=={0.130}||exercise-131=={0.131}||exercise-132=={0.132}||exercise-133=={0.133}||exercise-134=={0.134}||exercise-135=={0.135}||exercise-136=={0.136}||exercise-137=={0.137}||exercise-138=={0.138}||exercise-139=={0.139}||exercise-140=={0.140}||exercise-141=={0.141}||exercise-142=={0.142}||exercise-143=={0.143}||exercise-144=={0.144}||exercise-145=={0.145}||exercise-146=={0.146}||exercise-147=={0.147}||exercise-148=={0.148}||exercise-149=={0.149}||exercise-150=={0.150}||exercise-151=={0.151}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={0}||exercise-2=={0}||exercise-3=={0}||exercise-4=={0}||exercise-5=={0}||exercise-6=={0}||exercise-7=={0}||exercise-8=={0}||exercise-9=={0}||exercise-10=={0}||exercise-11=={0}||exercise-12=={0}||exercise-13=={0}||exercise-14=={0}||exercise-15=={0}||exercise-16=={0}||exercise-17=={0}||exercise-18=={0}||exercise-19=={0}||exercise-20=={0}||exercise-21=={0}||exercise-22=={0}||exercise-23=={0}||exercise-24=={0}||exercise-25=={0}||exercise-26=={0}||exercise-27=={0}||exercise-28=={0}||exercise-29=={0}||exercise-30=={0}||exercise-31=={0}||exercise-32=={0}||exercise-33=={0}||exercise-34=={0}||exercise-35=={0}||exercise-36=={0}||exercise-37=={0}||exercise-38=={0}||exercise-39=={0}||exercise-40=={0}||exercise-41=={0}||exercise-42=={0}||exercise-43=={0}||exercise-44=={0}||exercise-45=={0}||exercise-46=={0}||exercise-47=={0}||exercise-48=={0}||exercise-49=={0}||exercise-50=={0}||exercise-51=={0}||exercise-52=={0}||exercise-53=={0}||exercise-54=={0}||exercise-55=={0}||exercise-56=={0}||exercise-57=={0}||exercise-58=={0}||exercise-59=={0}||exercise-60=={0}||exercise-61=={0}||exercise-62=={0}||exercise-63=={0}||exercise-64=={0}||exercise-65=={0}||exercise-66=={0}||exercise-67=={0}||exercise-68=={0}||exercise-69=={0}||exercise-70=={0}||exercise-71=={0}||exercise-72=={0}||exercise-73=={0}||exercise-74=={0}||exercise-75=={0}||exercise-76=={0}||exercise-77=={0}||exercise-78=={0}||exercise-79=={0}||exercise-80=={0}||exercise-81=={0}||exercise-82=={0}||exercise-83=={0}||exercise-84=={0}||exercise-85=={0}||exercise-86=={0}||exercise-87=={0}||exercise-88=={0}||exercise-89=={0}||exercise-90=={0}||exercise-91=={0}||exercise-92=={0}||exercise-93=={0}||exercise-94=={0}||exercise-95=={0}||exercise-96=={0}||exercise-97=={0}||exercise-98=={0}||exercise-99=={0}||exercise-100=={0}||exercise-101=={0}||exercise-102=={0}||exercise-103=={0}||exercise-104=={0}||exercise-105=={0}||exercise-106=={0}||exercise-107=={0}||exercise-108=={0}||exercise-109=={0}||exercise-110=={0}||exercise-111=={0}||exercise-112=={0}||exercise-113=={0}||exercise-114=={0}||exercise-115=={0}||exercise-116=={0}||exercise-117=={0}||exercise-118=={0}||exercise-119=={0}||exercise-120=={0}||exercise-121=={0}||exercise-122=={0}||exercise-123=={0}||exercise-124=={0}||exercise-125=={0}||exercise-126=={0}||exercise-127=={0}||exercise-128=={0}||exercise-129=={0}||exercise-130=={0}||exercise-131=={0}||exercise-132=={0}||exercise-133=={0}||exercise-134=={0}||exercise-135=={0}||exercise-136=={0}||exercise-137=={0}||exercise-138=={0}||exercise-139=={0}||exercise-140=={0}||exercise-141=={0}||exercise-142=={0}||exercise-143=={0}||exercise-144=={0}||exercise-145=={0}||exercise-146=={0}||exercise-147=={0}||exercise-148=={0}||exercise-149=={0}||exercise-150=={0}||exercise-151=={0}}
\XSIM{section}{exercise-1=={0}||exercise-2=={0}||exercise-3=={0}||exercise-4=={0}||exercise-5=={0}||exercise-6=={0}||exercise-7=={0}||exercise-8=={0}||exercise-9=={0}||exercise-10=={0}||exercise-11=={0}||exercise-12=={0}||exercise-13=={0}||exercise-14=={0}||exercise-15=={0}||exercise-16=={0}||exercise-17=={0}||exercise-18=={0}||exercise-19=={0}||exercise-20=={0}||exercise-21=={0}||exercise-22=={0}||exercise-23=={0}||exercise-24=={0}||exercise-25=={0}||exercise-26=={0}||exercise-27=={0}||exercise-28=={0}||exercise-29=={0}||exercise-30=={0}||exercise-31=={0}||exercise-32=={0}||exercise-33=={0}||exercise-34=={0}||exercise-35=={0}||exercise-36=={0}||exercise-37=={0}||exercise-38=={0}||exercise-39=={0}||exercise-40=={0}||exercise-41=={0}||exercise-42=={0}||exercise-43=={0}||exercise-44=={0}||exercise-45=={0}||exercise-46=={0}||exercise-47=={0}||exercise-48=={0}||exercise-49=={0}||exercise-50=={0}||exercise-51=={0}||exercise-52=={0}||exercise-53=={0}||exercise-54=={0}||exercise-55=={0}||exercise-56=={0}||exercise-57=={0}||exercise-58=={0}||exercise-59=={0}||exercise-60=={0}||exercise-61=={0}||exercise-62=={0}||exercise-63=={0}||exercise-64=={0}||exercise-65=={0}||exercise-66=={0}||exercise-67=={0}||exercise-68=={0}||exercise-69=={0}||exercise-70=={0}||exercise-71=={0}||exercise-72=={0}||exercise-73=={0}||exercise-74=={0}||exercise-75=={0}||exercise-76=={0}||exercise-77=={0}||exercise-78=={0}||exercise-79=={0}||exercise-80=={0}||exercise-81=={0}||exercise-82=={0}||exercise-83=={0}||exercise-84=={0}||exercise-85=={0}||exercise-86=={0}||exercise-87=={0}||exercise-88=={0}||exercise-89=={0}||exercise-90=={0}||exercise-91=={0}||exercise-92=={0}||exercise-93=={0}||exercise-94=={0}||exercise-95=={0}||exercise-96=={0}||exercise-97=={0}||exercise-98=={0}||exercise-99=={0}||exercise-100=={0}||exercise-101=={0}||exercise-102=={0}||exercise-103=={0}||exercise-104=={0}||exercise-105=={0}||exercise-106=={0}||exercise-107=={0}||exercise-108=={0}||exercise-109=={0}||exercise-110=={0}||exercise-111=={0}||exercise-112=={0}||exercise-113=={0}||exercise-114=={0}||exercise-115=={0}||exercise-116=={0}||exercise-117=={0}||exercise-118=={0}||exercise-119=={0}||exercise-120=={0}||exercise-121=={0}||exercise-122=={0}||exercise-123=={0}||exercise-124=={0}||exercise-125=={0}||exercise-126=={0}||exercise-127=={0}||exercise-128=={0}||exercise-129=={0}||exercise-130=={0}||exercise-131=={0}||exercise-132=={0}||exercise-133=={0}||exercise-134=={0}||exercise-135=={0}||exercise-136=={0}||exercise-137=={0}||exercise-138=={0}||exercise-139=={0}||exercise-140=={0}||exercise-141=={0}||exercise-142=={0}||exercise-143=={0}||exercise-144=={0}||exercise-145=={0}||exercise-146=={0}||exercise-147=={0}||exercise-148=={0}||exercise-149=={0}||exercise-150=={0}||exercise-151=={0}}
\XSIM{sectioning}{exercise-1=={{0}{0}{0}{0}{0}}||exercise-2=={{0}{0}{0}{0}{0}}||exercise-3=={{0}{0}{0}{0}{0}}||exercise-4=={{0}{0}{0}{0}{0}}||exercise-5=={{0}{0}{0}{0}{0}}||exercise-6=={{0}{0}{0}{0}{0}}||exercise-7=={{0}{0}{0}{0}{0}}||exercise-8=={{0}{0}{0}{0}{0}}||exercise-9=={{0}{0}{0}{0}{0}}||exercise-10=={{0}{0}{0}{0}{0}}||exercise-11=={{0}{0}{0}{0}{0}}||exercise-12=={{0}{0}{0}{0}{0}}||exercise-13=={{0}{0}{0}{0}{0}}||exercise-14=={{0}{0}{0}{0}{0}}||exercise-15=={{0}{0}{0}{0}{0}}||exercise-16=={{0}{0}{0}{0}{0}}||exercise-17=={{0}{0}{0}{0}{0}}||exercise-18=={{0}{0}{0}{0}{0}}||exercise-19=={{0}{0}{0}{0}{0}}||exercise-20=={{0}{0}{0}{0}{0}}||exercise-21=={{0}{0}{0}{0}{0}}||exercise-22=={{0}{0}{0}{0}{0}}||exercise-23=={{0}{0}{0}{0}{0}}||exercise-24=={{0}{0}{0}{0}{0}}||exercise-25=={{0}{0}{0}{0}{0}}||exercise-26=={{0}{0}{0}{0}{0}}||exercise-27=={{0}{0}{0}{0}{0}}||exercise-28=={{0}{0}{0}{0}{0}}||exercise-29=={{0}{0}{0}{0}{0}}||exercise-30=={{0}{0}{0}{0}{0}}||exercise-31=={{0}{0}{0}{0}{0}}||exercise-32=={{0}{0}{0}{0}{0}}||exercise-33=={{0}{0}{0}{0}{0}}||exercise-34=={{0}{0}{0}{0}{0}}||exercise-35=={{0}{0}{0}{0}{0}}||exercise-36=={{0}{0}{0}{0}{0}}||exercise-37=={{0}{0}{0}{0}{0}}||exercise-38=={{0}{0}{0}{0}{0}}||exercise-39=={{0}{0}{0}{0}{0}}||exercise-40=={{0}{0}{0}{0}{0}}||exercise-41=={{0}{0}{0}{0}{0}}||exercise-42=={{0}{0}{0}{0}{0}}||exercise-43=={{0}{0}{0}{0}{0}}||exercise-44=={{0}{0}{0}{0}{0}}||exercise-45=={{0}{0}{0}{0}{0}}||exercise-46=={{0}{0}{0}{0}{0}}||exercise-47=={{0}{0}{0}{0}{0}}||exercise-48=={{0}{0}{0}{0}{0}}||exercise-49=={{0}{0}{0}{0}{0}}||exercise-50=={{0}{0}{0}{0}{0}}||exercise-51=={{0}{0}{0}{0}{0}}||exercise-52=={{0}{0}{0}{0}{0}}||exercise-53=={{0}{0}{0}{0}{0}}||exercise-54=={{0}{0}{0}{0}{0}}||exercise-55=={{0}{0}{0}{0}{0}}||exercise-56=={{0}{0}{0}{0}{0}}||exercise-57=={{0}{0}{0}{0}{0}}||exercise-58=={{0}{0}{0}{0}{0}}||exercise-59=={{0}{0}{0}{0}{0}}||exercise-60=={{0}{0}{0}{0}{0}}||exercise-61=={{0}{0}{0}{0}{0}}||exercise-62=={{0}{0}{0}{0}{0}}||exercise-63=={{0}{0}{0}{0}{0}}||exercise-64=={{0}{0}{0}{0}{0}}||exercise-65=={{0}{0}{0}{0}{0}}||exercise-66=={{0}{0}{0}{0}{0}}||exercise-67=={{0}{0}{0}{0}{0}}||exercise-68=={{0}{0}{0}{0}{0}}||exercise-69=={{0}{0}{0}{0}{0}}||exercise-70=={{0}{0}{0}{0}{0}}||exercise-71=={{0}{0}{0}{0}{0}}||exercise-72=={{0}{0}{0}{0}{0}}||exercise-73=={{0}{0}{0}{0}{0}}||exercise-74=={{0}{0}{0}{0}{0}}||exercise-75=={{0}{0}{0}{0}{0}}||exercise-76=={{0}{0}{0}{0}{0}}||exercise-77=={{0}{0}{0}{0}{0}}||exercise-78=={{0}{0}{0}{0}{0}}||exercise-79=={{0}{0}{0}{0}{0}}||exercise-80=={{0}{0}{0}{0}{0}}||exercise-81=={{0}{0}{0}{0}{0}}||exercise-82=={{0}{0}{0}{0}{0}}||exercise-83=={{0}{0}{0}{0}{0}}||exercise-84=={{0}{0}{0}{0}{0}}||exercise-85=={{0}{0}{0}{0}{0}}||exercise-86=={{0}{0}{0}{0}{0}}||exercise-87=={{0}{0}{0}{0}{0}}||exercise-88=={{0}{0}{0}{0}{0}}||exercise-89=={{0}{0}{0}{0}{0}}||exercise-90=={{0}{0}{0}{0}{0}}||exercise-91=={{0}{0}{0}{0}{0}}||exercise-92=={{0}{0}{0}{0}{0}}||exercise-93=={{0}{0}{0}{0}{0}}||exercise-94=={{0}{0}{0}{0}{0}}||exercise-95=={{0}{0}{0}{0}{0}}||exercise-96=={{0}{0}{0}{0}{0}}||exercise-97=={{0}{0}{0}{0}{0}}||exercise-98=={{0}{0}{0}{0}{0}}||exercise-99=={{0}{0}{0}{0}{0}}||exercise-100=={{0}{0}{0}{0}{0}}||exercise-101=={{0}{0}{0}{0}{0}}||exercise-102=={{0}{0}{0}{0}{0}}||exercise-103=={{0}{0}{0}{0}{0}}||exercise-104=={{0}{0}{0}{0}{0}}||exercise-105=={{0}{0}{0}{0}{0}}||exercise-106=={{0}{0}{0}{0}{0}}||exercise-107=={{0}{0}{0}{0}{0}}||exercise-108=={{0}{0}{0}{0}{0}}||exercise-109=={{0}{0}{0}{0}{0}}||exercise-110=={{0}{0}{0}{0}{0}}||exercise-111=={{0}{0}{0}{0}{0}}||exercise-112=={{0}{0}{0}{0}{0}}||exercise-113=={{0}{0}{0}{0}{0}}||exercise-114=={{0}{0}{0}{0}{0}}||exercise-115=={{0}{0}{0}{0}{0}}||exercise-116=={{0}{0}{0}{0}{0}}||exercise-117=={{0}{0}{0}{0}{0}}||exercise-118=={{0}{0}{0}{0}{0}}||exercise-119=={{0}{0}{0}{0}{0}}||exercise-120=={{0}{0}{0}{0}{0}}||exercise-121=={{0}{0}{0}{0}{0}}||exercise-122=={{0}{0}{0}{0}{0}}||exercise-123=={{0}{0}{0}{0}{0}}||exercise-124=={{0}{0}{0}{0}{0}}||exercise-125=={{0}{0}{0}{0}{0}}||exercise-126=={{0}{0}{0}{0}{0}}||exercise-127=={{0}{0}{0}{0}{0}}||exercise-128=={{0}{0}{0}{0}{0}}||exercise-129=={{0}{0}{0}{0}{0}}||exercise-130=={{0}{0}{0}{0}{0}}||exercise-131=={{0}{0}{0}{0}{0}}||exercise-132=={{0}{0}{0}{0}{0}}||exercise-133=={{0}{0}{0}{0}{0}}||exercise-134=={{0}{0}{0}{0}{0}}||exercise-135=={{0}{0}{0}{0}{0}}||exercise-136=={{0}{0}{0}{0}{0}}||exercise-137=={{0}{0}{0}{0}{0}}||exercise-138=={{0}{0}{0}{0}{0}}||exercise-139=={{0}{0}{0}{0}{0}}||exercise-140=={{0}{0}{0}{0}{0}}||exercise-141=={{0}{0}{0}{0}{0}}||exercise-142=={{0}{0}{0}{0}{0}}||exercise-143=={{0}{0}{0}{0}{0}}||exercise-144=={{0}{0}{0}{0}{0}}||exercise-145=={{0}{0}{0}{0}{0}}||exercise-146=={{0}{0}{0}{0}{0}}||exercise-147=={{0}{0}{0}{0}{0}}||exercise-148=={{0}{0}{0}{0}{0}}||exercise-149=={{0}{0}{0}{0}{0}}||exercise-150=={{0}{0}{0}{0}{0}}||exercise-151=={{0}{0}{0}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={2}||exercise-14=={2}||exercise-15=={2}||exercise-16=={2}||exercise-17=={2}||exercise-18=={2}||exercise-19=={2}||exercise-20=={2}||exercise-21=={2}||exercise-22=={2}||exercise-23=={2}||exercise-24=={3}||exercise-25=={3}||exercise-26=={3}||exercise-27=={3}||exercise-28=={3}||exercise-29=={3}||exercise-30=={3}||exercise-31=={3}||exercise-32=={3}||exercise-33=={3}||exercise-34=={3}||exercise-35=={3}||exercise-36=={3}||exercise-37=={4}||exercise-38=={4}||exercise-39=={4}||exercise-40=={4}||exercise-41=={4}||exercise-42=={4}||exercise-43=={4}||exercise-44=={4}||exercise-45=={4}||exercise-46=={4}||exercise-47=={4}||exercise-48=={4}||exercise-49=={5}||exercise-50=={5}||exercise-51=={5}||exercise-52=={5}||exercise-53=={5}||exercise-54=={5}||exercise-55=={5}||exercise-56=={5}||exercise-57=={5}||exercise-58=={5}||exercise-59=={5}||exercise-60=={6}||exercise-61=={6}||exercise-62=={6}||exercise-63=={6}||exercise-64=={6}||exercise-65=={6}||exercise-66=={6}||exercise-67=={7}||exercise-68=={7}||exercise-69=={7}||exercise-70=={7}||exercise-71=={7}||exercise-72=={7}||exercise-73=={7}||exercise-74=={7}||exercise-75=={8}||exercise-76=={8}||exercise-77=={8}||exercise-78=={8}||exercise-79=={8}||exercise-80=={8}||exercise-81=={8}||exercise-82=={9}||exercise-83=={9}||exercise-84=={9}||exercise-85=={9}||exercise-86=={9}||exercise-87=={9}||exercise-88=={9}||exercise-89=={9}||exercise-90=={9}||exercise-91=={9}||exercise-92=={9}||exercise-93=={10}||exercise-94=={10}||exercise-95=={10}||exercise-96=={10}||exercise-97=={10}||exercise-98=={10}||exercise-99=={10}||exercise-100=={10}||exercise-101=={10}||exercise-102=={11}||exercise-103=={11}||exercise-104=={11}||exercise-105=={11}||exercise-106=={11}||exercise-107=={11}||exercise-108=={11}||exercise-109=={12}||exercise-110=={12}||exercise-111=={12}||exercise-112=={12}||exercise-113=={12}||exercise-114=={12}||exercise-115=={12}||exercise-116=={12}||exercise-117=={13}||exercise-118=={13}||exercise-119=={13}||exercise-120=={13}||exercise-121=={13}||exercise-122=={13}||exercise-123=={13}||exercise-124=={13}||exercise-125=={13}||exercise-126=={14}||exercise-127=={14}||exercise-128=={14}||exercise-129=={14}||exercise-130=={14}||exercise-131=={14}||exercise-132=={14}||exercise-133=={14}||exercise-134=={15}||exercise-135=={15}||exercise-136=={15}||exercise-137=={15}||exercise-138=={15}||exercise-139=={15}||exercise-140=={15}||exercise-141=={15}||exercise-142=={16}||exercise-143=={16}||exercise-144=={16}||exercise-145=={16}||exercise-146=={16}||exercise-147=={16}||exercise-148=={16}||exercise-149=={17}||exercise-150=={17}||exercise-151=={17}}
\XSIM{page}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={2}||exercise-14=={2}||exercise-15=={2}||exercise-16=={2}||exercise-17=={2}||exercise-18=={2}||exercise-19=={2}||exercise-20=={2}||exercise-21=={2}||exercise-22=={2}||exercise-23=={2}||exercise-24=={3}||exercise-25=={3}||exercise-26=={3}||exercise-27=={3}||exercise-28=={3}||exercise-29=={3}||exercise-30=={3}||exercise-31=={3}||exercise-32=={3}||exercise-33=={3}||exercise-34=={3}||exercise-35=={3}||exercise-36=={3}||exercise-37=={4}||exercise-38=={4}||exercise-39=={4}||exercise-40=={4}||exercise-41=={4}||exercise-42=={4}||exercise-43=={4}||exercise-44=={4}||exercise-45=={4}||exercise-46=={4}||exercise-47=={4}||exercise-48=={4}||exercise-49=={5}||exercise-50=={5}||exercise-51=={5}||exercise-52=={5}||exercise-53=={5}||exercise-54=={5}||exercise-55=={5}||exercise-56=={5}||exercise-57=={5}||exercise-58=={5}||exercise-59=={5}||exercise-60=={6}||exercise-61=={6}||exercise-62=={6}||exercise-63=={6}||exercise-64=={6}||exercise-65=={6}||exercise-66=={6}||exercise-67=={7}||exercise-68=={7}||exercise-69=={7}||exercise-70=={7}||exercise-71=={7}||exercise-72=={7}||exercise-73=={7}||exercise-74=={7}||exercise-75=={8}||exercise-76=={8}||exercise-77=={8}||exercise-78=={8}||exercise-79=={8}||exercise-80=={8}||exercise-81=={8}||exercise-82=={9}||exercise-83=={9}||exercise-84=={9}||exercise-85=={9}||exercise-86=={9}||exercise-87=={9}||exercise-88=={9}||exercise-89=={9}||exercise-90=={9}||exercise-91=={9}||exercise-92=={9}||exercise-93=={10}||exercise-94=={10}||exercise-95=={10}||exercise-96=={10}||exercise-97=={10}||exercise-98=={10}||exercise-99=={10}||exercise-100=={10}||exercise-101=={10}||exercise-102=={11}||exercise-103=={11}||exercise-104=={11}||exercise-105=={11}||exercise-106=={11}||exercise-107=={11}||exercise-108=={11}||exercise-109=={12}||exercise-110=={12}||exercise-111=={12}||exercise-112=={12}||exercise-113=={12}||exercise-114=={12}||exercise-115=={12}||exercise-116=={12}||exercise-117=={13}||exercise-118=={13}||exercise-119=={13}||exercise-120=={13}||exercise-121=={13}||exercise-122=={13}||exercise-123=={13}||exercise-124=={13}||exercise-125=={13}||exercise-126=={14}||exercise-127=={14}||exercise-128=={14}||exercise-129=={14}||exercise-130=={14}||exercise-131=={14}||exercise-132=={14}||exercise-133=={14}||exercise-134=={15}||exercise-135=={15}||exercise-136=={15}||exercise-137=={15}||exercise-138=={15}||exercise-139=={15}||exercise-140=={15}||exercise-141=={15}||exercise-142=={16}||exercise-143=={16}||exercise-144=={16}||exercise-145=={16}||exercise-146=={16}||exercise-147=={16}||exercise-148=={16}||exercise-149=={17}||exercise-150=={17}||exercise-151=={17}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
\XSIM{correctchoice}{exercise-1=={(\textbf {B}) L1}||exercise-2=={(\textbf {B}) Trains multiple models on different subsets of the data}||exercise-3=={(\textbf {C}) Both \circled {A} and \circled {B}}||exercise-4=={(\textbf {A}) Shallow neural network}||exercise-5=={(\textbf {B}) unstructured data}||exercise-6=={(\textbf {C}) convolutional neural networks}||exercise-7=={(\textbf {D}) All of the above}||exercise-8=={(\textbf {C}) Constructs an ensemble by iteratively updating weights}||exercise-9=={(\textbf {E}) All of the previous}||exercise-10=={(\textbf {C}) Random Forest}||exercise-11=={(\textbf {B}) Boosting algorithm}||exercise-12=={(\textbf {C}) Constructs an ensemble by iteratively updating weights}||exercise-13=={(\textbf {B}) Boosting algorithm}||exercise-14=={(\textbf {D}) Trains a meta-model to make predictions based on outputs of base models}||exercise-15=={(\textbf {A}) Random Forest}||exercise-16=={(\textbf {A}) Reduce overfitting and improve generalization}||exercise-17=={(\textbf {A}) Handling imbalanced datasets}||exercise-18=={(\textbf {A}) AdaBoost}||exercise-19=={(\textbf {C}) Random Forest}||exercise-20=={(\textbf {B}) Boosting}||exercise-21=={(\textbf {C}) Gradient Boosting}||exercise-22=={(\textbf {B}) The dataset is large and high-dimensional}||exercise-23=={(\textbf {B}) Reducing variance}||exercise-24=={(\textbf {C}) Graident Boosting}||exercise-25=={(\textbf {C}) Random Forest}||exercise-26=={(\textbf {B}) Combining predictions by taking the mode of their classes}||exercise-27=={(\textbf {A}) Bagging}||exercise-28=={(\textbf {A}) Improving model stability}||exercise-29=={(\textbf {C}) Graident Boosting}||exercise-30=={(\textbf {C}) Handling unbalanced datasets}||exercise-31=={(\textbf {A}) Bagging}||exercise-32=={(\textbf {C}) Random Forest}||exercise-33=={(\textbf {B}) AdaBoost}||exercise-34=={(\textbf {C}) Gradient Boosting}||exercise-35=={(\textbf {B}) False}||exercise-36=={(\textbf {D}) All of the previous}||exercise-37=={(\textbf {A}) Softmax}||exercise-38=={(\textbf {A}) Weight between input and hidden layer}||exercise-39=={(\textbf {C}) 96}||exercise-40=={(\textbf {C}) \(22 \times 22\)}||exercise-41=={(\textbf {A}) 50}||exercise-42=={(\textbf {D}) all of the previous}||exercise-43=={(\textbf {D}) 41\%}||exercise-44=={(\textbf {D}) All of the above}||exercise-45=={(\textbf {D}) all of the previous}||exercise-46=={(\textbf {C}) ReLU}||exercise-47=={(\textbf {A}) True}||exercise-48=={(\textbf {E}) All of the previous}||exercise-49=={(\textbf {A}) True}||exercise-50=={(\textbf {B}) False}||exercise-51=={(\textbf {B}) False}||exercise-52=={(\textbf {D}) all of the previous}||exercise-53=={(\textbf {D}) All of the above}||exercise-54=={(\textbf {A}) True}||exercise-55=={(\textbf {B}) False}||exercise-56=={(\textbf {C}) to prevent overfitting}||exercise-57=={(\textbf {A}) L1 regularization}||exercise-58=={(\textbf {C}) small weight values}||exercise-59=={(\textbf {B}) It randomly drops entire layers during training}||exercise-60=={(\textbf {C}) Elastic Net}||exercise-61=={(\textbf {B}) To prevent the model from memorizing the training data}||exercise-62=={(\textbf {C}) Regularization can help balance bias and variance}||exercise-63=={(\textbf {B}) The gradual decrease in weight values during training}||exercise-64=={(\textbf {D}) Reduced capacity to capture complex patterns}||exercise-65=={(\textbf {A}) A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.}||exercise-66=={(\textbf {A}) 98\% train. 1\% dev. 1\% test}||exercise-67=={(\textbf {A}) Come from the same distribution}||exercise-68=={(\textbf {B}) Get more training data}||exercise-69=={(\textbf {A}) Increase the regularization parameter lambda}||exercise-70=={(\textbf {A}) Weights are pushed twoard becoming smaller (closer to 0)}||exercise-71=={(\textbf {B}) You don't apply dropout (do not randomly eliminate units) and do not keep the \texttt {1/keep\_prob} factor in the calculations usd in the training}||exercise-72=={(\textbf {A}) Dropout}||exercise-73=={(\textbf {B}) It makes the cost function faster to optimize}||exercise-74=={(\textbf {C}) Regulates the softness of the target distribution}||exercise-75=={(\textbf {C}) The probability of dropping out a unit in the hidden layers during training}||exercise-76=={(\textbf {B}) Learning rate annealing}||exercise-77=={(\textbf {C}) To reduce the impact of outliers in the input data}||exercise-78=={(\textbf {C}) Constraining the magnitude of the weights in the model}||exercise-79=={(\textbf {D}) Dropout helps prevent co-adaptation of hidden units}||exercise-80=={(\textbf {C}) To improve the predictive performance of a model by combining multiple models}||exercise-81=={(\textbf {B}) It trains multiple models independently on different subsets of the training data.}||exercise-82=={(\textbf {C}) To introduce randomness by considering a random subset of features for each tree}||exercise-83=={(\textbf {C}) Sequentially, with higher weights for misclassified instances}||exercise-84=={(\textbf {B}) Stacking}||exercise-85=={(\textbf {C}) Ensemble methods often generalize better and have improved robustness.}||exercise-86=={(\textbf {B}) A model that performs slightly better than random chance}||exercise-87=={(\textbf {C}) Bagging}||exercise-88=={(\textbf {A}) Bootstrap Aggregating}||exercise-89=={(\textbf {B}) AdaBoost}||exercise-90=={(\textbf {B}) Improved generalization and robustness}||exercise-91=={(\textbf {C}) Random Forest}||exercise-92=={(\textbf {A}) Long Short-Term Memory}||exercise-93=={(\textbf {A}) It adjusts the amount by which weights are updated during each iteration}||exercise-94=={(\textbf {C}) Random Forest introduces randomness by considering a random subset of features for each tree}||exercise-95=={(\textbf {D}) Stacking uses multiple base models to form a meta-model}||exercise-96=={(\textbf {C}) Ensemble methods help balance bias and variance}||exercise-97=={(\textbf {A}) Increased risk of overfitting}||exercise-98=={(\textbf {A}) Randomly and with replacement}||exercise-99=={(\textbf {B}) Better handling of outliers}||exercise-100=={(\textbf {C}) Boosting}||exercise-101=={(\textbf {D}) Using multiple base models to form a meta-model}||exercise-102=={(\textbf {C}) Random Forest}||exercise-103=={(\textbf {C}) It is an estimate of the test error obtained from the unused samples during training}||exercise-104=={(\textbf {B}) It limits the maximum depth of individual decision trees}||exercise-105=={(\textbf {B}) Early stopping prevents overfitting by stopping the training process when the model starts to memorize the training data.}||exercise-106=={(\textbf {B}) The computational complexity increases linearly}||exercise-107=={(\textbf {A}) Adversarial training involves training models to be robust against adversarial attacks.}||exercise-108=={(\textbf {B}) It uses multiple cross-validated models, reducing overfitting.}||exercise-109=={(\textbf {B}) Increased risk of overfitting}||exercise-110=={(\textbf {B}) Feature importance indicates the relevance of a feature in predicting the target variable.}||exercise-111=={(\textbf {C}) It specifies the number of base models in the ensemble.}||exercise-112=={(\textbf {A}) Stacking with meta-features involves using the output of base models as features for a meta-model.}||exercise-113=={(\textbf {B}) Removing random neurons during training}||exercise-114=={(\textbf {C}) To prevent co-adaptation of neurons}||exercise-115=={(\textbf {B}) Dropout is applied to all layers except the output layer}||exercise-116=={(\textbf {C}) By reducing the model's capacity}||exercise-117=={(\textbf {B}) The neuron is removed from the network temporarily}||exercise-118=={(\textbf {B}) Overfitting}||exercise-119=={(\textbf {A}) Slows down the training process}||exercise-120=={(\textbf {B}) 0.2 to 0.5}||exercise-121=={(\textbf {C}) By reducing the sensitivity of neurons to specific input features}||exercise-122=={(\textbf {A}) Training phase}||exercise-123=={(\textbf {A}) Co-adaptation refers to neurons relying too much on each other, and Dropout breaks these dependencies by randomly dropping neurons during training.}||exercise-124=={(\textbf {B}) Dropout is more effective in large and complex networks}||exercise-125=={(\textbf {C}) Dropout and ensemble learning achieve the same result in terms of model diversity}||exercise-126=={(\textbf {A}) High Dropout rates lead to overfitting, while low Dropout rates may result in underfitting.}||exercise-127=={(\textbf {C}) By introducing noise to the input data}||exercise-128=={(\textbf {C}) To improve model performance by increasing the diversity of the training data}||exercise-129=={(\textbf {C}) Image rotation}||exercise-130=={(\textbf {D}) By providing a more diverse set of training examples}||exercise-131=={(\textbf {C}) Word substitution}||exercise-132=={(\textbf {B}) Potential introduction of unrealistic patterns}||exercise-133=={(\textbf {C}) To create variations in the spatial location of objects}||exercise-134=={(\textbf {B}) Time warping}||exercise-135=={(\textbf {A}) Jittering refers to the introduction of noise to input features}||exercise-136=={(\textbf {B}) To create mirror images}||exercise-137=={(\textbf {A}) Data augmentation focuses on creating new samples, while feature engineering manipulates existing features.}||exercise-138=={(\textbf {B}) Dropout enhances data augmentation by randomly removing features during training}||exercise-139=={(\textbf {B}) Spectrogram augmentation}||exercise-140=={(\textbf {B}) To introduce non-linear distortions to the image}||exercise-141=={(\textbf {D}) Sentence dropout}||exercise-142=={(\textbf {A}) Adversarial training focuses on creating adversarial examples to test the model's robustness against unseen patterns introduced by data augmentation.}||exercise-143=={(\textbf {C}) Data augmentation generates additional samples for minority classes, addressing class imbalance}||exercise-144=={(\textbf {C}) The potential introduction of unrealistic patterns}||exercise-145=={(\textbf {A}) Mixup involves blending two or more samples, creating new synthetic samples with averaged labels.}||exercise-146=={(\textbf {C}) Data augmentation reduces model interpretability due to the introduction of synthetic samples.}||exercise-147=={(\textbf {A}) To remove random portions from images}||exercise-148=={(\textbf {C}) Shearing introduces non-linear distortions to the image by tilting it along one of its axes.}||exercise-149=={(\textbf {C}) Random Forest}||exercise-150=={(\textbf {D}) The dataset is large}||exercise-151=={(\textbf {C}) Gradient Boosting}}
\XSIM{correctchoicetwo}{exercise-68=={(\textbf {E}) Add regularization}||exercise-69=={(\textbf {C}) get more training data}||exercise-72=={(\textbf {C}) Data augmentation}}
\XSIM{correctchoicethree}{exercise-72=={(\textbf {F}) L2 regularization}}
\XSIM{correctchoicefour}{}
\XSIM{correctchoicefive}{}
