\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={}||exercise-2=={}||exercise-3=={}||exercise-4=={}||exercise-5=={}||exercise-6=={}||exercise-7=={}||exercise-8=={}||exercise-9=={}||exercise-10=={}||exercise-11=={}||exercise-12=={}||exercise-13=={}||exercise-14=={}||exercise-15=={}||exercise-16=={}||exercise-17=={}||exercise-18=={}||exercise-19=={}||exercise-20=={}||exercise-21=={}||exercise-22=={}||exercise-23=={}||exercise-24=={}||exercise-25=={}||exercise-26=={}||exercise-27=={}||exercise-28=={}||exercise-29=={}||exercise-30=={}||exercise-31=={}||exercise-32=={}||exercise-33=={}||exercise-34=={}||exercise-35=={}||exercise-36=={}||exercise-37=={}||exercise-38=={}||exercise-39=={}||exercise-40=={}||exercise-41=={}||exercise-42=={}||exercise-43=={}||exercise-44=={}||exercise-45=={}||exercise-46=={}||exercise-47=={}||exercise-48=={}||exercise-49=={}||exercise-50=={}||exercise-51=={}||exercise-52=={}||exercise-53=={}||exercise-54=={}||exercise-55=={}||exercise-56=={}||exercise-57=={}||exercise-58=={}||exercise-59=={}||exercise-60=={}||exercise-61=={}||exercise-62=={}||exercise-63=={}||exercise-64=={}||exercise-65=={}||exercise-66=={}||exercise-67=={}||exercise-68=={}||exercise-69=={}||exercise-70=={}||exercise-71=={}||exercise-72=={}||exercise-73=={}||exercise-74=={}||exercise-75=={}||exercise-76=={}||exercise-77=={}||exercise-78=={}||exercise-79=={}||exercise-80=={}||exercise-81=={}||exercise-82=={}||exercise-83=={}||exercise-84=={}||exercise-85=={}||exercise-86=={}||exercise-87=={}||exercise-88=={}||exercise-89=={}||exercise-90=={}||exercise-91=={}||exercise-92=={}||exercise-93=={}||exercise-94=={}||exercise-95=={}||exercise-96=={}||exercise-97=={}||exercise-98=={}||exercise-99=={}||exercise-100=={}||exercise-101=={}||exercise-102=={}||exercise-103=={}||exercise-104=={}||exercise-105=={}||exercise-106=={}||exercise-107=={}||exercise-108=={}||exercise-109=={}||exercise-110=={}||exercise-111=={}||exercise-112=={}||exercise-113=={}||exercise-114=={}||exercise-115=={}||exercise-116=={}||exercise-117=={}||exercise-118=={}||exercise-119=={}||exercise-120=={}||exercise-121=={}||exercise-122=={}||exercise-123=={}||exercise-124=={}||exercise-125=={}||exercise-126=={}||exercise-127=={}||exercise-128=={}||exercise-129=={}||exercise-130=={}||exercise-131=={}||exercise-132=={}||exercise-133=={}||exercise-134=={}||exercise-135=={}||exercise-136=={}||exercise-137=={}||exercise-138=={}||exercise-139=={}||exercise-140=={}||exercise-141=={}||exercise-142=={}||exercise-143=={}||exercise-144=={}||exercise-145=={}||exercise-146=={}||exercise-147=={}||exercise-148=={}||exercise-149=={}||exercise-150=={}||exercise-151=={}||exercise-152=={}||exercise-153=={}||exercise-154=={}||exercise-155=={}||exercise-156=={}||exercise-157=={}||exercise-158=={}||exercise-159=={}||exercise-160=={}||exercise-161=={}||exercise-162=={}||exercise-163=={}||exercise-164=={}||exercise-165=={}||exercise-166=={}||exercise-167=={}||exercise-168=={}||exercise-169=={}||exercise-170=={}||exercise-171=={}||exercise-172=={}||exercise-173=={}||exercise-174=={}||exercise-175=={}||exercise-176=={In dropout, neurons are dropped, whereas in dropconnect, connections are dropped. So, both input and output weights will be rendered useless in dropconnect, while only one of them should be dropped in dropconnect.}||exercise-177=={In dropout, neurons are dropped, whereas in dropconnect, connections are dropped. So, both input and output weights will be rendered useless in dropconnect, while only one of them should be dropped in dropconnect.}||exercise-178=={}||exercise-179=={}||exercise-180=={}||exercise-181=={}||exercise-182=={}||exercise-183=={}||exercise-184=={}||exercise-185=={}||exercise-186=={}||exercise-187=={}||exercise-188=={}||exercise-189=={}||exercise-190=={}||exercise-191=={}||exercise-192=={}||exercise-193=={}||exercise-194=={}||exercise-195=={}||exercise-196=={}||exercise-197=={}||exercise-198=={}||exercise-199=={}||exercise-200=={}||exercise-201=={}||exercise-202=={}||exercise-203=={}||exercise-204=={}||exercise-205=={}||exercise-206=={}||exercise-207=={}||exercise-208=={}||exercise-209=={}||exercise-210=={}||exercise-211=={}||exercise-212=={}||exercise-213=={}||exercise-214=={}||exercise-215=={}||exercise-216=={}||exercise-217=={}||exercise-218=={}||exercise-219=={}||exercise-220=={}||exercise-221=={}||exercise-222=={}||exercise-223=={}||exercise-224=={}||exercise-225=={}||exercise-226=={}||exercise-227=={}||exercise-228=={}||exercise-229=={}||exercise-230=={}||exercise-231=={}||exercise-232=={}||exercise-233=={}||exercise-234=={}||exercise-235=={}||exercise-236=={}||exercise-237=={}||exercise-238=={}||exercise-239=={}||exercise-240=={}||exercise-241=={}||exercise-242=={}||exercise-243=={}||exercise-244=={}||exercise-245=={}||exercise-246=={}||exercise-247=={}||exercise-248=={}||exercise-249=={}}
\XSIM{exercise-body}{exercise-1=={\dots the weights may be reduced to zero. \begin {choice}(4) * L1 and L2 * \correctchoice {L1} * L2 * None of the above \end {choice}}||exercise-2=={Bagging is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * \correctchoice {Trains multiple models on different subsets of the data} * Constructs an ensemble by iteratively updating weights * Uses a committee of experts to make predictions \end {choice}}||exercise-3=={Which of the following is/are Limitations of deep learning? \begin {choice} (2) * Data labeling * Obtain huge training datasets * \correctchoice {Both \circled {A} and \circled {B}} * None of the previous \end {choice}}||exercise-4=={Which neural network has only one hidden layer between the input and output? \begin {choice} (2) * \correctchoice {Shallow neural network} * Deep neural network * Feed-forward neural networks * Recurrent neural networks \end {choice}}||exercise-5=={CNN is mostly used when there is an? \begin {choice} (4) * structured data * \correctchoice {unstructured data} * both \circled {A} and \circled {B} * None of the previous \end {choice}}||exercise-6=={Which of the following is well suited for perceptual tasks? \begin {choice} (1) * feed-forward neural networks * recurrent neural networks * \correctchoice {convolutional neural networks} * Reinforcement learning \end {choice}}||exercise-7=={Which of the following is/are Common uses of RNNs? \begin {choice} * BusinessesHelp securities traders to generate analytic reports * Detect fraudulent credit-card transaction * Provide a caption for images * \correctchoice {All of the above} \end {choice}}||exercise-8=={Boosting is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * \correctchoice {Constructs an ensemble by iteratively updating weights} * Uses a committee of experts to make predictions \end {choice}}||exercise-9=={What steps can we take to prevent overfitting in a Neural Network? \begin {choice}(2) * Data Augmentation * Weight Sharing * Early Stopping * Dropout * \correctchoice {All of the previous} \end {choice}}||exercise-10=={Which of the following is an example of an ensemble learning algorithm? \begin {choice} (4) * Decision tree * SVM * \correctchoice {Random Forest} * KNN \end {choice}}||exercise-11=={AdaBoost is an example of: \begin {choice} (2) * Bagging algorithm * \correctchoice {Boosting algorithm} * Randomized algorithm * Reinforcement learning algorithm \end {choice}}||exercise-12=={Gradient Boosting is an ensemble technique that: \begin {choice} * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * \correctchoice {Constructs an ensemble by iteratively updating weights} * Uses a committee of experts to make predictions \end {choice}}||exercise-13=={XGBoost is a popular implementation of: \begin {choice}(2) * Bagging algorithm * \correctchoice {Boosting algorithm} * Random Forest Algorithm * K-Means clustering algorithms \end {choice}}||exercise-14=={Stacking is an ensemble technique that: \begin {choice} (1) * Combines predictions using a weighted average * Trains multiple models on different subsets of the data * Constructs an ensemble by iteratively updating weights * \correctchoice {Trains a meta-model to make predictions based on outputs of base models} \end {choice}}||exercise-15=={Which ensemble learning algorithm uses bootstrapping and feature sampling? \begin {choice} (4) * \correctchoice {Random Forest} * AdaBoost * Gradient Boosting * Stacking \end {choice}}||exercise-16=={The purpose of using ensemble learning is to: \begin {choice} * \correctchoice {Reduce overfitting and improve generalization} * Increase training time and complexity * Decrease the number of models required * Eliminate the need for labeled data \end {choice}}||exercise-17=={Bagging algorithms are effective in: \begin {choice}(2) * \correctchoice {Handling imbalanced datasets} * sequential data prediction * Clustering high-dimensional data * Text classification tasks \end {choice}}||exercise-18=={Which ensemble learning algorithm assigns weights to base models based on their performance? \begin {choice} (4) * \correctchoice {AdaBoost} * Random Forest * Gradient Boosting * Stacking \end {choice}}||exercise-19=={Which ensemble learning algorithm uses a committee of experts to make predictions? \begin {choice} (4) * Bagging * Boosting * Random Forest * \correctchoice {Stacking} \end {choice}}||exercise-20=={Which ensemble learning algorithm is prone to overfitting if the base models are too complex? \begin {choice} (4) * Bagging * \correctchoice {Boosting} * Random Forest * Stacking \end {choice}}||exercise-21=={Which ensemble learning algorithm can handle both regression and classification tasks? \begin {choice} (5) * Bagging * AdaBoost * Gradient Boosting * Stacking * \correctchoice {All of the previous} \end {choice}}||exercise-22=={Ensemble learning algorithms are useful when: \begin {choice}(2) * The dataset is small and low-dimensional * \correctchoice {The dataset is large and high-dimensional} * The dataset is perfectly balanced * The dataset contains categorical variables \end {choice}}||exercise-23=={Ensemble learning algorithms can improve model performance by: \begin {choice} (2) * Reducing bias * \correctchoice {Reducing variance} * Increasing interpretability * Increasing training time \end {choice}}||exercise-24=={Which ensemble learning algorithm can handle both numerical and categorical data without requiring one-hot encoding? \begin {choice} (4) * Bagging * AdaBoost * Graident Boosting * \correctchoice {Stacking} \end {choice}}||exercise-25=={Which ensemble learning algorithm is less sensitive to outliers? \begin {choice} (4) * \correctchoice {Bagging} * Boosting * Random Forest * Stacking \end {choice}}||exercise-26=={The majority voting method in ensemble learning refers to: \begin {choice} * Combining predictions by averaging their probabilities * \correctchoice {Combining predictions by taking the mode of their classes} * Combining predictions by multiplying their probabilities * Combining predictions by taking the median of their values \end {choice}}||exercise-27=={Which ensemble learning algorithm can handle missing values in the dataset? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}||exercise-28=={Ensemble learning algorithms are useful for: \begin {choice} (2) * \correctchoice {Improving model stability} * Increasing model complexity * Reducing feature importance * Eliminating the need for cross-validation \end {choice}}||exercise-29=={Which ensemble learning algorithm can handle non-linear relationships in the data? \begin {choice} (4) * Bagging * AdaBoost * Graident Boosting * \correctchoice {Stacking} \end {choice}}||exercise-30=={Ensemble learning algorithms are effective in: \begin {choice} (2) * Reducing model interpretability * Increasing model training * \correctchoice {Handling unbalanced datasets} * Eliminating the need for hyperparameter tuning \end {choice}}||exercise-31=={Which ensemble learning algorithm can handle both numerical and categorical features effectively? \begin {choice} (4) * Bagging * AdaBoost * Gradient Boosting * \correctchoice {Stacking} \end {choice}}||exercise-32=={Which ensemble learning algorithm is less susceptible to overfitting compared to others? \begin {choice} (4) * Bagging * Boosting * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-33=={Which ensemble learning algorithm uses a weighted sum of predictions from base models? \begin {choice} (4) * Bagging * \correctchoice {AdaBoost} * Gradient boosting * Stacking \end {choice}}||exercise-34=={Which ensemble learning algorithm can be used to identify important features in a dataset? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}||exercise-35=={The ReLu activation has no effect on back-propagation and the vanishing gradient. \begin {choice} (2) * True * \correctchoice {False} * can be true and false * can't say \end {choice}}||exercise-36=={Why is the vanishing gradient a problem? \begin {choice} * Training is quick if the gradient is large and slow if it's small * with back propagation, the gradient becomes smaller as it works back through the net * The gradient is calculated multiplying two numbers between 0 and 1 * \correctchoice {All of the previous} \end {choice}}||exercise-37=={Which of the following functions can be used as an activation function in the output layer if we wish to predict the probabilities of \(n\) classes \((p_1,p_2,...,p_k)\) such that sum of \(p\) over all \(n\) equals to 1? \begin {choice} (4) * \correctchoice {Softmax} * ReLu * Sigmoid * \(\tanh \) \end {choice}}||exercise-38=={Which of the following would have a constant input in each epoch of training a Deep Learning model? \begin {choice} * \correctchoice {Weight between input and hidden layer} * Weight between hidden and output layer * Biases of all hidden layer neurons * Activation Function of output layer * none of the previous \end {choice}}||exercise-39=={Assume a simple MLP model with 3 neurons and inputs= 1,2,3. The weights to the input neurons are 4,5 and 6 respectively. Assume the activation function is a linear constant value of 3. What will be the output ? \begin {choice} (4) * 32 * 64 * \correctchoice {96} * 128 \end {choice}}||exercise-40=={The input image has been converted into a matrix of size 28 X 28 and a kernel/filter of size 7 X 7 with a stride of 1. What will be the size of the convoluted matrix? \begin {choice} (4) * \(20 \times 20\) * \(21 \times 21\) * \correctchoice {\(22 \times 22\)} * \(25 \times 25\) \end {choice}}||exercise-41=={The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer are \dots \begin {choice} (4) * \correctchoice {50} * less than 50 * more than 50 * it's an arbitrary value. \end {choice}}||exercise-42=={Which of the following statements is true when you use \(1\times 1\) convolutions in a CNN? \begin {choice} * It can help in dimensionality reduction * It can be used for feature pooling * It suffers less overfitting due to small kernel size * \correctchoice {all of the previous} \end {choice}}||exercise-43=={Deep learning algorithms are \dots more accurate than machine learning algorithm in image classification. \begin {choice} (4) * 33 \% * 37\% * 40\% * \correctchoice {41\%} \end {choice}}||exercise-44=={Which of the following are universal approximators? \begin {choice} (4) * Kernel SVM * Neural Networks * Boosted Decision trees * \correctchoice {All of the above} \end {choice}}||exercise-45=={In which of the following applications can we use deep learning to solve the problem? \begin {choice} (2) * Protein structure prediction * Prediction of chemical reactions * Detection of exotic particles * \correctchoice {all of the previous} \end {choice}}||exercise-46=={Which of following activation function can't be used at output layer to classify an image ? \begin {choice} (4) * Sigmoid * \(\tanh \) * \correctchoice {ReLU} * None of the previous \end {choice}}||exercise-47=={Dropout can be applied at visible layer of Neural Network model? \begin {choice} (2) * \correctchoice {True} * False \end {choice}}||exercise-48=={Which of the following neural network training challenge can be solved using batch normalization? \begin {choice} (2) * overfitting * Restrict activation to become too high or low * Training is too slow * Both \circled {B} and \circled {C} * \correctchoice {All of the previous} \end {choice}}||exercise-49=={Changing Sigmoid activation to ReLu will help to get over the vanishing gradient issue? \begin {choice} (2) * \correctchoice {True} * False \end {choice}}||exercise-50=={In CNN, having max pooling always decrease the parameters? \begin {choice} (4) * True * \correctchoice {False} * can be true and false * can't say \end {choice}}||exercise-51=={Bagging is more sensitive to noise. \begin {choice} (2) * True * \correctchoice {False} \end {choice}}||exercise-52=={What is \textbf {true} about the functions of a Multi Layer Perceptron? \begin {choice} (1) * The first neural nets that were born out of the need to address teh inaccuracy of an early classifier, the perceptron. * It predicts which group of given set of inputs falls into. * It generates a score that determines the confidence level of the prediction * \correctchoice {all of the previous} \end {choice}}||exercise-53=={Select reason(s) for using a Deep Neural Network. \begin {choice} * Some patterns are very complex and can't be deciphered precisely by alternate means * Deep nets are great at recognizing patterns and using them as building blocks in deciphering inputs * We finally have the technology, GPUs, to accelerate the training process by several folds of magnitude. * \correctchoice {All of the above} \end {choice}}||exercise-54=={Sentiment analysis using Deep Learning is a many-to one prediction task \begin {choice} (4) * \correctchoice {True} * False * Can be true and false * can't say \end {choice}}||exercise-55=={BackPropogation cannot be applied when using pooling layers \begin {choice}(2) * True * \correctchoice {False} \end {choice}}||exercise-56=={What is the primary purpose of regularization in deep learning? \begin {choice}(1) * to increase computational efficiency * to reduce the number of layers in a neural network * \correctchoice {to prevent overfitting} * to speed up the training process \end {choice}}||exercise-57=={Which of the following regularization techniques adds a penalty term based on the absolute values of the weights? \begin {choice} (4) * \correctchoice {L1 regularization} * L2 regularization * Dropout * Elastic Net \end {choice}}||exercise-58=={In neural networks, what does L2 regularization encourage? \begin {choice} (2) * Sparse weight matrices * large weight values * \correctchoice {small weight values} * No impact on weight values \end {choice}}||exercise-59=={How does dropout regularization work in a neural network? \begin {choice}(1) * It randomly drops input features during training * \correctchoice {It randomly drops entire layers during training} * It adds noise to the input data * It introduces a penalty term for large weights. \end {choice}}||exercise-60=={Which regularization technique combines both L1 and L2 penalties? \begin {choice} (2) * Dropout * Ride regression * \correctchoice {Elastic Net} * Batch Normalization \end {choice}}||exercise-61=={What is the purpose of early stopping as a form of regularization? \begin {choice} (1) * To stop the training process when the model is underfitting * \correctchoice {To prevent the model from memorizing the training data} * To speed up the convergence of the training process * To reduce the impact of outliers in the training data \end {choice}}||exercise-62=={Which of the following statements is true about the bias-variance tradeoff in the context of regularization? \begin {choice} (1) * Regularization always increases bias and decreases variance * Regularization always increases both bias and variance * \correctchoice {Regularization can help balance bias and variance} * Regularization has no impact on the bias-variance tradeoff \end {choice}}||exercise-63=={In the context of neural networks, what does weight decay refer to? \begin {choice} (1) * The gradual increase in weight values during training * \correctchoice {The gradual decrease in weight values during training} * The removal of unnecessary weights from the network * The introduction of noise to the weight values \end {choice}}||exercise-64=={Which of the following is a disadvantage of using a high regularization strength in a neural network? \begin {choice} (1) * Increased risk of overfitting * Faster convergence during training * Enhanced generalization to new data * \correctchoice {Reduced capacity to capture complex patterns} \end {choice}}||exercise-65=={What is weight decay? \begin {choice} * \correctchoice {A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.} * Gradual corruption of the weights in the neural network if it's training on noisy data. * The process of gradually decreasing the learning rate during training * A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights. \end {choice}}||exercise-66=={If you have 10,000,000 examples, how would you split the train/dev/test set? \begin {choice}(1) * \correctchoice {98\% train. 1\% dev. 1\% test} * 33\% train. 33\% dev. 33\% test * 60\% train. 20\% dev. 20\% test \end {choice}}||exercise-67=={The dev and test set should: \begin {choice}(1) * \correctchoice {Come from the same distribution} * Come from different distributions * Be identical to each other(same \((x,y)\) pairs) * Have the same number of examples \end {choice}}||exercise-68=={If your Neural Network model seems to have high variance, what of the following would be promising things to try? (choose all that apply) \begin {choice} (2) * Make the Neural network deeper * \correctchoice {Get more training data} * Get more test data * Increase the number of units in each hidden layer * \correctchoicetwo {Add regularization} \end {choice}}||exercise-69=={You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5\% and a dev set error of 7\%. Which of the following are promising things to try to improve your classifier? (Check all that apply) \begin {choice}(1) * \correctchoice {Increase the regularization parameter lambda} * decrease the regularization parameter lambda * \correctchoicetwo {get more training data} * use a bigger neural network \end {choice}}||exercise-70=={What happens when you increase the regularization hyperparameter lambda? \begin {choice} * \correctchoice {Weights are pushed twoard becoming smaller (closer to 0)} * weights are pushed toward becoming bigger (further from 0) * doubling lambda should roughly result in doubling the weights * Gradient descent taking bigger steps with each iteration (proportional to lambda) \end {choice}}||exercise-71=={With the inverted dropout, at test time: \begin {choice} * You don't apply dropout (do not randomly eliminate units), but keep \texttt {1/keep\_prob} factor in the calculations used in training * \correctchoice {You don't apply dropout (do not randomly eliminate units) and do not keep the \texttt {1/keep\_prob} factor in the calculations usd in the training} * You apply dropout (randomly eliminate units) but keep \texttt {1/keep\_prob} factor in the calculations used in training * You apply dropout (randomly eliminate units) and do not keep \texttt {1/keep\_prob} factor in the calculations used in training \end {choice}}||exercise-72=={Which of these techniques are useful for reducing variance (reduce overfitting)? (check all that apply) \begin {choice} (3) * \correctchoice {Dropout} * Gradient Checking * \correctchoicetwo {Data augmentation} * Vanishing gradient * Xavier initialization * \correctchoicethree {L2 regularization} * Exploding gradient \end {choice}}||exercise-73=={Why do we normalize the inputs \(x\)? \begin {choice} * Normalization is another word for regularization--it helps to reduce variance * \correctchoice {It makes the cost function faster to optimize} * It makes it easier to visualize the data. * It makes the parameter initialization faster. \end {choice}}||exercise-74=={What is the role of the temperature parameter in the context of knowledge distillation as a form of regularization? \begin {choice} (1) * Controls the learning rate * Adjusts the level of noise in the input data * \correctchoice {Regulates the softness of the target distribution} * Sets the threshold for dropout during training \end {choice}}||exercise-75=={In the context of neural networks, what does dropout rate refer to? \begin {choice}(1) * The percentage of training samples used during each iteration * The rate at which weight are decayed during training * \correctchoice {The probability of dropping out a unit in the hidden layers during training} * The learning rate for stochastic gradient descent. \end {choice}}||exercise-76=={Which of the following is a technique used for dynamic adjustment of the learning rate during training to improve convergence in deep learning? \begin {choice}(2) * Adversarial training * \correctchoice {Learning rate annealing} * Batch Normalization * Feature Scaling \end {choice}}||exercise-77=={What is the purpose of adding noise to the input data as a form of regularization? \begin {choice} (1) * To make the training process deterministic * To improve model interpretability * To reduce the impact of outliers in the input data * \correctchoice {To prevent the model from memorizing the training data} \end {choice}}||exercise-78=={In the context of regularization, what does the term "shrinkage" refer to? \begin {choice} (1) * Reducing the size of the input data * Reducing the number of hidden layers in the network * \correctchoice {Constraining the magnitude of the weights in the model} * Eliminating unnecessary features from the dataset \end {choice}}||exercise-79=={Which of the following statements is true about the dropout technique? \begin {choice} (1) * Dropout is more effective in shallow networks than deep networks * Dropout can be applied only to input layers * Dropout introduces random variations only during testing * \correctchoice {Dropout helps prevent co-adaptation of hidden units} \end {choice}}||exercise-80=={What is the primary goal of ensemble methods in machine learning? \begin {choice}(1) * To reduce the computational complexity of models * To increase the training time of individual models * \correctchoice {To improve the predictive performance of a model by combining multiple models} * To decrease the diversity among base models \end {choice}}||exercise-81=={Which of the following statements is true about bagging (Bootstrap Aggregating)? \begin {choice} (1) * It trains multiple models sequentially. * \correctchoice {It trains multiple models independently on different subsets of the training data.} * It combines models using a weighted average. * It is not suitable for high-variance models. \end {choice}}||exercise-82=={What is the purpose of random forests in ensemble learning? \begin {choice}(1) * To create a forest of decision trees with high correlation * To reduce the number of trees in the ensemble * \correctchoice {To introduce randomness by considering a random subset of features for each tree} * To eliminate the need for decision trees in the ensemble \end {choice}}||exercise-83=={In boosting, how are the weights assigned to misclassified instances during training? \begin {choice} (1) * Equally to all instances * Proportional to the difficulty of the instance * \correctchoice {Sequentially, with higher weights for misclassified instances} * Inversely proportional to the number of features \end {choice}}||exercise-84=={Which ensemble method combines the predictions of base models by taking a weighted average, where the weights are learned based on the performance of each model? \begin {choice}(4) * Bagging * \correctchoice {Stacking} * Boosting * Random Forest \end {choice}}||exercise-85=={What is the primary advantage of ensemble methods over individual base models? \begin {choice} * Ensemble methods are always faster than individual models. * Ensemble methods can handle only linear relationships. * \correctchoice {Ensemble methods often generalize better and have improved robustness.} * Ensemble methods are more prone to overfitting. \end {choice}}||exercise-86=={In the context of boosting, what does the term "weak learner" refer to? \begin {choice} * A model with high training accuracy * \correctchoice {A model that performs slightly better than random chance} * A model with a large number of parameters * A model that is highly overfit \end {choice}}||exercise-87=={Which ensemble method trains multiple models independently on different subsets of the training data? \begin {choice}(4) * Boosting * Stacking * \correctchoice {Bagging} * Random Forest \end {choice}}||exercise-88=={What is bagging short for in the context of ensemble methods? \par \begin {choice}(4) * \correctchoice {Bootstrap Aggregating} * Boosting Algorithm * Bagged Aggregation * Batch Aggregation \end {choice}}||exercise-89=={Which ensemble method is known for building a sequence of weak learners, each correcting the errors of its predecessor? \begin {choice} (4) * Bagging * \correctchoice {AdaBoost} * Random Forest * Gradient Boosting \end {choice}}||exercise-90=={What is the primary advantage of ensemble methods over individual base models? \begin {choice} * Faster training time * \correctchoice {Improved generalization and robustness} * Lower computational complexity * Higher sensitivity to outliers \end {choice}}||exercise-91=={Which ensemble method is based on constructing a forest of decision trees with high diversity? \begin {choice}(4) * Bagging * AdaBoost * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-92=={What does the acronym "LSTM" stand for in the context of deep learning? \begin {choice}(2) * \correctchoice {Long Short-Term Memory} * Linear Short-Term Memory * Limited Short-Term Memory * Lasting Short-Term Memory \end {choice}}||exercise-93=={In boosting, what is the purpose of the learning rate parameter? \begin {choice} * It controls the number of weak learners \correctchoice {It adjusts the amount by which weights are updated during each iteration} * It determines the depth of decision trees * It sets the threshold for feature selection \end {choice}}||exercise-94=={What distinguishes Random Forest from traditional bagging techniques? \begin {choice} * Random Forest uses a single decision tree * Random Forest trains models sequentially * \correctchoice {Random Forest introduces randomness by considering a random subset of features for each tree} * Random Forest assigns equal weights to all instances \end {choice}}||exercise-95=={How does stacking differ from bagging and boosting in ensemble methods? \begin {choice} * Stacking trains models independently on different subsets * Stacking combines predictions using a weighted average * Stacking builds a sequence of weak learners * \correctchoice {Stacking uses multiple base models to form a meta-model} \end {choice}}||exercise-96=={What role does the concept of "bias-variance tradeoff" play in ensemble methods? \begin {choice} (1) * Ensemble methods eliminate the bias-variance tradeoff * Ensemble methods intensify the bias-variance tradeoff * \correctchoice {Ensemble methods help balance bias and variance} * Ensemble methods have no impact on bias and variance \end {choice}}||exercise-97=={What is the primary limitation of using too many weak learners in boosting? \begin {choice} (2) * \correctchoice {Increased risk of overfitting} * Decreased computational complexity * Improved generalization * Faster training time \end {choice}}||exercise-98=={In bagging, how are the subsets of the training data created for each base model? \begin {choice} * \correctchoice {Randomly and with replacement} * Randomly and without replacement * Sequentially and with replacement * Sequentially and without replacement \end {choice}}||exercise-99=={What is the primary advantage of using gradient boosting over traditional AdaBoost? \begin {choice} (2) * Faster convergence * \correctchoice {Better handling of outliers} * Reduced risk of overfitting * Simplicity in implementation \end {choice}}||exercise-100=={Which ensemble method is prone to becoming computationally expensive as the number of models increases? \begin {choice} (4) * Bagging * Stacking * \correctchoice {Boosting} * Random Forest \end {choice}}||exercise-101=={What does the term "stacking" refer to in ensemble learning? \begin {choice} * Combining models using a weighted average * Training models independently on different subsets * Constructing a sequence of weak learners * \correctchoice {Using multiple base models to form a meta-model} \end {choice}}||exercise-102=={Which ensemble method is known for its ability to handle both linear and non-linear relationships in the data? \begin {choice}(4) * Bagging * Stacking * \correctchoice {Random Forest} * Gradient Boosting \end {choice}}||exercise-103=={Explain the concept of "out-of-bag" error in the context of bagging. \begin {choice} * It is the error rate calculated on the training set * It is the error rate on the validation set * \correctchoice {It is an estimate of the test error obtained from the unused samples during training} * It is a measure of the model's performance on out-of-distribution data \end {choice}}||exercise-104=={What is the role of the hyperparameter "max depth" in decision trees within a Random Forest? \begin {choice} * It controls the number of trees in the forest * \correctchoice {It limits the maximum depth of individual decision trees} * It sets the learning rate for boosting * It adjusts the weights assigned to misclassified instances \end {choice}}||exercise-105=={In the context of ensemble methods, what is "early stopping," and how does it contribute to regularization? \begin {choice} * Early stopping involves terminating the training process when the model is underfitting, contributing to model simplicity. * \correctchoice {Early stopping prevents overfitting by stopping the training process when the model starts to memorize the training data.} * Early stopping introduces noise to the input data during training, preventing overfitting. * Early stopping is not related to regularization in ensemble methods. \end {choice}}||exercise-106=={What is the impact of increasing the number of base models on the computational complexity of stacking? \begin {choice} * The computational complexity decreases linearly * \correctchoice {The computational complexity increases linearly} * The computational complexity remains constant * The computational complexity depends on the type of base models used \end {choice}}||exercise-107=={Explain the concept of "adversarial training" in the context of ensemble methods. \begin {choice} * \correctchoice {Adversarial training involves training models to be robust against adversarial attacks.} * Adversarial training focuses on maximizing the accuracy on the training set. * Adversarial training eliminates the need for ensemble methods. * Adversarial training refers to using adversarial examples as additional training data. \end {choice}}||exercise-108=={How does the concept of "stacking with cross-validation" address the risk of overfitting in stacking? \begin {choice} * It eliminates the need for cross-validation in stacking. * \correctchoice {It uses multiple cross-validated models, reducing overfitting.} * It increases the depth of individual base models. * It has no impact on the risk of overfitting. \end {choice}}||exercise-109=={What is the primary drawback of using a high learning rate in boosting algorithms? \begin {choice}(2) * Slower convergence * \correctchoice {Increased risk of overfitting} * Decreased model performance * Improved generalization \end {choice}}||exercise-110=={Explain the concept of "feature importance" in the context of Random Forest. \begin {choice} * Feature importance represents the number of times a feature is selected by a base model. * \correctchoice {Feature importance indicates the relevance of a feature in predicting the target variable.} * Feature importance is not applicable to ensemble methods. * Feature importance measures the computational cost of using a specific feature. \end {choice}}||exercise-111=={What is the role of the "n estimators" hyperparameter in ensemble methods such as Random Forest and Gradient Boosting? \begin {choice} * It controls the learning rate in boosting algorithms. * It sets the maximum depth of individual decision trees. * \correctchoice {It specifies the number of base models in the ensemble.} * It determines the subset of features considered for each base model. \end {choice}}||exercise-112=={Explain the concept of "stacking with meta-features" in the context of ensemble methods. \begin {choice} * \correctchoice {Stacking with meta-features involves using the output of base models as features for a meta-model.} * Stacking with meta-features eliminates the need for multiple base models. * Stacking with meta-features refers to combining models using a weighted average. * Stacking with meta-features involves using only one type of base model in the ensemble. \end {choice}}||exercise-113=={What is Dropout in the context of neural networks? \begin {choice} * Adding noise to input features * \correctchoice {Removing random neurons during training} * Reducing the learning rate * Increasing the number of hidden layers \end {choice}}||exercise-114=={What is the main purpose of Dropout in neural networks? \begin {choice} * To increase overfitting * To speed up the training process * \correctchoice {To prevent co-adaptation of neurons} * To eliminate the need for activation functions \end {choice}}||exercise-115=={Which of the following statements is true about the application of Dropout during training? \begin {choice} * Dropout is only applied to input layers * \correctchoice {Dropout is applied to all layers except the output layer} * Dropout is applied during both training and testing * Dropout is never applied to neural networks \end {choice}}||exercise-116=={How does Dropout contribute to regularization in neural networks? \begin {choice} * By increasing the number of parameters * By introducing noise to the input data * \correctchoice {By reducing the model's capacity} * By promoting co-adaptation of neurons \end {choice}}||exercise-117=={In terms of training, what does it mean if a neuron is "dropped out"? \begin {choice} * The neuron's weights are set to zero * \correctchoice {The neuron is removed from the network temporarily} * The neuron's activation function is bypassed * The neuron's output is squared \end {choice}}||exercise-118=={What challenge does Dropout aim to address in neural networks? \begin {choice} (4) * Underfitting * \correctchoice {Overfitting} * Vanishing gradients * Exploding gradients \end {choice}}||exercise-119=={How does Dropout affect the training time of a neural network? \begin {choice} * \correctchoice {Slows down the training process} * Speeds up the training process * No impact on training time * Depends on the type of activation function used \end {choice}}||exercise-120=={What is the recommended range for Dropout rates in neural networks? \begin {choice} (4) * 0.0 to 0.1 * \correctchoice {0.2 to 0.5} * 0.5 to 0.8 * 0.9 to 1.0 \end {choice}}||exercise-121=={How does Dropout contribute to model generalization? \begin {choice} * By memorizing the training data * By promoting co-adaptation of neurons * \correctchoice {By reducing the sensitivity of neurons to specific input features} * By increasing the number of hidden layers \end {choice}}||exercise-122=={When applying Dropout, which phase is used for adjusting the weights of the neural network? \begin {choice} * \correctchoice {Training phase} * Testing phase * Both training and testing phases * Neither training nor testing phases \end {choice}}||exercise-123=={Explain the term "co-adaptation of neurons" in the context of neural networks and how Dropout addresses it. \begin {choice} * \correctchoice {Co-adaptation refers to neurons relying too much on each other, and Dropout breaks these dependencies by randomly dropping neurons during training.} * Co-adaptation is a form of regularization, and Dropout exacerbates co-adaptation by introducing noise. * Co-adaptation occurs when neurons are independent, and Dropout enforces co-adaptation by removing dependencies. * Co-adaptation is unrelated to Dropout; Dropout only affects the learning rate. \end {choice}}||exercise-124=={How does the effectiveness of Dropout vary with the size and complexity of a neural network? \begin {choice} * Dropout is more effective in small and simple networks * \correctchoice {Dropout is more effective in large and complex networks} * Dropout is equally effective across all network sizes and complexities * Dropout is irrelevant to network size and complexity \end {choice}}||exercise-125=={What is the relationship between Dropout and the concept of ensemble learning? \begin {choice} * Dropout is a type of ensemble learning * Ensemble learning and Dropout are unrelated concepts * \correctchoice {Dropout and ensemble learning achieve the same result in terms of model diversity} * Dropout eliminates the need for ensemble learning \end {choice}}||exercise-126=={Explain the trade-off between using a high Dropout rate and a low Dropout rate in neural networks. \begin {choice} * \correctchoice {High Dropout rates lead to overfitting, while low Dropout rates may result in underfitting.} * High Dropout rates always improve model generalization, while low Dropout rates reduce model capacity. * There is no trade-off; the Dropout rate does not impact model performance. * The trade-off depends on the type of activation function used in the network. \end {choice}}||exercise-127=={How does Dropout contribute to mitigating the vanishing gradient problem in deep neural networks? \begin {choice} * a. By increasing the learning rate * By preventing co-adaptation of neurons * \correctchoice {By introducing noise to the input data} * By reducing the sensitivity of neurons to specific input features \end {choice}}||exercise-128=={What is the primary goal of data augmentation in machine learning? \begin {choice} * To decrease the size of the dataset * To increase the computational complexity * \correctchoice {To improve model performance by increasing the diversity of the training data} * To eliminate the need for validation data \end {choice}}||exercise-129=={Which of the following is a common technique used in data augmentation for image data? \begin {choice} (2) * Principal Component Analysis (PCA) * Feature scaling * \correctchoice {Image rotation} * Lasso regularization \end {choice}}||exercise-130=={How does data augmentation contribute to preventing overfitting in machine learning models? \begin {choice} * By reducing the size of the training dataset * By increasing the number of layers in the model * By introducing noise to the input data * \correctchoice {By providing a more diverse set of training examples} \end {choice}}||exercise-131=={In text data augmentation, what technique involves replacing words with their synonyms? \begin {choice}(4) * Tokenization * Embedding * \correctchoice {Word substitution} * Lemmatization \end {choice}}||exercise-132=={Which of the following is a disadvantage of data augmentation? \begin {choice} * Increased model generalization * \correctchoice {Potential introduction of unrealistic patterns} * Improved model robustness * Decreased computational efficiency \end {choice}}||exercise-133=={What is the purpose of random cropping in image data augmentation? \begin {choice} * To decrease the image resolution * To remove irrelevant features from the image * \correctchoice {To create variations in the spatial location of objects} * To increase the image contrast \end {choice}}||exercise-134=={Which type of data augmentation is commonly used for time series data? \begin {choice}(4) * Image rotation * \correctchoice {Time warping} * Word substitution * Feature scaling \end {choice}}||exercise-135=={Explain the concept of "jittering" in the context of data augmentation. \begin {choice} * \correctchoice {Jittering refers to the introduction of noise to input features} * Jittering involves the random selection of a subset of data points * Jittering is a synonym for image rotation * Jittering is irrelevant to data augmentation \end {choice}}||exercise-136=={In the context of image data augmentation, what is the purpose of horizontal flipping? \begin {choice} (2) * To rotate images clockwise * \correctchoice {To create mirror images} * To adjust the image brightness * To resize images \end {choice}}||exercise-137=={How does data augmentation differ from feature engineering? \begin {choice} * \correctchoice {Data augmentation focuses on creating new samples, while feature engineering manipulates existing features.} * Feature engineering is limited to image data, while data augmentation is applicable to all data types. * Data augmentation involves scaling features, while feature engineering involves randomization. * Feature engineering and data augmentation are synonymous terms. \end {choice}}||exercise-138=={What is the role of dropout in the context of data augmentation? \begin {choice} (1) * Dropout is not related to data augmentation * \correctchoice {Dropout enhances data augmentation by randomly removing features during training} * Dropout is a type of data augmentation technique * Dropout prevents data augmentation from introducing unrealistic patterns \end {choice}}||exercise-139=={Which data augmentation technique is commonly used for audio data to introduce variations in pitch? \begin {choice} (2) * Time warping * \correctchoice {Spectrogram augmentation} * Random cropping * Jittering \end {choice}}||exercise-140=={What is the purpose of elastic deformation in image data augmentation? \begin {choice} * To adjust the image contrast * \correctchoice {To introduce non-linear distortions to the image} * To resize the image * To rotate the image \end {choice}}||exercise-141=={In natural language processing, which technique involves randomly removing words from sentences during data augmentation? \begin {choice} (1) * Tokenization * Word substitution * Sentence splitting * \correctchoice {Sentence dropout} \end {choice}}||exercise-142=={Explain the concept of "adversarial training" in the context of data augmentation and how it addresses robustness. \begin {choice} (1) * \correctchoice {Adversarial training focuses on creating adversarial examples to test the model's robustness against unseen patterns introduced by data augmentation.} * Adversarial training is irrelevant to data augmentation. * Adversarial training involves increasing the size of the training set. * Adversarial training enhances data augmentation by introducing adversarial noise during the augmentation process. \end {choice}}||exercise-143=={How does data augmentation contribute to handling class imbalance in classification tasks? \begin {choice} (1) * Data augmentation exacerbates class imbalance * Data augmentation is not related to class imbalance * \correctchoice {Data augmentation generates additional samples for minority classes, addressing class imbalance} * Data augmentation reduces the need for addressing class imbalance \end {choice}}||exercise-144=={What challenges might arise when applying data augmentation to non-image data types, such as tabular data? \begin {choice}(1) * Difficulty in implementing data augmentation for non-image data * Limited applicability of data augmentation to non-image data * \correctchoice {The potential introduction of unrealistic patterns} * No challenges; data augmentation is equally effective for all data types \end {choice}}||exercise-145=={Explain the term "mixup" in the context of data augmentation and how it differs from traditional augmentation techniques. \begin {choice}(1) * \correctchoice {Mixup involves blending two or more samples, creating new synthetic samples with averaged labels.} * Mixup is a synonym for image rotation. * Mixup refers to the addition of random noise to input features. * Mixup is irrelevant to data augmentation. \end {choice}}||exercise-146=={How does data augmentation impact the interpretability of machine learning models? \begin {choice} (1) * Data augmentation improves model interpretability by providing more diverse training examples. * Data augmentation has no impact on model interpretability. * \correctchoice {Data augmentation reduces model interpretability due to the introduction of synthetic samples.} * Data augmentation improves model interpretability by eliminating the need for validation data. \end {choice}}||exercise-147=={What is the role of "cutout" in image data augmentation? \begin {choice} (1) * \correctchoice {To remove random portions from images} * To blur the edges of images * To rotate images * To resize images \end {choice}}||exercise-148=={In the context of data augmentation, explain how the technique of "shearing" is applied to image data. \begin {choice} (1) * Shearing involves adjusting the brightness of images. * Shearing is irrelevant to data augmentation. * \correctchoice {Shearing introduces non-linear distortions to the image by tilting it along one of its axes.} * Shearing is a synonym for image rotation. \end {choice}}||exercise-149=={Which ensemble learning algorithm can be applied to both regression and classification tasks? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Random Forest} * Stacking \end {choice}}||exercise-150=={Ensemble learning algorithms can be computationally expensive when: \begin {choice} (2) * The dataset is small * The base models are simple * The ensemble size is small * \correctchoice {The dataset is large} \end {choice}}||exercise-151=={Which ensemble learning algorithm can be used to identify important features in a dataset? \begin {choice} (4) * Bagging * AdaBoost * \correctchoice {Gradient Boosting} * Stacking \end {choice}}||exercise-152=={The difference between deep learning and machine learning algorithms is that there is no need of feature engineering in machine learning algorithms, whereas, it is recommended to do feature engineering first and then apply deep learning. \begin {choice} (2) * True * \correctchoice {False} \end {choice}}||exercise-153=={Which of the following is a representation learning algorithm? \begin {choice} (4) * \correctchoice {Neural Network} * Random Forest * k-Nearest neighbor * None of the above \end {choice}}||exercise-154=={Which of the following option is correct for the below-mentioned techniques?\\ 1.AdaGrad uses first order differentiation\\ 2.L-BFGS uses second order differentiation\\ 3.AdaGrad uses second order differentiation\\ 4.L-BFGS uses first order differentiation \begin {choice} (4) * \correctchoice {1 and 2} * 3 and 4 * 1 and 4 * 2 and 3 \end {choice}}||exercise-155=={Increase in size of a convolutional kernel would necessarily increase the performance of a convolutional neural network. \begin {choice} (2) *True * \correctchoice {False} \end {choice}}||exercise-156=={Suppose we have a deep neural network model which was trained on a vehicle detection problem. The dataset consisted of images on cars and trucks and the aim was to detect name of the vehicle (the number of classes of vehicles are 10). Now you want to use this model on different dataset which has images of only Ford Mustangs (aka car) and the task is to locate the car in an image.\\ \par Which of the following categories would be suitable for this type of problem? \begin {choice} (4) * \correctchoice {Fine tune only the last couple of layers and change the last layer (classification layer) to regression layer} * Freeze all the layers except the last, re-train the last layer * Re-train the model for the new dataset * None of these \end {choice}}||exercise-157=={Suppose you have 5 convolutional kernel of size 7 x 7 with zero padding and stride 1 in the first layer of a convolutional neural network. You pass an input of dimension 224 x 224 x 3 through this layer. What are the dimensions of the data which the next layer will receive? \begin {choice} (4) * 217x217x3 * 217x217x8 *\correctchoice {218x218x5} * 220x220x7 \end {choice}}||exercise-158=={Suppose you replace the ReLU activation function with linear activation in a neural network that was originally able to approximate an XNOR function with ReLU activations. Will the new neural network be able to approximate an XNOR function? \begin {choice} (2) * Yes *\correctchoice {No} \end {choice}}||exercise-159=={If a 5-layer neural network takes 3 hours to train on a GPU with 4GB VRAM and at test time, it takes 2 seconds for a single data point, what would be the testing time for the new architecture if dropout is added after the 2nd and 4th layers with rates 0.2 and 0.3, respectively? \begin {choice} (4) * Less than 2 secs *\correctchoice {Exactly 2 secs} * Greater than 2 secs * Can not Say \end {choice}}||exercise-160=={Which of the following options can be used to reduce overfitting in deep learning models? \begin {choice} (5) * Add more data * Use data augmentation * Use architecture that generalizes well * Add regularization * Reduce architectural complexity *\correctchoice {All of these} \end {choice}}||exercise-161=={Perplexity is a commonly used evaluation technique when applying deep learning for NLP tasks. Which of the following statements is correct? \begin {choice} (2) * Higher the perplexity the better *\correctchoice {Lower the perplexity the better} \end {choice}}||exercise-162=={Suppose an input to Max-Pooling layer is given above. The pooling size of neurons in the layer is (3, 3).\\ \par \includegraphics [width=0.5\textwidth ,height=.5\textwidth ]{Q_table} \par \begin {choice} (4) * 3 * 5 * 5.5 *\correctchoice {7} \end {choice}}||exercise-163=={If we remove the ReLU layers, we can still use this neural network to model non-linear functions.\\ \par \includegraphics [width=0.5\textwidth ,height=.5\textwidth ]{nn_layer} \begin {choice} (2) * True *\correctchoice {False} \end {choice}}||exercise-164=={Deep learning can be applied to which of the following NLP tasks? \begin {choice} (4) * Machine translation * Sentiment analysis * Question Answering system *\correctchoice {All of the above} \end {choice}}||exercise-165=={Scenario 1: You are given data of the map of Arcadia city, with aerial photographs of the city and its outskirts. The task is to segment the areas into industrial land, farmland, and natural landmarks like rivers, mountains, etc. \\ \par Scenario 2: You are given data of the map of Arcadia city, with detailed roads and distances between landmarks. This is represented as a graph structure. The task is to find out the nearest distance between two landmarks.\\ \par Can deep learning be applied to Scenario 1 but not Scenario 2? \begin {choice} (2) * TRUE *\correctchoice {FALSE} \end {choice}}||exercise-166=={Which of the following is a data augmentation technique used in image recognition tasks? \begin {choice} (7) * Horizontal flipping * Random cropping * Random scaling * Color jittering * Random translation * Random shearing *\correctchoice {All of these} \end {choice}}||exercise-167=={Given an n-character word, we want to predict which character would be the n+1th character in the sequence. For example, our input is “predictio” (which is a 9-character word) and we have to predict what would be the 10th character.\\ \par Which neural network architecture would be suitable to complete this task? \begin {choice} (4) * Fully-Connected Neural Network * Convolutional Neural Network *\correctchoice {Recurrent Neural Network} * Restricted Boltzmann Machine \end {choice}}||exercise-168=={What is generally the sequence followed when building a neural network architecture for semantic segmentation for an image? \begin {choice} (2) *\correctchoice {Convolutional network on input and deconvolutional network on output} * Deconvolutional network on input and convolutional network on output \end {choice}}||exercise-169=={A ReLU unit in neural network never gets saturated. \begin {choice} (2) *True *\correctchoice {False} \end {choice}}||exercise-170=={What is the relationship between dropout rate and regularization? \begin {choice} (2) * Higher the dropout rate, higher is the regularization *\correctchoice {Higher the dropout rate, lower is the regularization} \end {choice}}||exercise-171=={What is the technical difference between vanilla backpropagation algorithm and backpropagation through time (BPTT) algorithm? \begin {choice} (2) *\correctchoice {Unlike backprop, in BPTT we sum up gradients for corresponding weight for each time step} * Unlike backprop, in BPTT we subtract gradients for corresponding weight for each time step \end {choice}}||exercise-172=={Exploding gradient problem is an issue in training deep networks where the gradient gets so large that the loss goes to an infinitely high value and then explodes.\\ What is the probable approach when dealing with the "Exploding Gradient" problem in RNNs? \begin {choice} (2) * Use modified architectures like LSTM and GRUs *\correctchoice {Gradient clipping} * Dropout * None of these \end {choice}}||exercise-173=={There are many types of gradient descent algorithms. Two of the most notable ones are l-BFGS and SGD. l-BFGS is a second-order gradient descent technique whereas SGD is a first-order gradient descent technique.\\ In which of the following scenarios would you prefer l-BFGS over SGD? \begin {choice} (4) * Data is sparse * Number of parameters of neural network are small * \correctchoice {Both of them} * None of these \end {choice}}||exercise-174=={Which of the following is not a direct prediction technique for NLP tasks? \begin {choice} (2) * Recurrent Neural Network * Skip-gram model *\correctchoice {PCA} * Convolutional Neural Network \end {choice}}||exercise-175=={Which of the following would be the best for a non-continuous objective during optimization in deep neural net? \begin {choice} (4) * L-BFGS * SGD * AdaGrad *\correctchoice {Subgradient method} \end {choice}}||exercise-176=={Which of the following is correct? \begin {choice} (4) * Dropout randomly masks the input weights to a neuron * Dropconnect randomly masks both input and output weights to a neuron *\correctchoice {1 is False and 2 is True} * Both 1 and 2 are True \end {choice}}||exercise-177=={While training a neural network for image recognition task, we plot the graph of training error and validation error for debugging. \includegraphics [width=.5\textwidth , height=.5\textwidth ]{DL4} \begin {choice} (4) * A * B *\correctchoice {C} * D \end {choice}}||exercise-178=={Backpropagation works by first calculating the gradient of \_\_\_ and then propagating it backward. \begin {choice} (4) * Sum of squared error with respect to inputs * Sum of squared error with respect to weights *\correctchoice {Sum of squared error with respect to outputs} * None of the above \end {choice}}||exercise-179=={Mini-Batch sizes when defining a neural network are preferred to be multiples of 2's such as 256 or 512. What is the reason behind it? \begin {choice} (4) * Gradient descent optimizes best when you use an even number *\correctchoice {Parallelization of the neural network is best when the memory is used optimally} * Losses are erratic when you don't use an even number * None of these \end {choice}}||exercise-180=={As the length of a sentence increases, it becomes harder for a neural translation machine to perform as sentence meaning is represented by a fixed dimensional vector. To solve this, which of the following could we do? \begin {choice} (4) * Use recursive units instead of recurrent *\correctchoice {Use attention mechanism} * Use character-level translation * None of these \end {choice}}||exercise-181=={A recurrent neural network can be unfolded into a fully connected neural network with infinite length. \begin {choice} (2) *\correctchoice {TRUE} * FALSE \end {choice}}||exercise-182=={Which of the following is a bottleneck for deep learning algorithms? \begin {choice} (4) * Data related to the problem * CPU to GPU communication * GPU memory *\correctchoice {All of the above} \end {choice}}||exercise-183=={When deriving a memory cell in memory networks, we choose to read values as vector values instead of scalars. Which type of addressing would this entail? \begin {choice} (2) *\correctchoice {Content-based addressing} * Location-based addressing \end {choice}}||exercise-184=={It is generally recommended to replace pooling layers in the generator part of convolutional generative adversarial nets with \_\_\_\_\_\_\_\_\_\_? \begin {choice} (4) * Affine layer * Strided convolutional layer *\correctchoice {Fractional strided convolutional layer} * ReLU layer \end {choice}}||exercise-185=={Which of the following statements is true with respect to GRU? \begin {choice} (4) *Units with short-term dependencies have a very active reset gate. *Units with long-term dependencies have a very active update gate. * None of them * \correctchoice {Both 1 and 2} \end {choice}}||exercise-186=={If the calculation of the reset gate in a GRU unit is close to 0, which of the following would occur? \begin {choice} (2) *\correctchoice {Previous hidden state would be ignored} * Previous hidden state would not be ignored \end {choice}}||exercise-187=={If the calculation of the update gate in a GRU unit is close to 1, which of the following would occur? \begin {choice} (2) * Forgets the information for future time steps *\correctchoice {Copies the information through many time steps} \end {choice}}||exercise-188=={Dropout technique is not an advantageous technique for which of the following layers? \begin {choice} (4) * Affine layer * Convolutional layer *\correctchoice {RNN layer} * None of these \end {choice}}||exercise-189=={Suppose your task is to predict the next few notes of a song when you are given the preceding segment of the song. Which architecture of a neural network would be better suited to solve the problem? \begin {choice} (4) * End-to-End fully connected neural network * \correctchoice {CNN followed by recurrent units} * Neural Turing Machine * None of these \end {choice}}||exercise-190=={What is the primary purpose of a Convolutional Neural Network (CNN)? \begin {choice} (4) * Object detection * \correctchoice {Image classification} * Text generation * Reinforcement learning \end {choice}}||exercise-191=={Which layer type is typically used to extract local features in a CNN? \begin {choice} (4) * \correctchoice {Convolutional layer} * Pooling layer * Fully connected layer * Activation layer \end {choice}}||exercise-192=={What is the advantage of using convolutional layers in a CNN? \begin {choice} (4) * \correctchoice {They can capture local spatial patterns in the input data} * They can handle sequential data * They can generate synthetic data * They can capture local spatial patterns in the input data \end {choice}}||exercise-193=={What is the purpose of the pooling layer in a CNN? \begin {choice} (4) * To introduce non-linearity to the network * \correctchoice {To reduce the spatial dimensions of the feature maps} * To adjust the weights and biases of the network * To compute the gradients for backpropagation \end {choice}}||exercise-194=={Which activation function is commonly used in the convolutional layers of a CNN? \begin {choice} (4) * Sigmoid * \correctchoice {ReLU (Rectified Linear Unit)} * Tanh (Hyperbolic Tangent) * Softmax \end {choice}}||exercise-195=={What is the purpose of the stride parameter in a convolutional layer? \begin {choice} (4) * To determine the size of the receptive field * \correctchoice {To control the step size of the convolution operation} * To adjust the learning rate during training * None of the above \end {choice}}||exercise-196=={Which layer type is used to reduce the spatial dimensions in a CNN? \begin {choice} (4) * Convolutional layer * \correctchoice {Pooling layer} * Fully connected layer * Activation layer \end {choice}}||exercise-197=={What is the purpose of the padding parameter in a convolutional layer? \begin {choice} (4) * To adjust the learning rate during training * \correctchoice {To prevent the reduction of spatial dimensions} * To regularize the network and prevent overfitting * None of the above \end {choice}}||exercise-198=={Which layer type is responsible for making final predictions in a CNN? \begin {choice} (4) * Convolutional layer * Pooling layer * \correctchoice {Fully connected layer} * Activation layer \end {choice}}||exercise-199=={What is the purpose of the fully connected layers in a CNN? \begin {choice} (4) * \correctchoice {To capture global patterns and make predictions} * To reduce the spatial dimensions of the input data * To apply non-linear transformations to the feature maps * To initialize the weights and biases of the network \end {choice}}||exercise-200=={Which layer type is responsible for applying non-linear transformations to the feature maps in a CNN? \begin {choice} (4) * Convolutional layer * Pooling layer * Fully connected layer * \correctchoice {Activation layer} \end {choice}}||exercise-201=={What is the purpose of dropout regularization in a CNN? \begin {choice} (4) * \correctchoice {To randomly disable neurons during training to prevent overfitting} * To adjust the learning rate during training * To increase the number of layers in the network * None of the above \end {choice}}||exercise-202=={Which layer type is responsible for backpropagating the gradients and updating the network's parameters in a CNN? \begin {choice} (4) * Convolutional layer * Pooling layer * \correctchoice {Fully connected layer} * Activation layer \end {choice}}||exercise-203=={What is the primary advantage of using a CNN over a fully connected neural network for image processing tasks? \begin {choice} (4) * CNNs have a higher training speed * CNNs can handle sequential data * CNNs have a higher number of neurons * \correctchoice {CNNs can capture local spatial patterns in the input data} \end {choice}}||exercise-204=={Which layer type is responsible for parameter sharing in a CNN? \begin {choice} (4) * \correctchoice {Convolutional layer} * Pooling layer * Fully connected layer * Activation layer \end {choice}}||exercise-205=={What is the purpose of the receptive field in a convolutional layer? \begin {choice} (4) * To determine the number of filters in the layer * To determine the size of the feature maps * \correctchoice {To specify the size of the local region for the convolution operation} * None of the above \end {choice}}||exercise-206=={Which layer type is responsible for spatial downsampling in a CNN? \begin {choice} (4) * Convolutional layer * \correctchoice {Pooling layer} * Fully connected layer * Activation layer \end {choice}}||exercise-207=={What is the purpose of the filter/kernel in a convolutional layer? \begin {choice} (4) * To determine the number of neurons in the layer * To specify the size of the feature maps * \correctchoice {To extract local features from the input data} * None of the above \end {choice}}||exercise-208=={Which layer type is commonly used in CNNs to normalize the input data? \begin {choice} (4) * Convolutional layer * Pooling layer * \correctchoice {Batch normalization layer} * Activation layer \end {choice}}||exercise-209=={What is the primary goal of training a CNN? \begin {choice} (4) * \correctchoice {To minimize the prediction error on the training data} * To maximize the number of layers in the network * To achieve 100* None of the above \end {choice}}||exercise-210=={Which layer type is responsible for introducing translation invariance in a CNN? \begin {choice} (4) * \correctchoice {Convolutional layer} * Pooling layer * Fully connected layer * Activation layer \end {choice}}||exercise-211=={What is the purpose of the output layer in a CNN? \begin {choice} (4) * \correctchoice {To compute the predicted output based on the final feature representation} * To reduce the spatial dimensions of the input data * To apply non-linear transformations to the feature maps * To initialize the weights and biases of the network \end {choice}}||exercise-212=={What is the purpose of zero-padding in a CNN? \begin {choice} (4) * To adjust the learning rate during training * \correctchoice {To prevent the reduction of spatial dimensions} * To regularize the network and prevent overfitting * None of the above \end {choice}}||exercise-213=={Which layer type is commonly used in CNNs for semantic segmentation tasks? \begin {choice} (4) * Convolutional layer * Pooling layer * Fully connected layer * \correctchoice {Upsampling layer} \end {choice}}||exercise-214=={What is the purpose of the loss function in CNN training? \begin {choice} (4) * \correctchoice {To measure the prediction error and guide the learning process} * To initialize the weights and biases of the network * To adjust the learning rate during training * None of the above \end {choice}}||exercise-215=={Which layer type is commonly used in CNNs to introduce non-linearity? \begin {choice} (4) * Convolutional layer * Pooling layer * Fully connected layer * \correctchoice {Activation layer} \end {choice}}||exercise-216=={What is the purpose of the learning rate in CNN training? \begin {choice} (4) * \correctchoice {To control the step size of the parameter updates during optimization} * To adjust the size of the filters in the convolutional layers * To increase the number of layers in the network * None of the above \end {choice}}||exercise-217=={Which layer type is responsible for feature extraction in a CNN? \begin {choice} (4) * \correctchoice {Convolutional layer} * Pooling layer * Fully connected layer * Activation layer \end {choice}}||exercise-218=={What is the purpose of data augmentation in CNN training? \begin {choice} (4) * To increase the number of layers in the network * \correctchoice {To introduce noise and variations in the training data} * To adjust the learning rate during training * None of the above \end {choice}}||exercise-219=={Which layer type is commonly used in CNNs to handle variable-sized inputs? \begin {choice} (4) * Convolutional layer * Pooling layer * Fully connected layer * \correctchoice {None of the above} \end {choice}}||exercise-220=={What is the primary purpose of a Recurrent Neural Network (RNN)? \begin {choice} (4) * Image classification * \correctchoice {Text generation} * Reinforcement learning * Object detection \end {choice}}||exercise-221=={Which layer type is typically used to capture sequential dependencies in an RNN? \begin {choice} (4) * Input layer * \correctchoice {Hidden layer} * Output layer * Activation layer \end {choice}}||exercise-222=={What is the advantage of using recurrent layers in an RNN? \begin {choice} (4) * They can handle non-linear transformations * They can handle variable-length inputs * They can generate synthetic data * \correctchoice {They can capture temporal dependencies in the input data} \end {choice}}||exercise-223=={What is the purpose of the hidden state in an RNN? \begin {choice} (4) * \correctchoice {To store the information from the previous time step} * To adjust the learning rate during training * To compute the gradients for backpropagation * None of the above \end {choice}}||exercise-224=={Which activation function is commonly used in the recurrent layers of an RNN? \begin {choice} (4) * ReLU (Rectified Linear Unit) * Sigmoid * \correctchoice {Tanh (Hyperbolic Tangent)} * Softmax \end {choice}}||exercise-225=={What is the purpose of the time step parameter in an RNN? \begin {choice} (4) * To determine the number of recurrent layers in the network * To adjust the learning rate during training * \correctchoice {To specify the length of the input sequence} * None of the above \end {choice}}||exercise-226=={Which layer type is commonly used to initialize the hidden state in an RNN? \begin {choice} (4) * Input layer * \correctchoice {Hidden layer} * Output layer * Activation layer \end {choice}}||exercise-227=={What is the purpose of the bidirectional RNN architecture? \begin {choice} (4) * \correctchoice {To handle sequential data in both forward and backward directions} * To reduce the computational complexity of the network * To adjust the learning rate during training * None of the above \end {choice}}||exercise-228=={Which layer type is responsible for making final predictions in an RNN? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * Activation layer \end {choice}}||exercise-229=={What is the purpose of the recurrent connection in an RNN? \begin {choice} (4) * \correctchoice {To propagate the hidden state across different time steps} * To adjust the weights and biases of the network * To reduce the dimensionality of the input data * None of the above \end {choice}}||exercise-230=={Which layer type is commonly used in RNNs for sequence-to-sequence tasks? \begin {choice} (4) * Input layer * Hidden layer * Output layer * \correctchoice {Attention layer} \end {choice}}||exercise-231=={What is the purpose of the backpropagation through time (BPTT) algorithm in RNN training? \begin {choice} (4) * \correctchoice {To compute the gradients and update the network's parameters} * To adjust the learning rate during training * To prevent overfitting by regularizing the model * None of the above \end {choice}}||exercise-232=={Which layer type is commonly used in RNNs to handle variable-length inputs? \begin {choice} (4) * \correctchoice {Input layer} * Hidden layer * Output layer * None of the above \end {choice}}||exercise-233=={What is the purpose of the initial hidden state in an RNN? \begin {choice} (4) * \correctchoice {To provide the starting point for the recurrent computation} * To adjust the learning rate during training * To compute the gradients for backpropagation * None of the above \end {choice}}||exercise-234=={Which layer type is responsible for handling the output at each time step in an RNN? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * Activation layer \end {choice}}||exercise-235=={What is the purpose of the teacher forcing technique in RNN training? \begin {choice} (4) * To adjust the learning rate during training * \correctchoice {To propagate the gradients through time} * To reduce the computational complexity of the network * None of the above \end {choice}}||exercise-236=={Which layer type is commonly used in RNNs for language modeling tasks? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * None of the above \end {choice}}||exercise-237=={What is the purpose of the sequence-to-vector architecture in an RNN? \begin {choice} (4) * \correctchoice {To process an input sequence and produce a fixed-length representation} * To adjust the weights and biases of the network * To reduce the dimensionality of the input data * None of the above \end {choice}}||exercise-238=={Which layer type is responsible for introducing non-linearity in an RNN? \begin {choice} (4) * Input layer * Hidden layer * Output layer * \correctchoice {Activation layer} \end {choice}}||exercise-239=={What is the purpose of the forget gate in a Gated Recurrent Unit (GRU)? \begin {choice} (4) * \correctchoice {To control the flow of information from the previous hidden state} * To adjust the learning rate during training * To compute the gradients for backpropagation * None of the above \end {choice}}||exercise-240=={Which layer type is commonly used in RNNs for machine translation tasks? \begin {choice} (4) * Input layer * Hidden layer * Output layer * \correctchoice {Attention layer} \end {choice}}||exercise-241=={What is the purpose of the peephole connections in a Long Short-Term Memory (LSTM) network? \begin {choice} (4) * \correctchoice {To allow the cell state to influence the gating mechanisms} * To adjust the learning rate during training * To introduce non-linearity to the network * None of the above \end {choice}}||exercise-242=={Which layer type is responsible for handling variable-length outputs in an RNN? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * None of the above \end {choice}}||exercise-243=={What is the purpose of the cell state in an LSTM network? \begin {choice} (4) * \correctchoice {To store long-term dependencies in the input sequence} * To adjust the learning rate during training * To compute the gradients for backpropagation * None of the above \end {choice}}||exercise-244=={Which layer type is commonly used in RNNs for speech recognition tasks? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * None of the above \end {choice}}||exercise-245=={What is the purpose of the input gate in an LSTM network? \begin {choice} (4) * \correctchoice {To control the flow of information from the current input} * To adjust the learning rate during training * To introduce non-linearity to the network * None of the above \end {choice}}||exercise-246=={Which layer type is responsible for handling variable-length inputs and outputs in an RNN? \begin {choice} (4) * Input layer * Hidden layer * Output layer * \correctchoice {None of the above} \end {choice}}||exercise-247=={What is the purpose of the output gate in an LSTM network? \begin {choice} (4) * \correctchoice {To control the flow of information to the current output} * To adjust the learning rate during training * To introduce non-linearity to the network * None of the above \end {choice}}||exercise-248=={Which layer type is commonly used in RNNs for time series prediction tasks? \begin {choice} (4) * Input layer * Hidden layer * \correctchoice {Output layer} * None of the above \end {choice}}||exercise-249=={What is the purpose of the reset gate in a Gated Recurrent Unit (GRU)? \begin {choice} (4) * \correctchoice {To reset the hidden state based on the current input} * To adjust the learning rate during training * To introduce non-linearity to the network * None of the above \end {choice}}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7||8||9||10||11||12||13||14||15||16||17||18||19||20||21||22||23||24||25||26||27||28||29||30||31||32||33||34||35||36||37||38||39||40||41||42||43||44||45||46||47||48||49||50||51||52||53||54||55||56||57||58||59||60||61||62||63||64||65||66||67||68||69||70||71||72||73||74||75||76||77||78||79||80||81||82||83||84||85||86||87||88||89||90||91||92||93||94||95||96||97||98||99||100||101||102||103||104||105||106||107||108||109||110||111||112||113||114||115||116||117||118||119||120||121||122||123||124||125||126||127||128||129||130||131||132||133||134||135||136||137||138||139||140||141||142||143||144||145||146||147||148||149||150||151||152||153||154||155||156||157||158||159||160||161||162||163||164||165||166||167||168||169||170||171||172||173||174||175||176||177||178||179||180||181||182||183||184||185||186||187||188||189||190||191||192||193||194||195||196||197||198||199||200||201||202||203||204||205||206||207||208||209||210||211||212||213||214||215||216||217||218||219||220||221||222||223||224||225||226||227||228||229||230||231||232||233||234||235||236||237||238||239||240||241||242||243||244||245||246||247||248||249}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}||exercise-79=={true}||exercise-80=={true}||exercise-81=={true}||exercise-82=={true}||exercise-83=={true}||exercise-84=={true}||exercise-85=={true}||exercise-86=={true}||exercise-87=={true}||exercise-88=={true}||exercise-89=={true}||exercise-90=={true}||exercise-91=={true}||exercise-92=={true}||exercise-93=={true}||exercise-94=={true}||exercise-95=={true}||exercise-96=={true}||exercise-97=={true}||exercise-98=={true}||exercise-99=={true}||exercise-100=={true}||exercise-101=={true}||exercise-102=={true}||exercise-103=={true}||exercise-104=={true}||exercise-105=={true}||exercise-106=={true}||exercise-107=={true}||exercise-108=={true}||exercise-109=={true}||exercise-110=={true}||exercise-111=={true}||exercise-112=={true}||exercise-113=={true}||exercise-114=={true}||exercise-115=={true}||exercise-116=={true}||exercise-117=={true}||exercise-118=={true}||exercise-119=={true}||exercise-120=={true}||exercise-121=={true}||exercise-122=={true}||exercise-123=={true}||exercise-124=={true}||exercise-125=={true}||exercise-126=={true}||exercise-127=={true}||exercise-128=={true}||exercise-129=={true}||exercise-130=={true}||exercise-131=={true}||exercise-132=={true}||exercise-133=={true}||exercise-134=={true}||exercise-135=={true}||exercise-136=={true}||exercise-137=={true}||exercise-138=={true}||exercise-139=={true}||exercise-140=={true}||exercise-141=={true}||exercise-142=={true}||exercise-143=={true}||exercise-144=={true}||exercise-145=={true}||exercise-146=={true}||exercise-147=={true}||exercise-148=={true}||exercise-149=={true}||exercise-150=={true}||exercise-151=={true}||exercise-152=={true}||exercise-153=={true}||exercise-154=={true}||exercise-155=={true}||exercise-156=={true}||exercise-157=={true}||exercise-158=={true}||exercise-159=={true}||exercise-160=={true}||exercise-161=={true}||exercise-162=={true}||exercise-163=={true}||exercise-164=={true}||exercise-165=={true}||exercise-166=={true}||exercise-167=={true}||exercise-168=={true}||exercise-169=={true}||exercise-170=={true}||exercise-171=={true}||exercise-172=={true}||exercise-173=={true}||exercise-174=={true}||exercise-175=={true}||exercise-176=={true}||exercise-177=={true}||exercise-178=={true}||exercise-179=={true}||exercise-180=={true}||exercise-181=={true}||exercise-182=={true}||exercise-183=={true}||exercise-184=={true}||exercise-185=={true}||exercise-186=={true}||exercise-187=={true}||exercise-188=={true}||exercise-189=={true}||exercise-190=={true}||exercise-191=={true}||exercise-192=={true}||exercise-193=={true}||exercise-194=={true}||exercise-195=={true}||exercise-196=={true}||exercise-197=={true}||exercise-198=={true}||exercise-199=={true}||exercise-200=={true}||exercise-201=={true}||exercise-202=={true}||exercise-203=={true}||exercise-204=={true}||exercise-205=={true}||exercise-206=={true}||exercise-207=={true}||exercise-208=={true}||exercise-209=={true}||exercise-210=={true}||exercise-211=={true}||exercise-212=={true}||exercise-213=={true}||exercise-214=={true}||exercise-215=={true}||exercise-216=={true}||exercise-217=={true}||exercise-218=={true}||exercise-219=={true}||exercise-220=={true}||exercise-221=={true}||exercise-222=={true}||exercise-223=={true}||exercise-224=={true}||exercise-225=={true}||exercise-226=={true}||exercise-227=={true}||exercise-228=={true}||exercise-229=={true}||exercise-230=={true}||exercise-231=={true}||exercise-232=={true}||exercise-233=={true}||exercise-234=={true}||exercise-235=={true}||exercise-236=={true}||exercise-237=={true}||exercise-238=={true}||exercise-239=={true}||exercise-240=={true}||exercise-241=={true}||exercise-242=={true}||exercise-243=={true}||exercise-244=={true}||exercise-245=={true}||exercise-246=={true}||exercise-247=={true}||exercise-248=={true}||exercise-249=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}||exercise-79=={true}||exercise-80=={true}||exercise-81=={true}||exercise-82=={true}||exercise-83=={true}||exercise-84=={true}||exercise-85=={true}||exercise-86=={true}||exercise-87=={true}||exercise-88=={true}||exercise-89=={true}||exercise-90=={true}||exercise-91=={true}||exercise-92=={true}||exercise-93=={true}||exercise-94=={true}||exercise-95=={true}||exercise-96=={true}||exercise-97=={true}||exercise-98=={true}||exercise-99=={true}||exercise-100=={true}||exercise-101=={true}||exercise-102=={true}||exercise-103=={true}||exercise-104=={true}||exercise-105=={true}||exercise-106=={true}||exercise-107=={true}||exercise-108=={true}||exercise-109=={true}||exercise-110=={true}||exercise-111=={true}||exercise-112=={true}||exercise-113=={true}||exercise-114=={true}||exercise-115=={true}||exercise-116=={true}||exercise-117=={true}||exercise-118=={true}||exercise-119=={true}||exercise-120=={true}||exercise-121=={true}||exercise-122=={true}||exercise-123=={true}||exercise-124=={true}||exercise-125=={true}||exercise-126=={true}||exercise-127=={true}||exercise-128=={true}||exercise-129=={true}||exercise-130=={true}||exercise-131=={true}||exercise-132=={true}||exercise-133=={true}||exercise-134=={true}||exercise-135=={true}||exercise-136=={true}||exercise-137=={true}||exercise-138=={true}||exercise-139=={true}||exercise-140=={true}||exercise-141=={true}||exercise-142=={true}||exercise-143=={true}||exercise-144=={true}||exercise-145=={true}||exercise-146=={true}||exercise-147=={true}||exercise-148=={true}||exercise-149=={true}||exercise-150=={true}||exercise-151=={true}||exercise-152=={true}||exercise-153=={true}||exercise-154=={true}||exercise-155=={true}||exercise-156=={true}||exercise-157=={true}||exercise-158=={true}||exercise-159=={true}||exercise-160=={true}||exercise-161=={true}||exercise-162=={true}||exercise-163=={true}||exercise-164=={true}||exercise-165=={true}||exercise-166=={true}||exercise-167=={true}||exercise-168=={true}||exercise-169=={true}||exercise-170=={true}||exercise-171=={true}||exercise-172=={true}||exercise-173=={true}||exercise-174=={true}||exercise-175=={true}||exercise-176=={true}||exercise-177=={true}||exercise-178=={true}||exercise-179=={true}||exercise-180=={true}||exercise-181=={true}||exercise-182=={true}||exercise-183=={true}||exercise-184=={true}||exercise-185=={true}||exercise-186=={true}||exercise-187=={true}||exercise-188=={true}||exercise-189=={true}||exercise-190=={true}||exercise-191=={true}||exercise-192=={true}||exercise-193=={true}||exercise-194=={true}||exercise-195=={true}||exercise-196=={true}||exercise-197=={true}||exercise-198=={true}||exercise-199=={true}||exercise-200=={true}||exercise-201=={true}||exercise-202=={true}||exercise-203=={true}||exercise-204=={true}||exercise-205=={true}||exercise-206=={true}||exercise-207=={true}||exercise-208=={true}||exercise-209=={true}||exercise-210=={true}||exercise-211=={true}||exercise-212=={true}||exercise-213=={true}||exercise-214=={true}||exercise-215=={true}||exercise-216=={true}||exercise-217=={true}||exercise-218=={true}||exercise-219=={true}||exercise-220=={true}||exercise-221=={true}||exercise-222=={true}||exercise-223=={true}||exercise-224=={true}||exercise-225=={true}||exercise-226=={true}||exercise-227=={true}||exercise-228=={true}||exercise-229=={true}||exercise-230=={true}||exercise-231=={true}||exercise-232=={true}||exercise-233=={true}||exercise-234=={true}||exercise-235=={true}||exercise-236=={true}||exercise-237=={true}||exercise-238=={true}||exercise-239=={true}||exercise-240=={true}||exercise-241=={true}||exercise-242=={true}||exercise-243=={true}||exercise-244=={true}||exercise-245=={true}||exercise-246=={true}||exercise-247=={true}||exercise-248=={true}||exercise-249=={true}}
\XSIM{total-number}{249}
\XSIM{exercise}{249}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}||8=={exercise}||9=={exercise}||10=={exercise}||11=={exercise}||12=={exercise}||13=={exercise}||14=={exercise}||15=={exercise}||16=={exercise}||17=={exercise}||18=={exercise}||19=={exercise}||20=={exercise}||21=={exercise}||22=={exercise}||23=={exercise}||24=={exercise}||25=={exercise}||26=={exercise}||27=={exercise}||28=={exercise}||29=={exercise}||30=={exercise}||31=={exercise}||32=={exercise}||33=={exercise}||34=={exercise}||35=={exercise}||36=={exercise}||37=={exercise}||38=={exercise}||39=={exercise}||40=={exercise}||41=={exercise}||42=={exercise}||43=={exercise}||44=={exercise}||45=={exercise}||46=={exercise}||47=={exercise}||48=={exercise}||49=={exercise}||50=={exercise}||51=={exercise}||52=={exercise}||53=={exercise}||54=={exercise}||55=={exercise}||56=={exercise}||57=={exercise}||58=={exercise}||59=={exercise}||60=={exercise}||61=={exercise}||62=={exercise}||63=={exercise}||64=={exercise}||65=={exercise}||66=={exercise}||67=={exercise}||68=={exercise}||69=={exercise}||70=={exercise}||71=={exercise}||72=={exercise}||73=={exercise}||74=={exercise}||75=={exercise}||76=={exercise}||77=={exercise}||78=={exercise}||79=={exercise}||80=={exercise}||81=={exercise}||82=={exercise}||83=={exercise}||84=={exercise}||85=={exercise}||86=={exercise}||87=={exercise}||88=={exercise}||89=={exercise}||90=={exercise}||91=={exercise}||92=={exercise}||93=={exercise}||94=={exercise}||95=={exercise}||96=={exercise}||97=={exercise}||98=={exercise}||99=={exercise}||100=={exercise}||101=={exercise}||102=={exercise}||103=={exercise}||104=={exercise}||105=={exercise}||106=={exercise}||107=={exercise}||108=={exercise}||109=={exercise}||110=={exercise}||111=={exercise}||112=={exercise}||113=={exercise}||114=={exercise}||115=={exercise}||116=={exercise}||117=={exercise}||118=={exercise}||119=={exercise}||120=={exercise}||121=={exercise}||122=={exercise}||123=={exercise}||124=={exercise}||125=={exercise}||126=={exercise}||127=={exercise}||128=={exercise}||129=={exercise}||130=={exercise}||131=={exercise}||132=={exercise}||133=={exercise}||134=={exercise}||135=={exercise}||136=={exercise}||137=={exercise}||138=={exercise}||139=={exercise}||140=={exercise}||141=={exercise}||142=={exercise}||143=={exercise}||144=={exercise}||145=={exercise}||146=={exercise}||147=={exercise}||148=={exercise}||149=={exercise}||150=={exercise}||151=={exercise}||152=={exercise}||153=={exercise}||154=={exercise}||155=={exercise}||156=={exercise}||157=={exercise}||158=={exercise}||159=={exercise}||160=={exercise}||161=={exercise}||162=={exercise}||163=={exercise}||164=={exercise}||165=={exercise}||166=={exercise}||167=={exercise}||168=={exercise}||169=={exercise}||170=={exercise}||171=={exercise}||172=={exercise}||173=={exercise}||174=={exercise}||175=={exercise}||176=={exercise}||177=={exercise}||178=={exercise}||179=={exercise}||180=={exercise}||181=={exercise}||182=={exercise}||183=={exercise}||184=={exercise}||185=={exercise}||186=={exercise}||187=={exercise}||188=={exercise}||189=={exercise}||190=={exercise}||191=={exercise}||192=={exercise}||193=={exercise}||194=={exercise}||195=={exercise}||196=={exercise}||197=={exercise}||198=={exercise}||199=={exercise}||200=={exercise}||201=={exercise}||202=={exercise}||203=={exercise}||204=={exercise}||205=={exercise}||206=={exercise}||207=={exercise}||208=={exercise}||209=={exercise}||210=={exercise}||211=={exercise}||212=={exercise}||213=={exercise}||214=={exercise}||215=={exercise}||216=={exercise}||217=={exercise}||218=={exercise}||219=={exercise}||220=={exercise}||221=={exercise}||222=={exercise}||223=={exercise}||224=={exercise}||225=={exercise}||226=={exercise}||227=={exercise}||228=={exercise}||229=={exercise}||230=={exercise}||231=={exercise}||232=={exercise}||233=={exercise}||234=={exercise}||235=={exercise}||236=={exercise}||237=={exercise}||238=={exercise}||239=={exercise}||240=={exercise}||241=={exercise}||242=={exercise}||243=={exercise}||244=={exercise}||245=={exercise}||246=={exercise}||247=={exercise}||248=={exercise}||249=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}||exercise-8=={all exercises}||exercise-9=={all exercises}||exercise-10=={all exercises}||exercise-11=={all exercises}||exercise-12=={all exercises}||exercise-13=={all exercises}||exercise-14=={all exercises}||exercise-15=={all exercises}||exercise-16=={all exercises}||exercise-17=={all exercises}||exercise-18=={all exercises}||exercise-19=={all exercises}||exercise-20=={all exercises}||exercise-21=={all exercises}||exercise-22=={all exercises}||exercise-23=={all exercises}||exercise-24=={all exercises}||exercise-25=={all exercises}||exercise-26=={all exercises}||exercise-27=={all exercises}||exercise-28=={all exercises}||exercise-29=={all exercises}||exercise-30=={all exercises}||exercise-31=={all exercises}||exercise-32=={all exercises}||exercise-33=={all exercises}||exercise-34=={all exercises}||exercise-35=={all exercises}||exercise-36=={all exercises}||exercise-37=={all exercises}||exercise-38=={all exercises}||exercise-39=={all exercises}||exercise-40=={all exercises}||exercise-41=={all exercises}||exercise-42=={all exercises}||exercise-43=={all exercises}||exercise-44=={all exercises}||exercise-45=={all exercises}||exercise-46=={all exercises}||exercise-47=={all exercises}||exercise-48=={all exercises}||exercise-49=={all exercises}||exercise-50=={all exercises}||exercise-51=={all exercises}||exercise-52=={all exercises}||exercise-53=={all exercises}||exercise-54=={all exercises}||exercise-55=={all exercises}||exercise-56=={all exercises}||exercise-57=={all exercises}||exercise-58=={all exercises}||exercise-59=={all exercises}||exercise-60=={all exercises}||exercise-61=={all exercises}||exercise-62=={all exercises}||exercise-63=={all exercises}||exercise-64=={all exercises}||exercise-65=={all exercises}||exercise-66=={all exercises}||exercise-67=={all exercises}||exercise-68=={all exercises}||exercise-69=={all exercises}||exercise-70=={all exercises}||exercise-71=={all exercises}||exercise-72=={all exercises}||exercise-73=={all exercises}||exercise-74=={all exercises}||exercise-75=={all exercises}||exercise-76=={all exercises}||exercise-77=={all exercises}||exercise-78=={all exercises}||exercise-79=={all exercises}||exercise-80=={all exercises}||exercise-81=={all exercises}||exercise-82=={all exercises}||exercise-83=={all exercises}||exercise-84=={all exercises}||exercise-85=={all exercises}||exercise-86=={all exercises}||exercise-87=={all exercises}||exercise-88=={all exercises}||exercise-89=={all exercises}||exercise-90=={all exercises}||exercise-91=={all exercises}||exercise-92=={all exercises}||exercise-93=={all exercises}||exercise-94=={all exercises}||exercise-95=={all exercises}||exercise-96=={all exercises}||exercise-97=={all exercises}||exercise-98=={all exercises}||exercise-99=={all exercises}||exercise-100=={all exercises}||exercise-101=={all exercises}||exercise-102=={all exercises}||exercise-103=={all exercises}||exercise-104=={all exercises}||exercise-105=={all exercises}||exercise-106=={all exercises}||exercise-107=={all exercises}||exercise-108=={all exercises}||exercise-109=={all exercises}||exercise-110=={all exercises}||exercise-111=={all exercises}||exercise-112=={all exercises}||exercise-113=={all exercises}||exercise-114=={all exercises}||exercise-115=={all exercises}||exercise-116=={all exercises}||exercise-117=={all exercises}||exercise-118=={all exercises}||exercise-119=={all exercises}||exercise-120=={all exercises}||exercise-121=={all exercises}||exercise-122=={all exercises}||exercise-123=={all exercises}||exercise-124=={all exercises}||exercise-125=={all exercises}||exercise-126=={all exercises}||exercise-127=={all exercises}||exercise-128=={all exercises}||exercise-129=={all exercises}||exercise-130=={all exercises}||exercise-131=={all exercises}||exercise-132=={all exercises}||exercise-133=={all exercises}||exercise-134=={all exercises}||exercise-135=={all exercises}||exercise-136=={all exercises}||exercise-137=={all exercises}||exercise-138=={all exercises}||exercise-139=={all exercises}||exercise-140=={all exercises}||exercise-141=={all exercises}||exercise-142=={all exercises}||exercise-143=={all exercises}||exercise-144=={all exercises}||exercise-145=={all exercises}||exercise-146=={all exercises}||exercise-147=={all exercises}||exercise-148=={all exercises}||exercise-149=={all exercises}||exercise-150=={all exercises}||exercise-151=={all exercises}||exercise-152=={all exercises}||exercise-153=={all exercises}||exercise-154=={all exercises}||exercise-155=={all exercises}||exercise-156=={all exercises}||exercise-157=={all exercises}||exercise-158=={all exercises}||exercise-159=={all exercises}||exercise-160=={all exercises}||exercise-161=={all exercises}||exercise-162=={all exercises}||exercise-163=={all exercises}||exercise-164=={all exercises}||exercise-165=={all exercises}||exercise-166=={all exercises}||exercise-167=={all exercises}||exercise-168=={all exercises}||exercise-169=={all exercises}||exercise-170=={all exercises}||exercise-171=={all exercises}||exercise-172=={all exercises}||exercise-173=={all exercises}||exercise-174=={all exercises}||exercise-175=={all exercises}||exercise-176=={all exercises}||exercise-177=={all exercises}||exercise-178=={all exercises}||exercise-179=={all exercises}||exercise-180=={all exercises}||exercise-181=={all exercises}||exercise-182=={all exercises}||exercise-183=={all exercises}||exercise-184=={all exercises}||exercise-185=={all exercises}||exercise-186=={all exercises}||exercise-187=={all exercises}||exercise-188=={all exercises}||exercise-189=={all exercises}||exercise-190=={all exercises}||exercise-191=={all exercises}||exercise-192=={all exercises}||exercise-193=={all exercises}||exercise-194=={all exercises}||exercise-195=={all exercises}||exercise-196=={all exercises}||exercise-197=={all exercises}||exercise-198=={all exercises}||exercise-199=={all exercises}||exercise-200=={all exercises}||exercise-201=={all exercises}||exercise-202=={all exercises}||exercise-203=={all exercises}||exercise-204=={all exercises}||exercise-205=={all exercises}||exercise-206=={all exercises}||exercise-207=={all exercises}||exercise-208=={all exercises}||exercise-209=={all exercises}||exercise-210=={all exercises}||exercise-211=={all exercises}||exercise-212=={all exercises}||exercise-213=={all exercises}||exercise-214=={all exercises}||exercise-215=={all exercises}||exercise-216=={all exercises}||exercise-217=={all exercises}||exercise-218=={all exercises}||exercise-219=={all exercises}||exercise-220=={all exercises}||exercise-221=={all exercises}||exercise-222=={all exercises}||exercise-223=={all exercises}||exercise-224=={all exercises}||exercise-225=={all exercises}||exercise-226=={all exercises}||exercise-227=={all exercises}||exercise-228=={all exercises}||exercise-229=={all exercises}||exercise-230=={all exercises}||exercise-231=={all exercises}||exercise-232=={all exercises}||exercise-233=={all exercises}||exercise-234=={all exercises}||exercise-235=={all exercises}||exercise-236=={all exercises}||exercise-237=={all exercises}||exercise-238=={all exercises}||exercise-239=={all exercises}||exercise-240=={all exercises}||exercise-241=={all exercises}||exercise-242=={all exercises}||exercise-243=={all exercises}||exercise-244=={all exercises}||exercise-245=={all exercises}||exercise-246=={all exercises}||exercise-247=={all exercises}||exercise-248=={all exercises}||exercise-249=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11||exercise-12||exercise-13||exercise-14||exercise-15||exercise-16||exercise-17||exercise-18||exercise-19||exercise-20||exercise-21||exercise-22||exercise-23||exercise-24||exercise-25||exercise-26||exercise-27||exercise-28||exercise-29||exercise-30||exercise-31||exercise-32||exercise-33||exercise-34||exercise-35||exercise-36||exercise-37||exercise-38||exercise-39||exercise-40||exercise-41||exercise-42||exercise-43||exercise-44||exercise-45||exercise-46||exercise-47||exercise-48||exercise-49||exercise-50||exercise-51||exercise-52||exercise-53||exercise-54||exercise-55||exercise-56||exercise-57||exercise-58||exercise-59||exercise-60||exercise-61||exercise-62||exercise-63||exercise-64||exercise-65||exercise-66||exercise-67||exercise-68||exercise-69||exercise-70||exercise-71||exercise-72||exercise-73||exercise-74||exercise-75||exercise-76||exercise-77||exercise-78||exercise-79||exercise-80||exercise-81||exercise-82||exercise-83||exercise-84||exercise-85||exercise-86||exercise-87||exercise-88||exercise-89||exercise-90||exercise-91||exercise-92||exercise-93||exercise-94||exercise-95||exercise-96||exercise-97||exercise-98||exercise-99||exercise-100||exercise-101||exercise-102||exercise-103||exercise-104||exercise-105||exercise-106||exercise-107||exercise-108||exercise-109||exercise-110||exercise-111||exercise-112||exercise-113||exercise-114||exercise-115||exercise-116||exercise-117||exercise-118||exercise-119||exercise-120||exercise-121||exercise-122||exercise-123||exercise-124||exercise-125||exercise-126||exercise-127||exercise-128||exercise-129||exercise-130||exercise-131||exercise-132||exercise-133||exercise-134||exercise-135||exercise-136||exercise-137||exercise-138||exercise-139||exercise-140||exercise-141||exercise-142||exercise-143||exercise-144||exercise-145||exercise-146||exercise-147||exercise-148||exercise-149||exercise-150||exercise-151||exercise-152||exercise-153||exercise-154||exercise-155||exercise-156||exercise-157||exercise-158||exercise-159||exercise-160||exercise-161||exercise-162||exercise-163||exercise-164||exercise-165||exercise-166||exercise-167||exercise-168||exercise-169||exercise-170||exercise-171||exercise-172||exercise-173||exercise-174||exercise-175||exercise-176||exercise-177||exercise-178||exercise-179||exercise-180||exercise-181||exercise-182||exercise-183||exercise-184||exercise-185||exercise-186||exercise-187||exercise-188||exercise-189||exercise-190||exercise-191||exercise-192||exercise-193||exercise-194||exercise-195||exercise-196||exercise-197||exercise-198||exercise-199||exercise-200||exercise-201||exercise-202||exercise-203||exercise-204||exercise-205||exercise-206||exercise-207||exercise-208||exercise-209||exercise-210||exercise-211||exercise-212||exercise-213||exercise-214||exercise-215||exercise-216||exercise-217||exercise-218||exercise-219||exercise-220||exercise-221||exercise-222||exercise-223||exercise-224||exercise-225||exercise-226||exercise-227||exercise-228||exercise-229||exercise-230||exercise-231||exercise-232||exercise-233||exercise-234||exercise-235||exercise-236||exercise-237||exercise-238||exercise-239||exercise-240||exercise-241||exercise-242||exercise-243||exercise-244||exercise-245||exercise-246||exercise-247||exercise-248||exercise-249}
\setcounter{totalexerciseinall exercises}{249}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}||exercise-152=={152}||exercise-153=={153}||exercise-154=={154}||exercise-155=={155}||exercise-156=={156}||exercise-157=={157}||exercise-158=={158}||exercise-159=={159}||exercise-160=={160}||exercise-161=={161}||exercise-162=={162}||exercise-163=={163}||exercise-164=={164}||exercise-165=={165}||exercise-166=={166}||exercise-167=={167}||exercise-168=={168}||exercise-169=={169}||exercise-170=={170}||exercise-171=={171}||exercise-172=={172}||exercise-173=={173}||exercise-174=={174}||exercise-175=={175}||exercise-176=={176}||exercise-177=={177}||exercise-178=={178}||exercise-179=={179}||exercise-180=={180}||exercise-181=={181}||exercise-182=={182}||exercise-183=={183}||exercise-184=={184}||exercise-185=={185}||exercise-186=={186}||exercise-187=={187}||exercise-188=={188}||exercise-189=={189}||exercise-190=={190}||exercise-191=={191}||exercise-192=={192}||exercise-193=={193}||exercise-194=={194}||exercise-195=={195}||exercise-196=={196}||exercise-197=={197}||exercise-198=={198}||exercise-199=={199}||exercise-200=={200}||exercise-201=={201}||exercise-202=={202}||exercise-203=={203}||exercise-204=={204}||exercise-205=={205}||exercise-206=={206}||exercise-207=={207}||exercise-208=={208}||exercise-209=={209}||exercise-210=={210}||exercise-211=={211}||exercise-212=={212}||exercise-213=={213}||exercise-214=={214}||exercise-215=={215}||exercise-216=={216}||exercise-217=={217}||exercise-218=={218}||exercise-219=={219}||exercise-220=={220}||exercise-221=={221}||exercise-222=={222}||exercise-223=={223}||exercise-224=={224}||exercise-225=={225}||exercise-226=={226}||exercise-227=={227}||exercise-228=={228}||exercise-229=={229}||exercise-230=={230}||exercise-231=={231}||exercise-232=={232}||exercise-233=={233}||exercise-234=={234}||exercise-235=={235}||exercise-236=={236}||exercise-237=={237}||exercise-238=={238}||exercise-239=={239}||exercise-240=={240}||exercise-241=={241}||exercise-242=={242}||exercise-243=={243}||exercise-244=={244}||exercise-245=={245}||exercise-246=={246}||exercise-247=={247}||exercise-248=={248}||exercise-249=={249}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}||exercise-152=={152}||exercise-153=={153}||exercise-154=={154}||exercise-155=={155}||exercise-156=={156}||exercise-157=={157}||exercise-158=={158}||exercise-159=={159}||exercise-160=={160}||exercise-161=={161}||exercise-162=={162}||exercise-163=={163}||exercise-164=={164}||exercise-165=={165}||exercise-166=={166}||exercise-167=={167}||exercise-168=={168}||exercise-169=={169}||exercise-170=={170}||exercise-171=={171}||exercise-172=={172}||exercise-173=={173}||exercise-174=={174}||exercise-175=={175}||exercise-176=={176}||exercise-177=={177}||exercise-178=={178}||exercise-179=={179}||exercise-180=={180}||exercise-181=={181}||exercise-182=={182}||exercise-183=={183}||exercise-184=={184}||exercise-185=={185}||exercise-186=={186}||exercise-187=={187}||exercise-188=={188}||exercise-189=={189}||exercise-190=={190}||exercise-191=={191}||exercise-192=={192}||exercise-193=={193}||exercise-194=={194}||exercise-195=={195}||exercise-196=={196}||exercise-197=={197}||exercise-198=={198}||exercise-199=={199}||exercise-200=={200}||exercise-201=={201}||exercise-202=={202}||exercise-203=={203}||exercise-204=={204}||exercise-205=={205}||exercise-206=={206}||exercise-207=={207}||exercise-208=={208}||exercise-209=={209}||exercise-210=={210}||exercise-211=={211}||exercise-212=={212}||exercise-213=={213}||exercise-214=={214}||exercise-215=={215}||exercise-216=={216}||exercise-217=={217}||exercise-218=={218}||exercise-219=={219}||exercise-220=={220}||exercise-221=={221}||exercise-222=={222}||exercise-223=={223}||exercise-224=={224}||exercise-225=={225}||exercise-226=={226}||exercise-227=={227}||exercise-228=={228}||exercise-229=={229}||exercise-230=={230}||exercise-231=={231}||exercise-232=={232}||exercise-233=={233}||exercise-234=={234}||exercise-235=={235}||exercise-236=={236}||exercise-237=={237}||exercise-238=={238}||exercise-239=={239}||exercise-240=={240}||exercise-241=={241}||exercise-242=={242}||exercise-243=={243}||exercise-244=={244}||exercise-245=={245}||exercise-246=={246}||exercise-247=={247}||exercise-248=={248}||exercise-249=={249}}
\XSIM{counter}{exercise-1=={0.1}||exercise-2=={0.2}||exercise-3=={0.3}||exercise-4=={0.4}||exercise-5=={0.5}||exercise-6=={0.6}||exercise-7=={0.7}||exercise-8=={0.8}||exercise-9=={0.9}||exercise-10=={0.10}||exercise-11=={0.11}||exercise-12=={0.12}||exercise-13=={0.13}||exercise-14=={0.14}||exercise-15=={0.15}||exercise-16=={0.16}||exercise-17=={0.17}||exercise-18=={0.18}||exercise-19=={0.19}||exercise-20=={0.20}||exercise-21=={0.21}||exercise-22=={0.22}||exercise-23=={0.23}||exercise-24=={0.24}||exercise-25=={0.25}||exercise-26=={0.26}||exercise-27=={0.27}||exercise-28=={0.28}||exercise-29=={0.29}||exercise-30=={0.30}||exercise-31=={0.31}||exercise-32=={0.32}||exercise-33=={0.33}||exercise-34=={0.34}||exercise-35=={0.35}||exercise-36=={0.36}||exercise-37=={0.37}||exercise-38=={0.38}||exercise-39=={0.39}||exercise-40=={0.40}||exercise-41=={0.41}||exercise-42=={0.42}||exercise-43=={0.43}||exercise-44=={0.44}||exercise-45=={0.45}||exercise-46=={0.46}||exercise-47=={0.47}||exercise-48=={0.48}||exercise-49=={0.49}||exercise-50=={0.50}||exercise-51=={0.51}||exercise-52=={0.52}||exercise-53=={0.53}||exercise-54=={0.54}||exercise-55=={0.55}||exercise-56=={0.56}||exercise-57=={0.57}||exercise-58=={0.58}||exercise-59=={0.59}||exercise-60=={0.60}||exercise-61=={0.61}||exercise-62=={0.62}||exercise-63=={0.63}||exercise-64=={0.64}||exercise-65=={0.65}||exercise-66=={0.66}||exercise-67=={0.67}||exercise-68=={0.68}||exercise-69=={0.69}||exercise-70=={0.70}||exercise-71=={0.71}||exercise-72=={0.72}||exercise-73=={0.73}||exercise-74=={0.74}||exercise-75=={0.75}||exercise-76=={0.76}||exercise-77=={0.77}||exercise-78=={0.78}||exercise-79=={0.79}||exercise-80=={0.80}||exercise-81=={0.81}||exercise-82=={0.82}||exercise-83=={0.83}||exercise-84=={0.84}||exercise-85=={0.85}||exercise-86=={0.86}||exercise-87=={0.87}||exercise-88=={0.88}||exercise-89=={0.89}||exercise-90=={0.90}||exercise-91=={0.91}||exercise-92=={0.92}||exercise-93=={0.93}||exercise-94=={0.94}||exercise-95=={0.95}||exercise-96=={0.96}||exercise-97=={0.97}||exercise-98=={0.98}||exercise-99=={0.99}||exercise-100=={0.100}||exercise-101=={0.101}||exercise-102=={0.102}||exercise-103=={0.103}||exercise-104=={0.104}||exercise-105=={0.105}||exercise-106=={0.106}||exercise-107=={0.107}||exercise-108=={0.108}||exercise-109=={0.109}||exercise-110=={0.110}||exercise-111=={0.111}||exercise-112=={0.112}||exercise-113=={0.113}||exercise-114=={0.114}||exercise-115=={0.115}||exercise-116=={0.116}||exercise-117=={0.117}||exercise-118=={0.118}||exercise-119=={0.119}||exercise-120=={0.120}||exercise-121=={0.121}||exercise-122=={0.122}||exercise-123=={0.123}||exercise-124=={0.124}||exercise-125=={0.125}||exercise-126=={0.126}||exercise-127=={0.127}||exercise-128=={0.128}||exercise-129=={0.129}||exercise-130=={0.130}||exercise-131=={0.131}||exercise-132=={0.132}||exercise-133=={0.133}||exercise-134=={0.134}||exercise-135=={0.135}||exercise-136=={0.136}||exercise-137=={0.137}||exercise-138=={0.138}||exercise-139=={0.139}||exercise-140=={0.140}||exercise-141=={0.141}||exercise-142=={0.142}||exercise-143=={0.143}||exercise-144=={0.144}||exercise-145=={0.145}||exercise-146=={0.146}||exercise-147=={0.147}||exercise-148=={0.148}||exercise-149=={0.149}||exercise-150=={0.150}||exercise-151=={0.151}||exercise-152=={0.152}||exercise-153=={0.153}||exercise-154=={0.154}||exercise-155=={0.155}||exercise-156=={0.156}||exercise-157=={0.157}||exercise-158=={0.158}||exercise-159=={0.159}||exercise-160=={0.160}||exercise-161=={0.161}||exercise-162=={0.162}||exercise-163=={0.163}||exercise-164=={0.164}||exercise-165=={0.165}||exercise-166=={0.166}||exercise-167=={0.167}||exercise-168=={0.168}||exercise-169=={0.169}||exercise-170=={0.170}||exercise-171=={0.171}||exercise-172=={0.172}||exercise-173=={0.173}||exercise-174=={0.174}||exercise-175=={0.175}||exercise-176=={0.176}||exercise-177=={0.177}||exercise-178=={0.178}||exercise-179=={0.179}||exercise-180=={0.180}||exercise-181=={0.181}||exercise-182=={0.182}||exercise-183=={0.183}||exercise-184=={0.184}||exercise-185=={0.185}||exercise-186=={0.186}||exercise-187=={0.187}||exercise-188=={0.188}||exercise-189=={0.189}||exercise-190=={0.190}||exercise-191=={0.191}||exercise-192=={0.192}||exercise-193=={0.193}||exercise-194=={0.194}||exercise-195=={0.195}||exercise-196=={0.196}||exercise-197=={0.197}||exercise-198=={0.198}||exercise-199=={0.199}||exercise-200=={0.200}||exercise-201=={0.201}||exercise-202=={0.202}||exercise-203=={0.203}||exercise-204=={0.204}||exercise-205=={0.205}||exercise-206=={0.206}||exercise-207=={0.207}||exercise-208=={0.208}||exercise-209=={0.209}||exercise-210=={0.210}||exercise-211=={0.211}||exercise-212=={0.212}||exercise-213=={0.213}||exercise-214=={0.214}||exercise-215=={0.215}||exercise-216=={0.216}||exercise-217=={0.217}||exercise-218=={0.218}||exercise-219=={0.219}||exercise-220=={0.220}||exercise-221=={0.221}||exercise-222=={0.222}||exercise-223=={0.223}||exercise-224=={0.224}||exercise-225=={0.225}||exercise-226=={0.226}||exercise-227=={0.227}||exercise-228=={0.228}||exercise-229=={0.229}||exercise-230=={0.230}||exercise-231=={0.231}||exercise-232=={0.232}||exercise-233=={0.233}||exercise-234=={0.234}||exercise-235=={0.235}||exercise-236=={0.236}||exercise-237=={0.237}||exercise-238=={0.238}||exercise-239=={0.239}||exercise-240=={0.240}||exercise-241=={0.241}||exercise-242=={0.242}||exercise-243=={0.243}||exercise-244=={0.244}||exercise-245=={0.245}||exercise-246=={0.246}||exercise-247=={0.247}||exercise-248=={0.248}||exercise-249=={0.249}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}||exercise-79=={79}||exercise-80=={80}||exercise-81=={81}||exercise-82=={82}||exercise-83=={83}||exercise-84=={84}||exercise-85=={85}||exercise-86=={86}||exercise-87=={87}||exercise-88=={88}||exercise-89=={89}||exercise-90=={90}||exercise-91=={91}||exercise-92=={92}||exercise-93=={93}||exercise-94=={94}||exercise-95=={95}||exercise-96=={96}||exercise-97=={97}||exercise-98=={98}||exercise-99=={99}||exercise-100=={100}||exercise-101=={101}||exercise-102=={102}||exercise-103=={103}||exercise-104=={104}||exercise-105=={105}||exercise-106=={106}||exercise-107=={107}||exercise-108=={108}||exercise-109=={109}||exercise-110=={110}||exercise-111=={111}||exercise-112=={112}||exercise-113=={113}||exercise-114=={114}||exercise-115=={115}||exercise-116=={116}||exercise-117=={117}||exercise-118=={118}||exercise-119=={119}||exercise-120=={120}||exercise-121=={121}||exercise-122=={122}||exercise-123=={123}||exercise-124=={124}||exercise-125=={125}||exercise-126=={126}||exercise-127=={127}||exercise-128=={128}||exercise-129=={129}||exercise-130=={130}||exercise-131=={131}||exercise-132=={132}||exercise-133=={133}||exercise-134=={134}||exercise-135=={135}||exercise-136=={136}||exercise-137=={137}||exercise-138=={138}||exercise-139=={139}||exercise-140=={140}||exercise-141=={141}||exercise-142=={142}||exercise-143=={143}||exercise-144=={144}||exercise-145=={145}||exercise-146=={146}||exercise-147=={147}||exercise-148=={148}||exercise-149=={149}||exercise-150=={150}||exercise-151=={151}||exercise-152=={152}||exercise-153=={153}||exercise-154=={154}||exercise-155=={155}||exercise-156=={156}||exercise-157=={157}||exercise-158=={158}||exercise-159=={159}||exercise-160=={160}||exercise-161=={161}||exercise-162=={162}||exercise-163=={163}||exercise-164=={164}||exercise-165=={165}||exercise-166=={166}||exercise-167=={167}||exercise-168=={168}||exercise-169=={169}||exercise-170=={170}||exercise-171=={171}||exercise-172=={172}||exercise-173=={173}||exercise-174=={174}||exercise-175=={175}||exercise-176=={176}||exercise-177=={177}||exercise-178=={178}||exercise-179=={179}||exercise-180=={180}||exercise-181=={181}||exercise-182=={182}||exercise-183=={183}||exercise-184=={184}||exercise-185=={185}||exercise-186=={186}||exercise-187=={187}||exercise-188=={188}||exercise-189=={189}||exercise-190=={190}||exercise-191=={191}||exercise-192=={192}||exercise-193=={193}||exercise-194=={194}||exercise-195=={195}||exercise-196=={196}||exercise-197=={197}||exercise-198=={198}||exercise-199=={199}||exercise-200=={200}||exercise-201=={201}||exercise-202=={202}||exercise-203=={203}||exercise-204=={204}||exercise-205=={205}||exercise-206=={206}||exercise-207=={207}||exercise-208=={208}||exercise-209=={209}||exercise-210=={210}||exercise-211=={211}||exercise-212=={212}||exercise-213=={213}||exercise-214=={214}||exercise-215=={215}||exercise-216=={216}||exercise-217=={217}||exercise-218=={218}||exercise-219=={219}||exercise-220=={220}||exercise-221=={221}||exercise-222=={222}||exercise-223=={223}||exercise-224=={224}||exercise-225=={225}||exercise-226=={226}||exercise-227=={227}||exercise-228=={228}||exercise-229=={229}||exercise-230=={230}||exercise-231=={231}||exercise-232=={232}||exercise-233=={233}||exercise-234=={234}||exercise-235=={235}||exercise-236=={236}||exercise-237=={237}||exercise-238=={238}||exercise-239=={239}||exercise-240=={240}||exercise-241=={241}||exercise-242=={242}||exercise-243=={243}||exercise-244=={244}||exercise-245=={245}||exercise-246=={246}||exercise-247=={247}||exercise-248=={248}||exercise-249=={249}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={0}||exercise-2=={0}||exercise-3=={0}||exercise-4=={0}||exercise-5=={0}||exercise-6=={0}||exercise-7=={0}||exercise-8=={0}||exercise-9=={0}||exercise-10=={0}||exercise-11=={0}||exercise-12=={0}||exercise-13=={0}||exercise-14=={0}||exercise-15=={0}||exercise-16=={0}||exercise-17=={0}||exercise-18=={0}||exercise-19=={0}||exercise-20=={0}||exercise-21=={0}||exercise-22=={0}||exercise-23=={0}||exercise-24=={0}||exercise-25=={0}||exercise-26=={0}||exercise-27=={0}||exercise-28=={0}||exercise-29=={0}||exercise-30=={0}||exercise-31=={0}||exercise-32=={0}||exercise-33=={0}||exercise-34=={0}||exercise-35=={0}||exercise-36=={0}||exercise-37=={0}||exercise-38=={0}||exercise-39=={0}||exercise-40=={0}||exercise-41=={0}||exercise-42=={0}||exercise-43=={0}||exercise-44=={0}||exercise-45=={0}||exercise-46=={0}||exercise-47=={0}||exercise-48=={0}||exercise-49=={0}||exercise-50=={0}||exercise-51=={0}||exercise-52=={0}||exercise-53=={0}||exercise-54=={0}||exercise-55=={0}||exercise-56=={0}||exercise-57=={0}||exercise-58=={0}||exercise-59=={0}||exercise-60=={0}||exercise-61=={0}||exercise-62=={0}||exercise-63=={0}||exercise-64=={0}||exercise-65=={0}||exercise-66=={0}||exercise-67=={0}||exercise-68=={0}||exercise-69=={0}||exercise-70=={0}||exercise-71=={0}||exercise-72=={0}||exercise-73=={0}||exercise-74=={0}||exercise-75=={0}||exercise-76=={0}||exercise-77=={0}||exercise-78=={0}||exercise-79=={0}||exercise-80=={0}||exercise-81=={0}||exercise-82=={0}||exercise-83=={0}||exercise-84=={0}||exercise-85=={0}||exercise-86=={0}||exercise-87=={0}||exercise-88=={0}||exercise-89=={0}||exercise-90=={0}||exercise-91=={0}||exercise-92=={0}||exercise-93=={0}||exercise-94=={0}||exercise-95=={0}||exercise-96=={0}||exercise-97=={0}||exercise-98=={0}||exercise-99=={0}||exercise-100=={0}||exercise-101=={0}||exercise-102=={0}||exercise-103=={0}||exercise-104=={0}||exercise-105=={0}||exercise-106=={0}||exercise-107=={0}||exercise-108=={0}||exercise-109=={0}||exercise-110=={0}||exercise-111=={0}||exercise-112=={0}||exercise-113=={0}||exercise-114=={0}||exercise-115=={0}||exercise-116=={0}||exercise-117=={0}||exercise-118=={0}||exercise-119=={0}||exercise-120=={0}||exercise-121=={0}||exercise-122=={0}||exercise-123=={0}||exercise-124=={0}||exercise-125=={0}||exercise-126=={0}||exercise-127=={0}||exercise-128=={0}||exercise-129=={0}||exercise-130=={0}||exercise-131=={0}||exercise-132=={0}||exercise-133=={0}||exercise-134=={0}||exercise-135=={0}||exercise-136=={0}||exercise-137=={0}||exercise-138=={0}||exercise-139=={0}||exercise-140=={0}||exercise-141=={0}||exercise-142=={0}||exercise-143=={0}||exercise-144=={0}||exercise-145=={0}||exercise-146=={0}||exercise-147=={0}||exercise-148=={0}||exercise-149=={0}||exercise-150=={0}||exercise-151=={0}||exercise-152=={0}||exercise-153=={0}||exercise-154=={0}||exercise-155=={0}||exercise-156=={0}||exercise-157=={0}||exercise-158=={0}||exercise-159=={0}||exercise-160=={0}||exercise-161=={0}||exercise-162=={0}||exercise-163=={0}||exercise-164=={0}||exercise-165=={0}||exercise-166=={0}||exercise-167=={0}||exercise-168=={0}||exercise-169=={0}||exercise-170=={0}||exercise-171=={0}||exercise-172=={0}||exercise-173=={0}||exercise-174=={0}||exercise-175=={0}||exercise-176=={0}||exercise-177=={0}||exercise-178=={0}||exercise-179=={0}||exercise-180=={0}||exercise-181=={0}||exercise-182=={0}||exercise-183=={0}||exercise-184=={0}||exercise-185=={0}||exercise-186=={0}||exercise-187=={0}||exercise-188=={0}||exercise-189=={0}||exercise-190=={0}||exercise-191=={0}||exercise-192=={0}||exercise-193=={0}||exercise-194=={0}||exercise-195=={0}||exercise-196=={0}||exercise-197=={0}||exercise-198=={0}||exercise-199=={0}||exercise-200=={0}||exercise-201=={0}||exercise-202=={0}||exercise-203=={0}||exercise-204=={0}||exercise-205=={0}||exercise-206=={0}||exercise-207=={0}||exercise-208=={0}||exercise-209=={0}||exercise-210=={0}||exercise-211=={0}||exercise-212=={0}||exercise-213=={0}||exercise-214=={0}||exercise-215=={0}||exercise-216=={0}||exercise-217=={0}||exercise-218=={0}||exercise-219=={0}||exercise-220=={0}||exercise-221=={0}||exercise-222=={0}||exercise-223=={0}||exercise-224=={0}||exercise-225=={0}||exercise-226=={0}||exercise-227=={0}||exercise-228=={0}||exercise-229=={0}||exercise-230=={0}||exercise-231=={0}||exercise-232=={0}||exercise-233=={0}||exercise-234=={0}||exercise-235=={0}||exercise-236=={0}||exercise-237=={0}||exercise-238=={0}||exercise-239=={0}||exercise-240=={0}||exercise-241=={0}||exercise-242=={0}||exercise-243=={0}||exercise-244=={0}||exercise-245=={0}||exercise-246=={0}||exercise-247=={0}||exercise-248=={0}||exercise-249=={0}}
\XSIM{section}{exercise-1=={0}||exercise-2=={0}||exercise-3=={0}||exercise-4=={0}||exercise-5=={0}||exercise-6=={0}||exercise-7=={0}||exercise-8=={0}||exercise-9=={0}||exercise-10=={0}||exercise-11=={0}||exercise-12=={0}||exercise-13=={0}||exercise-14=={0}||exercise-15=={0}||exercise-16=={0}||exercise-17=={0}||exercise-18=={0}||exercise-19=={0}||exercise-20=={0}||exercise-21=={0}||exercise-22=={0}||exercise-23=={0}||exercise-24=={0}||exercise-25=={0}||exercise-26=={0}||exercise-27=={0}||exercise-28=={0}||exercise-29=={0}||exercise-30=={0}||exercise-31=={0}||exercise-32=={0}||exercise-33=={0}||exercise-34=={0}||exercise-35=={0}||exercise-36=={0}||exercise-37=={0}||exercise-38=={0}||exercise-39=={0}||exercise-40=={0}||exercise-41=={0}||exercise-42=={0}||exercise-43=={0}||exercise-44=={0}||exercise-45=={0}||exercise-46=={0}||exercise-47=={0}||exercise-48=={0}||exercise-49=={0}||exercise-50=={0}||exercise-51=={0}||exercise-52=={0}||exercise-53=={0}||exercise-54=={0}||exercise-55=={0}||exercise-56=={0}||exercise-57=={0}||exercise-58=={0}||exercise-59=={0}||exercise-60=={0}||exercise-61=={0}||exercise-62=={0}||exercise-63=={0}||exercise-64=={0}||exercise-65=={0}||exercise-66=={0}||exercise-67=={0}||exercise-68=={0}||exercise-69=={0}||exercise-70=={0}||exercise-71=={0}||exercise-72=={0}||exercise-73=={0}||exercise-74=={0}||exercise-75=={0}||exercise-76=={0}||exercise-77=={0}||exercise-78=={0}||exercise-79=={0}||exercise-80=={0}||exercise-81=={0}||exercise-82=={0}||exercise-83=={0}||exercise-84=={0}||exercise-85=={0}||exercise-86=={0}||exercise-87=={0}||exercise-88=={0}||exercise-89=={0}||exercise-90=={0}||exercise-91=={0}||exercise-92=={0}||exercise-93=={0}||exercise-94=={0}||exercise-95=={0}||exercise-96=={0}||exercise-97=={0}||exercise-98=={0}||exercise-99=={0}||exercise-100=={0}||exercise-101=={0}||exercise-102=={0}||exercise-103=={0}||exercise-104=={0}||exercise-105=={0}||exercise-106=={0}||exercise-107=={0}||exercise-108=={0}||exercise-109=={0}||exercise-110=={0}||exercise-111=={0}||exercise-112=={0}||exercise-113=={0}||exercise-114=={0}||exercise-115=={0}||exercise-116=={0}||exercise-117=={0}||exercise-118=={0}||exercise-119=={0}||exercise-120=={0}||exercise-121=={0}||exercise-122=={0}||exercise-123=={0}||exercise-124=={0}||exercise-125=={0}||exercise-126=={0}||exercise-127=={0}||exercise-128=={0}||exercise-129=={0}||exercise-130=={0}||exercise-131=={0}||exercise-132=={0}||exercise-133=={0}||exercise-134=={0}||exercise-135=={0}||exercise-136=={0}||exercise-137=={0}||exercise-138=={0}||exercise-139=={0}||exercise-140=={0}||exercise-141=={0}||exercise-142=={0}||exercise-143=={0}||exercise-144=={0}||exercise-145=={0}||exercise-146=={0}||exercise-147=={0}||exercise-148=={0}||exercise-149=={0}||exercise-150=={0}||exercise-151=={0}||exercise-152=={0}||exercise-153=={0}||exercise-154=={0}||exercise-155=={0}||exercise-156=={0}||exercise-157=={0}||exercise-158=={0}||exercise-159=={0}||exercise-160=={0}||exercise-161=={0}||exercise-162=={0}||exercise-163=={0}||exercise-164=={0}||exercise-165=={0}||exercise-166=={0}||exercise-167=={0}||exercise-168=={0}||exercise-169=={0}||exercise-170=={0}||exercise-171=={0}||exercise-172=={0}||exercise-173=={0}||exercise-174=={0}||exercise-175=={0}||exercise-176=={0}||exercise-177=={0}||exercise-178=={0}||exercise-179=={0}||exercise-180=={0}||exercise-181=={0}||exercise-182=={0}||exercise-183=={0}||exercise-184=={0}||exercise-185=={0}||exercise-186=={0}||exercise-187=={0}||exercise-188=={0}||exercise-189=={0}||exercise-190=={0}||exercise-191=={0}||exercise-192=={0}||exercise-193=={0}||exercise-194=={0}||exercise-195=={0}||exercise-196=={0}||exercise-197=={0}||exercise-198=={0}||exercise-199=={0}||exercise-200=={0}||exercise-201=={0}||exercise-202=={0}||exercise-203=={0}||exercise-204=={0}||exercise-205=={0}||exercise-206=={0}||exercise-207=={0}||exercise-208=={0}||exercise-209=={0}||exercise-210=={0}||exercise-211=={0}||exercise-212=={0}||exercise-213=={0}||exercise-214=={0}||exercise-215=={0}||exercise-216=={0}||exercise-217=={0}||exercise-218=={0}||exercise-219=={0}||exercise-220=={0}||exercise-221=={0}||exercise-222=={0}||exercise-223=={0}||exercise-224=={0}||exercise-225=={0}||exercise-226=={0}||exercise-227=={0}||exercise-228=={0}||exercise-229=={0}||exercise-230=={0}||exercise-231=={0}||exercise-232=={0}||exercise-233=={0}||exercise-234=={0}||exercise-235=={0}||exercise-236=={0}||exercise-237=={0}||exercise-238=={0}||exercise-239=={0}||exercise-240=={0}||exercise-241=={0}||exercise-242=={0}||exercise-243=={0}||exercise-244=={0}||exercise-245=={0}||exercise-246=={0}||exercise-247=={0}||exercise-248=={0}||exercise-249=={0}}
\XSIM{sectioning}{exercise-1=={{0}{0}{0}{0}{0}}||exercise-2=={{0}{0}{0}{0}{0}}||exercise-3=={{0}{0}{0}{0}{0}}||exercise-4=={{0}{0}{0}{0}{0}}||exercise-5=={{0}{0}{0}{0}{0}}||exercise-6=={{0}{0}{0}{0}{0}}||exercise-7=={{0}{0}{0}{0}{0}}||exercise-8=={{0}{0}{0}{0}{0}}||exercise-9=={{0}{0}{0}{0}{0}}||exercise-10=={{0}{0}{0}{0}{0}}||exercise-11=={{0}{0}{0}{0}{0}}||exercise-12=={{0}{0}{0}{0}{0}}||exercise-13=={{0}{0}{0}{0}{0}}||exercise-14=={{0}{0}{0}{0}{0}}||exercise-15=={{0}{0}{0}{0}{0}}||exercise-16=={{0}{0}{0}{0}{0}}||exercise-17=={{0}{0}{0}{0}{0}}||exercise-18=={{0}{0}{0}{0}{0}}||exercise-19=={{0}{0}{0}{0}{0}}||exercise-20=={{0}{0}{0}{0}{0}}||exercise-21=={{0}{0}{0}{0}{0}}||exercise-22=={{0}{0}{0}{0}{0}}||exercise-23=={{0}{0}{0}{0}{0}}||exercise-24=={{0}{0}{0}{0}{0}}||exercise-25=={{0}{0}{0}{0}{0}}||exercise-26=={{0}{0}{0}{0}{0}}||exercise-27=={{0}{0}{0}{0}{0}}||exercise-28=={{0}{0}{0}{0}{0}}||exercise-29=={{0}{0}{0}{0}{0}}||exercise-30=={{0}{0}{0}{0}{0}}||exercise-31=={{0}{0}{0}{0}{0}}||exercise-32=={{0}{0}{0}{0}{0}}||exercise-33=={{0}{0}{0}{0}{0}}||exercise-34=={{0}{0}{0}{0}{0}}||exercise-35=={{0}{0}{0}{0}{0}}||exercise-36=={{0}{0}{0}{0}{0}}||exercise-37=={{0}{0}{0}{0}{0}}||exercise-38=={{0}{0}{0}{0}{0}}||exercise-39=={{0}{0}{0}{0}{0}}||exercise-40=={{0}{0}{0}{0}{0}}||exercise-41=={{0}{0}{0}{0}{0}}||exercise-42=={{0}{0}{0}{0}{0}}||exercise-43=={{0}{0}{0}{0}{0}}||exercise-44=={{0}{0}{0}{0}{0}}||exercise-45=={{0}{0}{0}{0}{0}}||exercise-46=={{0}{0}{0}{0}{0}}||exercise-47=={{0}{0}{0}{0}{0}}||exercise-48=={{0}{0}{0}{0}{0}}||exercise-49=={{0}{0}{0}{0}{0}}||exercise-50=={{0}{0}{0}{0}{0}}||exercise-51=={{0}{0}{0}{0}{0}}||exercise-52=={{0}{0}{0}{0}{0}}||exercise-53=={{0}{0}{0}{0}{0}}||exercise-54=={{0}{0}{0}{0}{0}}||exercise-55=={{0}{0}{0}{0}{0}}||exercise-56=={{0}{0}{0}{0}{0}}||exercise-57=={{0}{0}{0}{0}{0}}||exercise-58=={{0}{0}{0}{0}{0}}||exercise-59=={{0}{0}{0}{0}{0}}||exercise-60=={{0}{0}{0}{0}{0}}||exercise-61=={{0}{0}{0}{0}{0}}||exercise-62=={{0}{0}{0}{0}{0}}||exercise-63=={{0}{0}{0}{0}{0}}||exercise-64=={{0}{0}{0}{0}{0}}||exercise-65=={{0}{0}{0}{0}{0}}||exercise-66=={{0}{0}{0}{0}{0}}||exercise-67=={{0}{0}{0}{0}{0}}||exercise-68=={{0}{0}{0}{0}{0}}||exercise-69=={{0}{0}{0}{0}{0}}||exercise-70=={{0}{0}{0}{0}{0}}||exercise-71=={{0}{0}{0}{0}{0}}||exercise-72=={{0}{0}{0}{0}{0}}||exercise-73=={{0}{0}{0}{0}{0}}||exercise-74=={{0}{0}{0}{0}{0}}||exercise-75=={{0}{0}{0}{0}{0}}||exercise-76=={{0}{0}{0}{0}{0}}||exercise-77=={{0}{0}{0}{0}{0}}||exercise-78=={{0}{0}{0}{0}{0}}||exercise-79=={{0}{0}{0}{0}{0}}||exercise-80=={{0}{0}{0}{0}{0}}||exercise-81=={{0}{0}{0}{0}{0}}||exercise-82=={{0}{0}{0}{0}{0}}||exercise-83=={{0}{0}{0}{0}{0}}||exercise-84=={{0}{0}{0}{0}{0}}||exercise-85=={{0}{0}{0}{0}{0}}||exercise-86=={{0}{0}{0}{0}{0}}||exercise-87=={{0}{0}{0}{0}{0}}||exercise-88=={{0}{0}{0}{0}{0}}||exercise-89=={{0}{0}{0}{0}{0}}||exercise-90=={{0}{0}{0}{0}{0}}||exercise-91=={{0}{0}{0}{0}{0}}||exercise-92=={{0}{0}{0}{0}{0}}||exercise-93=={{0}{0}{0}{0}{0}}||exercise-94=={{0}{0}{0}{0}{0}}||exercise-95=={{0}{0}{0}{0}{0}}||exercise-96=={{0}{0}{0}{0}{0}}||exercise-97=={{0}{0}{0}{0}{0}}||exercise-98=={{0}{0}{0}{0}{0}}||exercise-99=={{0}{0}{0}{0}{0}}||exercise-100=={{0}{0}{0}{0}{0}}||exercise-101=={{0}{0}{0}{0}{0}}||exercise-102=={{0}{0}{0}{0}{0}}||exercise-103=={{0}{0}{0}{0}{0}}||exercise-104=={{0}{0}{0}{0}{0}}||exercise-105=={{0}{0}{0}{0}{0}}||exercise-106=={{0}{0}{0}{0}{0}}||exercise-107=={{0}{0}{0}{0}{0}}||exercise-108=={{0}{0}{0}{0}{0}}||exercise-109=={{0}{0}{0}{0}{0}}||exercise-110=={{0}{0}{0}{0}{0}}||exercise-111=={{0}{0}{0}{0}{0}}||exercise-112=={{0}{0}{0}{0}{0}}||exercise-113=={{0}{0}{0}{0}{0}}||exercise-114=={{0}{0}{0}{0}{0}}||exercise-115=={{0}{0}{0}{0}{0}}||exercise-116=={{0}{0}{0}{0}{0}}||exercise-117=={{0}{0}{0}{0}{0}}||exercise-118=={{0}{0}{0}{0}{0}}||exercise-119=={{0}{0}{0}{0}{0}}||exercise-120=={{0}{0}{0}{0}{0}}||exercise-121=={{0}{0}{0}{0}{0}}||exercise-122=={{0}{0}{0}{0}{0}}||exercise-123=={{0}{0}{0}{0}{0}}||exercise-124=={{0}{0}{0}{0}{0}}||exercise-125=={{0}{0}{0}{0}{0}}||exercise-126=={{0}{0}{0}{0}{0}}||exercise-127=={{0}{0}{0}{0}{0}}||exercise-128=={{0}{0}{0}{0}{0}}||exercise-129=={{0}{0}{0}{0}{0}}||exercise-130=={{0}{0}{0}{0}{0}}||exercise-131=={{0}{0}{0}{0}{0}}||exercise-132=={{0}{0}{0}{0}{0}}||exercise-133=={{0}{0}{0}{0}{0}}||exercise-134=={{0}{0}{0}{0}{0}}||exercise-135=={{0}{0}{0}{0}{0}}||exercise-136=={{0}{0}{0}{0}{0}}||exercise-137=={{0}{0}{0}{0}{0}}||exercise-138=={{0}{0}{0}{0}{0}}||exercise-139=={{0}{0}{0}{0}{0}}||exercise-140=={{0}{0}{0}{0}{0}}||exercise-141=={{0}{0}{0}{0}{0}}||exercise-142=={{0}{0}{0}{0}{0}}||exercise-143=={{0}{0}{0}{0}{0}}||exercise-144=={{0}{0}{0}{0}{0}}||exercise-145=={{0}{0}{0}{0}{0}}||exercise-146=={{0}{0}{0}{0}{0}}||exercise-147=={{0}{0}{0}{0}{0}}||exercise-148=={{0}{0}{0}{0}{0}}||exercise-149=={{0}{0}{0}{0}{0}}||exercise-150=={{0}{0}{0}{0}{0}}||exercise-151=={{0}{0}{0}{0}{0}}||exercise-152=={{0}{0}{0}{0}{0}}||exercise-153=={{0}{0}{0}{0}{0}}||exercise-154=={{0}{0}{0}{0}{0}}||exercise-155=={{0}{0}{0}{0}{0}}||exercise-156=={{0}{0}{0}{0}{0}}||exercise-157=={{0}{0}{0}{0}{0}}||exercise-158=={{0}{0}{0}{0}{0}}||exercise-159=={{0}{0}{0}{0}{0}}||exercise-160=={{0}{0}{0}{0}{0}}||exercise-161=={{0}{0}{0}{0}{0}}||exercise-162=={{0}{0}{0}{0}{0}}||exercise-163=={{0}{0}{0}{0}{0}}||exercise-164=={{0}{0}{0}{0}{0}}||exercise-165=={{0}{0}{0}{0}{0}}||exercise-166=={{0}{0}{0}{0}{0}}||exercise-167=={{0}{0}{0}{0}{0}}||exercise-168=={{0}{0}{0}{0}{0}}||exercise-169=={{0}{0}{0}{0}{0}}||exercise-170=={{0}{0}{0}{0}{0}}||exercise-171=={{0}{0}{0}{0}{0}}||exercise-172=={{0}{0}{0}{0}{0}}||exercise-173=={{0}{0}{0}{0}{0}}||exercise-174=={{0}{0}{0}{0}{0}}||exercise-175=={{0}{0}{0}{0}{0}}||exercise-176=={{0}{0}{0}{0}{0}}||exercise-177=={{0}{0}{0}{0}{0}}||exercise-178=={{0}{0}{0}{0}{0}}||exercise-179=={{0}{0}{0}{0}{0}}||exercise-180=={{0}{0}{0}{0}{0}}||exercise-181=={{0}{0}{0}{0}{0}}||exercise-182=={{0}{0}{0}{0}{0}}||exercise-183=={{0}{0}{0}{0}{0}}||exercise-184=={{0}{0}{0}{0}{0}}||exercise-185=={{0}{0}{0}{0}{0}}||exercise-186=={{0}{0}{0}{0}{0}}||exercise-187=={{0}{0}{0}{0}{0}}||exercise-188=={{0}{0}{0}{0}{0}}||exercise-189=={{0}{0}{0}{0}{0}}||exercise-190=={{0}{0}{0}{0}{0}}||exercise-191=={{0}{0}{0}{0}{0}}||exercise-192=={{0}{0}{0}{0}{0}}||exercise-193=={{0}{0}{0}{0}{0}}||exercise-194=={{0}{0}{0}{0}{0}}||exercise-195=={{0}{0}{0}{0}{0}}||exercise-196=={{0}{0}{0}{0}{0}}||exercise-197=={{0}{0}{0}{0}{0}}||exercise-198=={{0}{0}{0}{0}{0}}||exercise-199=={{0}{0}{0}{0}{0}}||exercise-200=={{0}{0}{0}{0}{0}}||exercise-201=={{0}{0}{0}{0}{0}}||exercise-202=={{0}{0}{0}{0}{0}}||exercise-203=={{0}{0}{0}{0}{0}}||exercise-204=={{0}{0}{0}{0}{0}}||exercise-205=={{0}{0}{0}{0}{0}}||exercise-206=={{0}{0}{0}{0}{0}}||exercise-207=={{0}{0}{0}{0}{0}}||exercise-208=={{0}{0}{0}{0}{0}}||exercise-209=={{0}{0}{0}{0}{0}}||exercise-210=={{0}{0}{0}{0}{0}}||exercise-211=={{0}{0}{0}{0}{0}}||exercise-212=={{0}{0}{0}{0}{0}}||exercise-213=={{0}{0}{0}{0}{0}}||exercise-214=={{0}{0}{0}{0}{0}}||exercise-215=={{0}{0}{0}{0}{0}}||exercise-216=={{0}{0}{0}{0}{0}}||exercise-217=={{0}{0}{0}{0}{0}}||exercise-218=={{0}{0}{0}{0}{0}}||exercise-219=={{0}{0}{0}{0}{0}}||exercise-220=={{0}{0}{0}{0}{0}}||exercise-221=={{0}{0}{0}{0}{0}}||exercise-222=={{0}{0}{0}{0}{0}}||exercise-223=={{0}{0}{0}{0}{0}}||exercise-224=={{0}{0}{0}{0}{0}}||exercise-225=={{0}{0}{0}{0}{0}}||exercise-226=={{0}{0}{0}{0}{0}}||exercise-227=={{0}{0}{0}{0}{0}}||exercise-228=={{0}{0}{0}{0}{0}}||exercise-229=={{0}{0}{0}{0}{0}}||exercise-230=={{0}{0}{0}{0}{0}}||exercise-231=={{0}{0}{0}{0}{0}}||exercise-232=={{0}{0}{0}{0}{0}}||exercise-233=={{0}{0}{0}{0}{0}}||exercise-234=={{0}{0}{0}{0}{0}}||exercise-235=={{0}{0}{0}{0}{0}}||exercise-236=={{0}{0}{0}{0}{0}}||exercise-237=={{0}{0}{0}{0}{0}}||exercise-238=={{0}{0}{0}{0}{0}}||exercise-239=={{0}{0}{0}{0}{0}}||exercise-240=={{0}{0}{0}{0}{0}}||exercise-241=={{0}{0}{0}{0}{0}}||exercise-242=={{0}{0}{0}{0}{0}}||exercise-243=={{0}{0}{0}{0}{0}}||exercise-244=={{0}{0}{0}{0}{0}}||exercise-245=={{0}{0}{0}{0}{0}}||exercise-246=={{0}{0}{0}{0}{0}}||exercise-247=={{0}{0}{0}{0}{0}}||exercise-248=={{0}{0}{0}{0}{0}}||exercise-249=={{0}{0}{0}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={1}||exercise-14=={2}||exercise-15=={2}||exercise-16=={2}||exercise-17=={2}||exercise-18=={2}||exercise-19=={2}||exercise-20=={2}||exercise-21=={2}||exercise-22=={2}||exercise-23=={2}||exercise-24=={2}||exercise-25=={2}||exercise-26=={2}||exercise-27=={2}||exercise-28=={3}||exercise-29=={3}||exercise-30=={3}||exercise-31=={3}||exercise-32=={3}||exercise-33=={3}||exercise-34=={3}||exercise-35=={3}||exercise-36=={3}||exercise-37=={3}||exercise-38=={3}||exercise-39=={3}||exercise-40=={3}||exercise-41=={3}||exercise-42=={4}||exercise-43=={4}||exercise-44=={4}||exercise-45=={4}||exercise-46=={4}||exercise-47=={4}||exercise-48=={4}||exercise-49=={4}||exercise-50=={4}||exercise-51=={4}||exercise-52=={4}||exercise-53=={4}||exercise-54=={4}||exercise-55=={5}||exercise-56=={5}||exercise-57=={5}||exercise-58=={5}||exercise-59=={5}||exercise-60=={5}||exercise-61=={5}||exercise-62=={5}||exercise-63=={5}||exercise-64=={5}||exercise-65=={6}||exercise-66=={6}||exercise-67=={6}||exercise-68=={6}||exercise-69=={6}||exercise-70=={6}||exercise-71=={6}||exercise-72=={6}||exercise-73=={6}||exercise-74=={7}||exercise-75=={7}||exercise-76=={7}||exercise-77=={7}||exercise-78=={7}||exercise-79=={7}||exercise-80=={7}||exercise-81=={7}||exercise-82=={7}||exercise-83=={8}||exercise-84=={8}||exercise-85=={8}||exercise-86=={8}||exercise-87=={8}||exercise-88=={8}||exercise-89=={8}||exercise-90=={8}||exercise-91=={8}||exercise-92=={8}||exercise-93=={8}||exercise-94=={9}||exercise-95=={9}||exercise-96=={9}||exercise-97=={9}||exercise-98=={9}||exercise-99=={9}||exercise-100=={9}||exercise-101=={9}||exercise-102=={9}||exercise-103=={9}||exercise-104=={10}||exercise-105=={10}||exercise-106=={10}||exercise-107=={10}||exercise-108=={10}||exercise-109=={10}||exercise-110=={10}||exercise-111=={10}||exercise-112=={10}||exercise-113=={11}||exercise-114=={11}||exercise-115=={11}||exercise-116=={11}||exercise-117=={11}||exercise-118=={11}||exercise-119=={11}||exercise-120=={11}||exercise-121=={11}||exercise-122=={12}||exercise-123=={12}||exercise-124=={12}||exercise-125=={12}||exercise-126=={12}||exercise-127=={12}||exercise-128=={12}||exercise-129=={12}||exercise-130=={12}||exercise-131=={13}||exercise-132=={13}||exercise-133=={13}||exercise-134=={13}||exercise-135=={13}||exercise-136=={13}||exercise-137=={13}||exercise-138=={13}||exercise-139=={13}||exercise-140=={13}||exercise-141=={14}||exercise-142=={14}||exercise-143=={14}||exercise-144=={14}||exercise-145=={14}||exercise-146=={14}||exercise-147=={14}||exercise-148=={14}||exercise-149=={15}||exercise-150=={15}||exercise-151=={15}||exercise-152=={15}||exercise-153=={15}||exercise-154=={15}||exercise-155=={15}||exercise-156=={15}||exercise-157=={15}||exercise-158=={15}||exercise-159=={15}||exercise-160=={15}||exercise-161=={15}||exercise-162=={16}||exercise-163=={16}||exercise-164=={16}||exercise-165=={16}||exercise-166=={16}||exercise-167=={16}||exercise-168=={17}||exercise-169=={17}||exercise-170=={17}||exercise-171=={17}||exercise-172=={17}||exercise-173=={17}||exercise-174=={17}||exercise-175=={17}||exercise-176=={17}||exercise-177=={17}||exercise-178=={18}||exercise-179=={18}||exercise-180=={18}||exercise-181=={18}||exercise-182=={18}||exercise-183=={18}||exercise-184=={18}||exercise-185=={18}||exercise-186=={18}||exercise-187=={18}||exercise-188=={18}||exercise-189=={18}||exercise-190=={18}||exercise-191=={18}||exercise-192=={18}||exercise-193=={18}||exercise-194=={18}||exercise-195=={19}||exercise-196=={19}||exercise-197=={19}||exercise-198=={19}||exercise-199=={19}||exercise-200=={19}||exercise-201=={19}||exercise-202=={19}||exercise-203=={19}||exercise-204=={19}||exercise-205=={19}||exercise-206=={19}||exercise-207=={19}||exercise-208=={19}||exercise-209=={19}||exercise-210=={19}||exercise-211=={20}||exercise-212=={20}||exercise-213=={20}||exercise-214=={20}||exercise-215=={20}||exercise-216=={20}||exercise-217=={20}||exercise-218=={20}||exercise-219=={20}||exercise-220=={20}||exercise-221=={20}||exercise-222=={20}||exercise-223=={20}||exercise-224=={20}||exercise-225=={20}||exercise-226=={21}||exercise-227=={21}||exercise-228=={21}||exercise-229=={21}||exercise-230=={21}||exercise-231=={21}||exercise-232=={21}||exercise-233=={21}||exercise-234=={21}||exercise-235=={21}||exercise-236=={21}||exercise-237=={21}||exercise-238=={21}||exercise-239=={21}||exercise-240=={21}||exercise-241=={22}||exercise-242=={22}||exercise-243=={22}||exercise-244=={22}||exercise-245=={22}||exercise-246=={22}||exercise-247=={22}||exercise-248=={22}||exercise-249=={22}}
\XSIM{page}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={1}||exercise-14=={2}||exercise-15=={2}||exercise-16=={2}||exercise-17=={2}||exercise-18=={2}||exercise-19=={2}||exercise-20=={2}||exercise-21=={2}||exercise-22=={2}||exercise-23=={2}||exercise-24=={2}||exercise-25=={2}||exercise-26=={2}||exercise-27=={2}||exercise-28=={3}||exercise-29=={3}||exercise-30=={3}||exercise-31=={3}||exercise-32=={3}||exercise-33=={3}||exercise-34=={3}||exercise-35=={3}||exercise-36=={3}||exercise-37=={3}||exercise-38=={3}||exercise-39=={3}||exercise-40=={3}||exercise-41=={3}||exercise-42=={4}||exercise-43=={4}||exercise-44=={4}||exercise-45=={4}||exercise-46=={4}||exercise-47=={4}||exercise-48=={4}||exercise-49=={4}||exercise-50=={4}||exercise-51=={4}||exercise-52=={4}||exercise-53=={4}||exercise-54=={4}||exercise-55=={5}||exercise-56=={5}||exercise-57=={5}||exercise-58=={5}||exercise-59=={5}||exercise-60=={5}||exercise-61=={5}||exercise-62=={5}||exercise-63=={5}||exercise-64=={5}||exercise-65=={6}||exercise-66=={6}||exercise-67=={6}||exercise-68=={6}||exercise-69=={6}||exercise-70=={6}||exercise-71=={6}||exercise-72=={6}||exercise-73=={6}||exercise-74=={7}||exercise-75=={7}||exercise-76=={7}||exercise-77=={7}||exercise-78=={7}||exercise-79=={7}||exercise-80=={7}||exercise-81=={7}||exercise-82=={7}||exercise-83=={8}||exercise-84=={8}||exercise-85=={8}||exercise-86=={8}||exercise-87=={8}||exercise-88=={8}||exercise-89=={8}||exercise-90=={8}||exercise-91=={8}||exercise-92=={8}||exercise-93=={8}||exercise-94=={9}||exercise-95=={9}||exercise-96=={9}||exercise-97=={9}||exercise-98=={9}||exercise-99=={9}||exercise-100=={9}||exercise-101=={9}||exercise-102=={9}||exercise-103=={9}||exercise-104=={10}||exercise-105=={10}||exercise-106=={10}||exercise-107=={10}||exercise-108=={10}||exercise-109=={10}||exercise-110=={10}||exercise-111=={10}||exercise-112=={10}||exercise-113=={11}||exercise-114=={11}||exercise-115=={11}||exercise-116=={11}||exercise-117=={11}||exercise-118=={11}||exercise-119=={11}||exercise-120=={11}||exercise-121=={11}||exercise-122=={12}||exercise-123=={12}||exercise-124=={12}||exercise-125=={12}||exercise-126=={12}||exercise-127=={12}||exercise-128=={12}||exercise-129=={12}||exercise-130=={12}||exercise-131=={13}||exercise-132=={13}||exercise-133=={13}||exercise-134=={13}||exercise-135=={13}||exercise-136=={13}||exercise-137=={13}||exercise-138=={13}||exercise-139=={13}||exercise-140=={13}||exercise-141=={14}||exercise-142=={14}||exercise-143=={14}||exercise-144=={14}||exercise-145=={14}||exercise-146=={14}||exercise-147=={14}||exercise-148=={14}||exercise-149=={15}||exercise-150=={15}||exercise-151=={15}||exercise-152=={15}||exercise-153=={15}||exercise-154=={15}||exercise-155=={15}||exercise-156=={15}||exercise-157=={15}||exercise-158=={15}||exercise-159=={15}||exercise-160=={15}||exercise-161=={15}||exercise-162=={16}||exercise-163=={16}||exercise-164=={16}||exercise-165=={16}||exercise-166=={16}||exercise-167=={16}||exercise-168=={17}||exercise-169=={17}||exercise-170=={17}||exercise-171=={17}||exercise-172=={17}||exercise-173=={17}||exercise-174=={17}||exercise-175=={17}||exercise-176=={17}||exercise-177=={17}||exercise-178=={18}||exercise-179=={18}||exercise-180=={18}||exercise-181=={18}||exercise-182=={18}||exercise-183=={18}||exercise-184=={18}||exercise-185=={18}||exercise-186=={18}||exercise-187=={18}||exercise-188=={18}||exercise-189=={18}||exercise-190=={18}||exercise-191=={18}||exercise-192=={18}||exercise-193=={18}||exercise-194=={18}||exercise-195=={19}||exercise-196=={19}||exercise-197=={19}||exercise-198=={19}||exercise-199=={19}||exercise-200=={19}||exercise-201=={19}||exercise-202=={19}||exercise-203=={19}||exercise-204=={19}||exercise-205=={19}||exercise-206=={19}||exercise-207=={19}||exercise-208=={19}||exercise-209=={19}||exercise-210=={19}||exercise-211=={20}||exercise-212=={20}||exercise-213=={20}||exercise-214=={20}||exercise-215=={20}||exercise-216=={20}||exercise-217=={20}||exercise-218=={20}||exercise-219=={20}||exercise-220=={20}||exercise-221=={20}||exercise-222=={20}||exercise-223=={20}||exercise-224=={20}||exercise-225=={20}||exercise-226=={21}||exercise-227=={21}||exercise-228=={21}||exercise-229=={21}||exercise-230=={21}||exercise-231=={21}||exercise-232=={21}||exercise-233=={21}||exercise-234=={21}||exercise-235=={21}||exercise-236=={21}||exercise-237=={21}||exercise-238=={21}||exercise-239=={21}||exercise-240=={21}||exercise-241=={22}||exercise-242=={22}||exercise-243=={22}||exercise-244=={22}||exercise-245=={22}||exercise-246=={22}||exercise-247=={22}||exercise-248=={22}||exercise-249=={22}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
\XSIM{correctchoice}{exercise-1=={(\textbf {B}) L1}||exercise-2=={(\textbf {B}) Trains multiple models on different subsets of the data}||exercise-3=={(\textbf {C}) Both \circled {A} and \circled {B}}||exercise-4=={(\textbf {A}) Shallow neural network}||exercise-5=={(\textbf {B}) unstructured data}||exercise-6=={(\textbf {C}) convolutional neural networks}||exercise-7=={(\textbf {D}) All of the above}||exercise-8=={(\textbf {C}) Constructs an ensemble by iteratively updating weights}||exercise-9=={(\textbf {E}) All of the previous}||exercise-10=={(\textbf {C}) Random Forest}||exercise-11=={(\textbf {B}) Boosting algorithm}||exercise-12=={(\textbf {C}) Constructs an ensemble by iteratively updating weights}||exercise-13=={(\textbf {B}) Boosting algorithm}||exercise-14=={(\textbf {D}) Trains a meta-model to make predictions based on outputs of base models}||exercise-15=={(\textbf {A}) Random Forest}||exercise-16=={(\textbf {A}) Reduce overfitting and improve generalization}||exercise-17=={(\textbf {A}) Handling imbalanced datasets}||exercise-18=={(\textbf {A}) AdaBoost}||exercise-19=={(\textbf {D}) Stacking}||exercise-20=={(\textbf {B}) Boosting}||exercise-21=={(\textbf {E}) All of the previous}||exercise-22=={(\textbf {B}) The dataset is large and high-dimensional}||exercise-23=={(\textbf {B}) Reducing variance}||exercise-24=={(\textbf {D}) Stacking}||exercise-25=={(\textbf {A}) Bagging}||exercise-26=={(\textbf {B}) Combining predictions by taking the mode of their classes}||exercise-27=={(\textbf {C}) Gradient Boosting}||exercise-28=={(\textbf {A}) Improving model stability}||exercise-29=={(\textbf {D}) Stacking}||exercise-30=={(\textbf {C}) Handling unbalanced datasets}||exercise-31=={(\textbf {D}) Stacking}||exercise-32=={(\textbf {C}) Random Forest}||exercise-33=={(\textbf {B}) AdaBoost}||exercise-34=={(\textbf {C}) Gradient Boosting}||exercise-35=={(\textbf {B}) False}||exercise-36=={(\textbf {D}) All of the previous}||exercise-37=={(\textbf {A}) Softmax}||exercise-38=={(\textbf {A}) Weight between input and hidden layer}||exercise-39=={(\textbf {C}) 96}||exercise-40=={(\textbf {C}) \(22 \times 22\)}||exercise-41=={(\textbf {A}) 50}||exercise-42=={(\textbf {D}) all of the previous}||exercise-43=={(\textbf {D}) 41\%}||exercise-44=={(\textbf {D}) All of the above}||exercise-45=={(\textbf {D}) all of the previous}||exercise-46=={(\textbf {C}) ReLU}||exercise-47=={(\textbf {A}) True}||exercise-48=={(\textbf {E}) All of the previous}||exercise-49=={(\textbf {A}) True}||exercise-50=={(\textbf {B}) False}||exercise-51=={(\textbf {B}) False}||exercise-52=={(\textbf {D}) all of the previous}||exercise-53=={(\textbf {D}) All of the above}||exercise-54=={(\textbf {A}) True}||exercise-55=={(\textbf {B}) False}||exercise-56=={(\textbf {C}) to prevent overfitting}||exercise-57=={(\textbf {A}) L1 regularization}||exercise-58=={(\textbf {C}) small weight values}||exercise-59=={(\textbf {B}) It randomly drops entire layers during training}||exercise-60=={(\textbf {C}) Elastic Net}||exercise-61=={(\textbf {B}) To prevent the model from memorizing the training data}||exercise-62=={(\textbf {C}) Regularization can help balance bias and variance}||exercise-63=={(\textbf {B}) The gradual decrease in weight values during training}||exercise-64=={(\textbf {D}) Reduced capacity to capture complex patterns}||exercise-65=={(\textbf {A}) A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.}||exercise-66=={(\textbf {A}) 98\% train. 1\% dev. 1\% test}||exercise-67=={(\textbf {A}) Come from the same distribution}||exercise-68=={(\textbf {B}) Get more training data}||exercise-69=={(\textbf {A}) Increase the regularization parameter lambda}||exercise-70=={(\textbf {A}) Weights are pushed twoard becoming smaller (closer to 0)}||exercise-71=={(\textbf {B}) You don't apply dropout (do not randomly eliminate units) and do not keep the \texttt {1/keep\_prob} factor in the calculations usd in the training}||exercise-72=={(\textbf {A}) Dropout}||exercise-73=={(\textbf {B}) It makes the cost function faster to optimize}||exercise-74=={(\textbf {C}) Regulates the softness of the target distribution}||exercise-75=={(\textbf {C}) The probability of dropping out a unit in the hidden layers during training}||exercise-76=={(\textbf {B}) Learning rate annealing}||exercise-77=={(\textbf {D}) To prevent the model from memorizing the training data}||exercise-78=={(\textbf {C}) Constraining the magnitude of the weights in the model}||exercise-79=={(\textbf {D}) Dropout helps prevent co-adaptation of hidden units}||exercise-80=={(\textbf {C}) To improve the predictive performance of a model by combining multiple models}||exercise-81=={(\textbf {B}) It trains multiple models independently on different subsets of the training data.}||exercise-82=={(\textbf {C}) To introduce randomness by considering a random subset of features for each tree}||exercise-83=={(\textbf {C}) Sequentially, with higher weights for misclassified instances}||exercise-84=={(\textbf {B}) Stacking}||exercise-85=={(\textbf {C}) Ensemble methods often generalize better and have improved robustness.}||exercise-86=={(\textbf {B}) A model that performs slightly better than random chance}||exercise-87=={(\textbf {C}) Bagging}||exercise-88=={(\textbf {A}) Bootstrap Aggregating}||exercise-89=={(\textbf {B}) AdaBoost}||exercise-90=={(\textbf {B}) Improved generalization and robustness}||exercise-91=={(\textbf {C}) Random Forest}||exercise-92=={(\textbf {A}) Long Short-Term Memory}||exercise-93=={(\textbf {A}) It adjusts the amount by which weights are updated during each iteration}||exercise-94=={(\textbf {C}) Random Forest introduces randomness by considering a random subset of features for each tree}||exercise-95=={(\textbf {D}) Stacking uses multiple base models to form a meta-model}||exercise-96=={(\textbf {C}) Ensemble methods help balance bias and variance}||exercise-97=={(\textbf {A}) Increased risk of overfitting}||exercise-98=={(\textbf {A}) Randomly and with replacement}||exercise-99=={(\textbf {B}) Better handling of outliers}||exercise-100=={(\textbf {C}) Boosting}||exercise-101=={(\textbf {D}) Using multiple base models to form a meta-model}||exercise-102=={(\textbf {C}) Random Forest}||exercise-103=={(\textbf {C}) It is an estimate of the test error obtained from the unused samples during training}||exercise-104=={(\textbf {B}) It limits the maximum depth of individual decision trees}||exercise-105=={(\textbf {B}) Early stopping prevents overfitting by stopping the training process when the model starts to memorize the training data.}||exercise-106=={(\textbf {B}) The computational complexity increases linearly}||exercise-107=={(\textbf {A}) Adversarial training involves training models to be robust against adversarial attacks.}||exercise-108=={(\textbf {B}) It uses multiple cross-validated models, reducing overfitting.}||exercise-109=={(\textbf {B}) Increased risk of overfitting}||exercise-110=={(\textbf {B}) Feature importance indicates the relevance of a feature in predicting the target variable.}||exercise-111=={(\textbf {C}) It specifies the number of base models in the ensemble.}||exercise-112=={(\textbf {A}) Stacking with meta-features involves using the output of base models as features for a meta-model.}||exercise-113=={(\textbf {B}) Removing random neurons during training}||exercise-114=={(\textbf {C}) To prevent co-adaptation of neurons}||exercise-115=={(\textbf {B}) Dropout is applied to all layers except the output layer}||exercise-116=={(\textbf {C}) By reducing the model's capacity}||exercise-117=={(\textbf {B}) The neuron is removed from the network temporarily}||exercise-118=={(\textbf {B}) Overfitting}||exercise-119=={(\textbf {A}) Slows down the training process}||exercise-120=={(\textbf {B}) 0.2 to 0.5}||exercise-121=={(\textbf {C}) By reducing the sensitivity of neurons to specific input features}||exercise-122=={(\textbf {A}) Training phase}||exercise-123=={(\textbf {A}) Co-adaptation refers to neurons relying too much on each other, and Dropout breaks these dependencies by randomly dropping neurons during training.}||exercise-124=={(\textbf {B}) Dropout is more effective in large and complex networks}||exercise-125=={(\textbf {C}) Dropout and ensemble learning achieve the same result in terms of model diversity}||exercise-126=={(\textbf {A}) High Dropout rates lead to overfitting, while low Dropout rates may result in underfitting.}||exercise-127=={(\textbf {C}) By introducing noise to the input data}||exercise-128=={(\textbf {C}) To improve model performance by increasing the diversity of the training data}||exercise-129=={(\textbf {C}) Image rotation}||exercise-130=={(\textbf {D}) By providing a more diverse set of training examples}||exercise-131=={(\textbf {C}) Word substitution}||exercise-132=={(\textbf {B}) Potential introduction of unrealistic patterns}||exercise-133=={(\textbf {C}) To create variations in the spatial location of objects}||exercise-134=={(\textbf {B}) Time warping}||exercise-135=={(\textbf {A}) Jittering refers to the introduction of noise to input features}||exercise-136=={(\textbf {B}) To create mirror images}||exercise-137=={(\textbf {A}) Data augmentation focuses on creating new samples, while feature engineering manipulates existing features.}||exercise-138=={(\textbf {B}) Dropout enhances data augmentation by randomly removing features during training}||exercise-139=={(\textbf {B}) Spectrogram augmentation}||exercise-140=={(\textbf {B}) To introduce non-linear distortions to the image}||exercise-141=={(\textbf {D}) Sentence dropout}||exercise-142=={(\textbf {A}) Adversarial training focuses on creating adversarial examples to test the model's robustness against unseen patterns introduced by data augmentation.}||exercise-143=={(\textbf {C}) Data augmentation generates additional samples for minority classes, addressing class imbalance}||exercise-144=={(\textbf {C}) The potential introduction of unrealistic patterns}||exercise-145=={(\textbf {A}) Mixup involves blending two or more samples, creating new synthetic samples with averaged labels.}||exercise-146=={(\textbf {C}) Data augmentation reduces model interpretability due to the introduction of synthetic samples.}||exercise-147=={(\textbf {A}) To remove random portions from images}||exercise-148=={(\textbf {C}) Shearing introduces non-linear distortions to the image by tilting it along one of its axes.}||exercise-149=={(\textbf {C}) Random Forest}||exercise-150=={(\textbf {D}) The dataset is large}||exercise-151=={(\textbf {C}) Gradient Boosting}||exercise-152=={(\textbf {B}) False}||exercise-153=={(\textbf {A}) Neural Network}||exercise-154=={(\textbf {A}) 1 and 2}||exercise-155=={(\textbf {B}) False}||exercise-156=={(\textbf {A}) Fine tune only the last couple of layers and change the last layer (classification layer) to regression layer}||exercise-157=={(\textbf {C}) 218x218x5}||exercise-158=={(\textbf {B}) No}||exercise-159=={(\textbf {B}) Exactly 2 secs}||exercise-160=={(\textbf {F}) All of these}||exercise-161=={(\textbf {B}) Lower the perplexity the better}||exercise-162=={(\textbf {D}) 7}||exercise-163=={(\textbf {B}) False}||exercise-164=={(\textbf {D}) All of the above}||exercise-165=={(\textbf {B}) FALSE}||exercise-166=={(\textbf {G}) All of these}||exercise-167=={(\textbf {C}) Recurrent Neural Network}||exercise-168=={(\textbf {A}) Convolutional network on input and deconvolutional network on output}||exercise-169=={(\textbf {B}) False}||exercise-170=={(\textbf {B}) Higher the dropout rate, lower is the regularization}||exercise-171=={(\textbf {A}) Unlike backprop, in BPTT we sum up gradients for corresponding weight for each time step}||exercise-172=={(\textbf {B}) Gradient clipping}||exercise-173=={(\textbf {C}) Both of them}||exercise-174=={(\textbf {C}) PCA}||exercise-175=={(\textbf {D}) Subgradient method}||exercise-176=={(\textbf {C}) 1 is False and 2 is True}||exercise-177=={(\textbf {C}) C}||exercise-178=={(\textbf {C}) Sum of squared error with respect to outputs}||exercise-179=={(\textbf {B}) Parallelization of the neural network is best when the memory is used optimally}||exercise-180=={(\textbf {B}) Use attention mechanism}||exercise-181=={(\textbf {A}) TRUE}||exercise-182=={(\textbf {D}) All of the above}||exercise-183=={(\textbf {A}) Content-based addressing}||exercise-184=={(\textbf {C}) Fractional strided convolutional layer}||exercise-185=={(\textbf {D}) Both 1 and 2}||exercise-186=={(\textbf {A}) Previous hidden state would be ignored}||exercise-187=={(\textbf {B}) Copies the information through many time steps}||exercise-188=={(\textbf {C}) RNN layer}||exercise-189=={(\textbf {B}) CNN followed by recurrent units}||exercise-190=={(\textbf {B}) Image classification}||exercise-191=={(\textbf {A}) Convolutional layer}||exercise-192=={(\textbf {A}) They can capture local spatial patterns in the input data}||exercise-193=={(\textbf {B}) To reduce the spatial dimensions of the feature maps}||exercise-194=={(\textbf {B}) ReLU (Rectified Linear Unit)}||exercise-195=={(\textbf {B}) To control the step size of the convolution operation}||exercise-196=={(\textbf {B}) Pooling layer}||exercise-197=={(\textbf {B}) To prevent the reduction of spatial dimensions}||exercise-198=={(\textbf {C}) Fully connected layer}||exercise-199=={(\textbf {A}) To capture global patterns and make predictions}||exercise-200=={(\textbf {D}) Activation layer}||exercise-201=={(\textbf {A}) To randomly disable neurons during training to prevent overfitting}||exercise-202=={(\textbf {C}) Fully connected layer}||exercise-203=={(\textbf {D}) CNNs can capture local spatial patterns in the input data}||exercise-204=={(\textbf {A}) Convolutional layer}||exercise-205=={(\textbf {C}) To specify the size of the local region for the convolution operation}||exercise-206=={(\textbf {B}) Pooling layer}||exercise-207=={(\textbf {C}) To extract local features from the input data}||exercise-208=={(\textbf {C}) Batch normalization layer}||exercise-209=={(\textbf {A}) To minimize the prediction error on the training data}||exercise-210=={(\textbf {A}) Convolutional layer}||exercise-211=={(\textbf {A}) To compute the predicted output based on the final feature representation}||exercise-212=={(\textbf {B}) To prevent the reduction of spatial dimensions}||exercise-213=={(\textbf {D}) Upsampling layer}||exercise-214=={(\textbf {A}) To measure the prediction error and guide the learning process}||exercise-215=={(\textbf {D}) Activation layer}||exercise-216=={(\textbf {A}) To control the step size of the parameter updates during optimization}||exercise-217=={(\textbf {A}) Convolutional layer}||exercise-218=={(\textbf {B}) To introduce noise and variations in the training data}||exercise-219=={(\textbf {D}) None of the above}||exercise-220=={(\textbf {B}) Text generation}||exercise-221=={(\textbf {B}) Hidden layer}||exercise-222=={(\textbf {D}) They can capture temporal dependencies in the input data}||exercise-223=={(\textbf {A}) To store the information from the previous time step}||exercise-224=={(\textbf {C}) Tanh (Hyperbolic Tangent)}||exercise-225=={(\textbf {C}) To specify the length of the input sequence}||exercise-226=={(\textbf {B}) Hidden layer}||exercise-227=={(\textbf {A}) To handle sequential data in both forward and backward directions}||exercise-228=={(\textbf {C}) Output layer}||exercise-229=={(\textbf {A}) To propagate the hidden state across different time steps}||exercise-230=={(\textbf {D}) Attention layer}||exercise-231=={(\textbf {A}) To compute the gradients and update the network's parameters}||exercise-232=={(\textbf {A}) Input layer}||exercise-233=={(\textbf {A}) To provide the starting point for the recurrent computation}||exercise-234=={(\textbf {C}) Output layer}||exercise-235=={(\textbf {B}) To propagate the gradients through time}||exercise-236=={(\textbf {C}) Output layer}||exercise-237=={(\textbf {A}) To process an input sequence and produce a fixed-length representation}||exercise-238=={(\textbf {D}) Activation layer}||exercise-239=={(\textbf {A}) To control the flow of information from the previous hidden state}||exercise-240=={(\textbf {D}) Attention layer}||exercise-241=={(\textbf {A}) To allow the cell state to influence the gating mechanisms}||exercise-242=={(\textbf {C}) Output layer}||exercise-243=={(\textbf {A}) To store long-term dependencies in the input sequence}||exercise-244=={(\textbf {C}) Output layer}||exercise-245=={(\textbf {A}) To control the flow of information from the current input}||exercise-246=={(\textbf {D}) None of the above}||exercise-247=={(\textbf {A}) To control the flow of information to the current output}||exercise-248=={(\textbf {C}) Output layer}||exercise-249=={(\textbf {A}) To reset the hidden state based on the current input}}
\XSIM{correctchoicetwo}{exercise-68=={(\textbf {E}) Add regularization}||exercise-69=={(\textbf {C}) get more training data}||exercise-72=={(\textbf {C}) Data augmentation}}
\XSIM{correctchoicethree}{exercise-72=={(\textbf {F}) L2 regularization}}
\XSIM{correctchoicefour}{}
\XSIM{correctchoicefive}{}
